{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tunning DinoV2-DPT on Parihaka Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/_distutils_hack/__init__.py:53: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "/home/vscode/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using xFormers lib!\n"
     ]
    }
   ],
   "source": [
    "from common import get_data_module, get_trainer_pipeline\n",
    "import torch\n",
    "from minerva.models.ssl.dinov2 import (\n",
    "    DinoVisionTransformer,\n",
    "    DPT,\n",
    "    NestedTensorBlock,\n",
    "    MemEffAttention,\n",
    "    DinoV2\n",
    ")\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variaveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_data_dir = \"/workspaces/HIAAC-KR-Dev-Container/shared_data/seam_ai_datasets/seam_ai/images\"\n",
    "root_annotation_dir = \"/workspaces/HIAAC-KR-Dev-Container/shared_data/seam_ai_datasets/seam_ai/annotations\"\n",
    "img_size = (1008, 784)          # Change this to the size of the images in the dataset\n",
    "model_name = \"dinov2_dpt\"       # Model name (just identifier)\n",
    "dataset_name = \"seam_ai\"        # Dataset name (just identifier)\n",
    "single_channel = False          # If True, the model will be trained with single channel images (instead of 3 channels)\n",
    "\n",
    "log_dir = \"./logs\"              # Directory to save logs\n",
    "batch_size = 1                  # Batch size    \n",
    "seed = 42                       # Seed for reproducibility\n",
    "num_epochs = 100                # Number of epochs to train\n",
    "is_debug = True                 # If True, only 3 batch will be processed for 3 epochs\n",
    "accelerator = \"gpu\"             # CPU or GPU\n",
    "devices = 1                     # Num GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataModule\n",
       "    Data: /workspaces/HIAAC-KR-Dev-Container/shared_data/seam_ai_datasets/seam_ai/images\n",
       "    Annotations: /workspaces/HIAAC-KR-Dev-Container/shared_data/seam_ai_datasets/seam_ai/annotations\n",
       "    Batch size: 1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module = get_data_module(\n",
    "    root_data_dir=root_data_dir,\n",
    "    root_annotation_dir=root_annotation_dir,\n",
    "    img_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    seed=seed,\n",
    "    single_channel=single_channel\n",
    ")\n",
    "\n",
    "data_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 1008, 784]), torch.Size([1, 1, 1008, 784]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just to check if the data module is working\n",
    "data_module.setup(\"fit\")\n",
    "train_batch_x, train_batch_y = next(iter(data_module.train_dataloader()))\n",
    "train_batch_x.shape, train_batch_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **** Create and Load model HERE ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = DinoVisionTransformer(\n",
    "    patch_size=14,\n",
    "    embed_dim=384,\n",
    "    depth=12,\n",
    "    num_heads=6,\n",
    "    mlp_ratio=4,\n",
    "    block_fn=partial(NestedTensorBlock, attn_class=MemEffAttention),  # type: ignore\n",
    "    init_values=1e-5,\n",
    "    block_chunks=0,\n",
    ")\n",
    "\n",
    "head = DPT(embedding_dim=384, num_classes=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DinoV2(\n",
       "  (backbone): DinoVisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x NestedTensorBlock(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): MemEffAttention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "    (head): Identity()\n",
       "  )\n",
       "  (head): DPT(\n",
       "    (scratch): Module(\n",
       "      (layer1_rn): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer2_rn): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer3_rn): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer4_rn): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (refinenet1): FeatureFusionBlock_custom(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit_custom(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): ReLU()\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit_custom(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): ReLU()\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet2): FeatureFusionBlock_custom(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit_custom(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): ReLU()\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit_custom(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): ReLU()\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet3): FeatureFusionBlock_custom(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit_custom(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): ReLU()\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit_custom(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): ReLU()\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet4): FeatureFusionBlock_custom(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit_custom(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): ReLU()\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit_custom(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): ReLU()\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (single_conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (output_conv): Sequential(\n",
       "        (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(32, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DinoV2(\n",
    "    backbone=backbone,\n",
    "    head=head,\n",
    "    loss_fn=torch.nn.CrossEntropyLoss(),\n",
    "    output_shape = (1008, 784),\n",
    "    middle=True\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing key renaming with: {'': 'backbone.'}\n",
      "\tRenaming key: cls_token -> backbone.cls_token (changed: True)\n",
      "\tRenaming key: pos_embed -> backbone.pos_embed (changed: True)\n",
      "\tRenaming key: mask_token -> backbone.mask_token (changed: True)\n",
      "\tRenaming key: patch_embed.proj.weight -> backbone.patch_embed.proj.weight (changed: True)\n",
      "\tRenaming key: patch_embed.proj.bias -> backbone.patch_embed.proj.bias (changed: True)\n",
      "\tRenaming key: blocks.0.norm1.weight -> backbone.blocks.0.norm1.weight (changed: True)\n",
      "\tRenaming key: blocks.0.norm1.bias -> backbone.blocks.0.norm1.bias (changed: True)\n",
      "\tRenaming key: blocks.0.attn.qkv.weight -> backbone.blocks.0.attn.qkv.weight (changed: True)\n",
      "\tRenaming key: blocks.0.attn.qkv.bias -> backbone.blocks.0.attn.qkv.bias (changed: True)\n",
      "\tRenaming key: blocks.0.attn.proj.weight -> backbone.blocks.0.attn.proj.weight (changed: True)\n",
      "\tRenaming key: blocks.0.attn.proj.bias -> backbone.blocks.0.attn.proj.bias (changed: True)\n",
      "\tRenaming key: blocks.0.ls1.gamma -> backbone.blocks.0.ls1.gamma (changed: True)\n",
      "\tRenaming key: blocks.0.norm2.weight -> backbone.blocks.0.norm2.weight (changed: True)\n",
      "\tRenaming key: blocks.0.norm2.bias -> backbone.blocks.0.norm2.bias (changed: True)\n",
      "\tRenaming key: blocks.0.mlp.fc1.weight -> backbone.blocks.0.mlp.fc1.weight (changed: True)\n",
      "\tRenaming key: blocks.0.mlp.fc1.bias -> backbone.blocks.0.mlp.fc1.bias (changed: True)\n",
      "\tRenaming key: blocks.0.mlp.fc2.weight -> backbone.blocks.0.mlp.fc2.weight (changed: True)\n",
      "\tRenaming key: blocks.0.mlp.fc2.bias -> backbone.blocks.0.mlp.fc2.bias (changed: True)\n",
      "\tRenaming key: blocks.0.ls2.gamma -> backbone.blocks.0.ls2.gamma (changed: True)\n",
      "\tRenaming key: blocks.1.norm1.weight -> backbone.blocks.1.norm1.weight (changed: True)\n",
      "\tRenaming key: blocks.1.norm1.bias -> backbone.blocks.1.norm1.bias (changed: True)\n",
      "\tRenaming key: blocks.1.attn.qkv.weight -> backbone.blocks.1.attn.qkv.weight (changed: True)\n",
      "\tRenaming key: blocks.1.attn.qkv.bias -> backbone.blocks.1.attn.qkv.bias (changed: True)\n",
      "\tRenaming key: blocks.1.attn.proj.weight -> backbone.blocks.1.attn.proj.weight (changed: True)\n",
      "\tRenaming key: blocks.1.attn.proj.bias -> backbone.blocks.1.attn.proj.bias (changed: True)\n",
      "\tRenaming key: blocks.1.ls1.gamma -> backbone.blocks.1.ls1.gamma (changed: True)\n",
      "\tRenaming key: blocks.1.norm2.weight -> backbone.blocks.1.norm2.weight (changed: True)\n",
      "\tRenaming key: blocks.1.norm2.bias -> backbone.blocks.1.norm2.bias (changed: True)\n",
      "\tRenaming key: blocks.1.mlp.fc1.weight -> backbone.blocks.1.mlp.fc1.weight (changed: True)\n",
      "\tRenaming key: blocks.1.mlp.fc1.bias -> backbone.blocks.1.mlp.fc1.bias (changed: True)\n",
      "\tRenaming key: blocks.1.mlp.fc2.weight -> backbone.blocks.1.mlp.fc2.weight (changed: True)\n",
      "\tRenaming key: blocks.1.mlp.fc2.bias -> backbone.blocks.1.mlp.fc2.bias (changed: True)\n",
      "\tRenaming key: blocks.1.ls2.gamma -> backbone.blocks.1.ls2.gamma (changed: True)\n",
      "\tRenaming key: blocks.2.norm1.weight -> backbone.blocks.2.norm1.weight (changed: True)\n",
      "\tRenaming key: blocks.2.norm1.bias -> backbone.blocks.2.norm1.bias (changed: True)\n",
      "\tRenaming key: blocks.2.attn.qkv.weight -> backbone.blocks.2.attn.qkv.weight (changed: True)\n",
      "\tRenaming key: blocks.2.attn.qkv.bias -> backbone.blocks.2.attn.qkv.bias (changed: True)\n",
      "\tRenaming key: blocks.2.attn.proj.weight -> backbone.blocks.2.attn.proj.weight (changed: True)\n",
      "\tRenaming key: blocks.2.attn.proj.bias -> backbone.blocks.2.attn.proj.bias (changed: True)\n",
      "\tRenaming key: blocks.2.ls1.gamma -> backbone.blocks.2.ls1.gamma (changed: True)\n",
      "\tRenaming key: blocks.2.norm2.weight -> backbone.blocks.2.norm2.weight (changed: True)\n",
      "\tRenaming key: blocks.2.norm2.bias -> backbone.blocks.2.norm2.bias (changed: True)\n",
      "\tRenaming key: blocks.2.mlp.fc1.weight -> backbone.blocks.2.mlp.fc1.weight (changed: True)\n",
      "\tRenaming key: blocks.2.mlp.fc1.bias -> backbone.blocks.2.mlp.fc1.bias (changed: True)\n",
      "\tRenaming key: blocks.2.mlp.fc2.weight -> backbone.blocks.2.mlp.fc2.weight (changed: True)\n",
      "\tRenaming key: blocks.2.mlp.fc2.bias -> backbone.blocks.2.mlp.fc2.bias (changed: True)\n",
      "\tRenaming key: blocks.2.ls2.gamma -> backbone.blocks.2.ls2.gamma (changed: True)\n",
      "\tRenaming key: blocks.3.norm1.weight -> backbone.blocks.3.norm1.weight (changed: True)\n",
      "\tRenaming key: blocks.3.norm1.bias -> backbone.blocks.3.norm1.bias (changed: True)\n",
      "\tRenaming key: blocks.3.attn.qkv.weight -> backbone.blocks.3.attn.qkv.weight (changed: True)\n",
      "\tRenaming key: blocks.3.attn.qkv.bias -> backbone.blocks.3.attn.qkv.bias (changed: True)\n",
      "\tRenaming key: blocks.3.attn.proj.weight -> backbone.blocks.3.attn.proj.weight (changed: True)\n",
      "\tRenaming key: blocks.3.attn.proj.bias -> backbone.blocks.3.attn.proj.bias (changed: True)\n",
      "\tRenaming key: blocks.3.ls1.gamma -> backbone.blocks.3.ls1.gamma (changed: True)\n",
      "\tRenaming key: blocks.3.norm2.weight -> backbone.blocks.3.norm2.weight (changed: True)\n",
      "\tRenaming key: blocks.3.norm2.bias -> backbone.blocks.3.norm2.bias (changed: True)\n",
      "\tRenaming key: blocks.3.mlp.fc1.weight -> backbone.blocks.3.mlp.fc1.weight (changed: True)\n",
      "\tRenaming key: blocks.3.mlp.fc1.bias -> backbone.blocks.3.mlp.fc1.bias (changed: True)\n",
      "\tRenaming key: blocks.3.mlp.fc2.weight -> backbone.blocks.3.mlp.fc2.weight (changed: True)\n",
      "\tRenaming key: blocks.3.mlp.fc2.bias -> backbone.blocks.3.mlp.fc2.bias (changed: True)\n",
      "\tRenaming key: blocks.3.ls2.gamma -> backbone.blocks.3.ls2.gamma (changed: True)\n",
      "\tRenaming key: blocks.4.norm1.weight -> backbone.blocks.4.norm1.weight (changed: True)\n",
      "\tRenaming key: blocks.4.norm1.bias -> backbone.blocks.4.norm1.bias (changed: True)\n",
      "\tRenaming key: blocks.4.attn.qkv.weight -> backbone.blocks.4.attn.qkv.weight (changed: True)\n",
      "\tRenaming key: blocks.4.attn.qkv.bias -> backbone.blocks.4.attn.qkv.bias (changed: True)\n",
      "\tRenaming key: blocks.4.attn.proj.weight -> backbone.blocks.4.attn.proj.weight (changed: True)\n",
      "\tRenaming key: blocks.4.attn.proj.bias -> backbone.blocks.4.attn.proj.bias (changed: True)\n",
      "\tRenaming key: blocks.4.ls1.gamma -> backbone.blocks.4.ls1.gamma (changed: True)\n",
      "\tRenaming key: blocks.4.norm2.weight -> backbone.blocks.4.norm2.weight (changed: True)\n",
      "\tRenaming key: blocks.4.norm2.bias -> backbone.blocks.4.norm2.bias (changed: True)\n",
      "\tRenaming key: blocks.4.mlp.fc1.weight -> backbone.blocks.4.mlp.fc1.weight (changed: True)\n",
      "\tRenaming key: blocks.4.mlp.fc1.bias -> backbone.blocks.4.mlp.fc1.bias (changed: True)\n",
      "\tRenaming key: blocks.4.mlp.fc2.weight -> backbone.blocks.4.mlp.fc2.weight (changed: True)\n",
      "\tRenaming key: blocks.4.mlp.fc2.bias -> backbone.blocks.4.mlp.fc2.bias (changed: True)\n",
      "\tRenaming key: blocks.4.ls2.gamma -> backbone.blocks.4.ls2.gamma (changed: True)\n",
      "\tRenaming key: blocks.5.norm1.weight -> backbone.blocks.5.norm1.weight (changed: True)\n",
      "\tRenaming key: blocks.5.norm1.bias -> backbone.blocks.5.norm1.bias (changed: True)\n",
      "\tRenaming key: blocks.5.attn.qkv.weight -> backbone.blocks.5.attn.qkv.weight (changed: True)\n",
      "\tRenaming key: blocks.5.attn.qkv.bias -> backbone.blocks.5.attn.qkv.bias (changed: True)\n",
      "\tRenaming key: blocks.5.attn.proj.weight -> backbone.blocks.5.attn.proj.weight (changed: True)\n",
      "\tRenaming key: blocks.5.attn.proj.bias -> backbone.blocks.5.attn.proj.bias (changed: True)\n",
      "\tRenaming key: blocks.5.ls1.gamma -> backbone.blocks.5.ls1.gamma (changed: True)\n",
      "\tRenaming key: blocks.5.norm2.weight -> backbone.blocks.5.norm2.weight (changed: True)\n",
      "\tRenaming key: blocks.5.norm2.bias -> backbone.blocks.5.norm2.bias (changed: True)\n",
      "\tRenaming key: blocks.5.mlp.fc1.weight -> backbone.blocks.5.mlp.fc1.weight (changed: True)\n",
      "\tRenaming key: blocks.5.mlp.fc1.bias -> backbone.blocks.5.mlp.fc1.bias (changed: True)\n",
      "\tRenaming key: blocks.5.mlp.fc2.weight -> backbone.blocks.5.mlp.fc2.weight (changed: True)\n",
      "\tRenaming key: blocks.5.mlp.fc2.bias -> backbone.blocks.5.mlp.fc2.bias (changed: True)\n",
      "\tRenaming key: blocks.5.ls2.gamma -> backbone.blocks.5.ls2.gamma (changed: True)\n",
      "\tRenaming key: blocks.6.norm1.weight -> backbone.blocks.6.norm1.weight (changed: True)\n",
      "\tRenaming key: blocks.6.norm1.bias -> backbone.blocks.6.norm1.bias (changed: True)\n",
      "\tRenaming key: blocks.6.attn.qkv.weight -> backbone.blocks.6.attn.qkv.weight (changed: True)\n",
      "\tRenaming key: blocks.6.attn.qkv.bias -> backbone.blocks.6.attn.qkv.bias (changed: True)\n",
      "\tRenaming key: blocks.6.attn.proj.weight -> backbone.blocks.6.attn.proj.weight (changed: True)\n",
      "\tRenaming key: blocks.6.attn.proj.bias -> backbone.blocks.6.attn.proj.bias (changed: True)\n",
      "\tRenaming key: blocks.6.ls1.gamma -> backbone.blocks.6.ls1.gamma (changed: True)\n",
      "\tRenaming key: blocks.6.norm2.weight -> backbone.blocks.6.norm2.weight (changed: True)\n",
      "\tRenaming key: blocks.6.norm2.bias -> backbone.blocks.6.norm2.bias (changed: True)\n",
      "\tRenaming key: blocks.6.mlp.fc1.weight -> backbone.blocks.6.mlp.fc1.weight (changed: True)\n",
      "\tRenaming key: blocks.6.mlp.fc1.bias -> backbone.blocks.6.mlp.fc1.bias (changed: True)\n",
      "\tRenaming key: blocks.6.mlp.fc2.weight -> backbone.blocks.6.mlp.fc2.weight (changed: True)\n",
      "\tRenaming key: blocks.6.mlp.fc2.bias -> backbone.blocks.6.mlp.fc2.bias (changed: True)\n",
      "\tRenaming key: blocks.6.ls2.gamma -> backbone.blocks.6.ls2.gamma (changed: True)\n",
      "\tRenaming key: blocks.7.norm1.weight -> backbone.blocks.7.norm1.weight (changed: True)\n",
      "\tRenaming key: blocks.7.norm1.bias -> backbone.blocks.7.norm1.bias (changed: True)\n",
      "\tRenaming key: blocks.7.attn.qkv.weight -> backbone.blocks.7.attn.qkv.weight (changed: True)\n",
      "\tRenaming key: blocks.7.attn.qkv.bias -> backbone.blocks.7.attn.qkv.bias (changed: True)\n",
      "\tRenaming key: blocks.7.attn.proj.weight -> backbone.blocks.7.attn.proj.weight (changed: True)\n",
      "\tRenaming key: blocks.7.attn.proj.bias -> backbone.blocks.7.attn.proj.bias (changed: True)\n",
      "\tRenaming key: blocks.7.ls1.gamma -> backbone.blocks.7.ls1.gamma (changed: True)\n",
      "\tRenaming key: blocks.7.norm2.weight -> backbone.blocks.7.norm2.weight (changed: True)\n",
      "\tRenaming key: blocks.7.norm2.bias -> backbone.blocks.7.norm2.bias (changed: True)\n",
      "\tRenaming key: blocks.7.mlp.fc1.weight -> backbone.blocks.7.mlp.fc1.weight (changed: True)\n",
      "\tRenaming key: blocks.7.mlp.fc1.bias -> backbone.blocks.7.mlp.fc1.bias (changed: True)\n",
      "\tRenaming key: blocks.7.mlp.fc2.weight -> backbone.blocks.7.mlp.fc2.weight (changed: True)\n",
      "\tRenaming key: blocks.7.mlp.fc2.bias -> backbone.blocks.7.mlp.fc2.bias (changed: True)\n",
      "\tRenaming key: blocks.7.ls2.gamma -> backbone.blocks.7.ls2.gamma (changed: True)\n",
      "\tRenaming key: blocks.8.norm1.weight -> backbone.blocks.8.norm1.weight (changed: True)\n",
      "\tRenaming key: blocks.8.norm1.bias -> backbone.blocks.8.norm1.bias (changed: True)\n",
      "\tRenaming key: blocks.8.attn.qkv.weight -> backbone.blocks.8.attn.qkv.weight (changed: True)\n",
      "\tRenaming key: blocks.8.attn.qkv.bias -> backbone.blocks.8.attn.qkv.bias (changed: True)\n",
      "\tRenaming key: blocks.8.attn.proj.weight -> backbone.blocks.8.attn.proj.weight (changed: True)\n",
      "\tRenaming key: blocks.8.attn.proj.bias -> backbone.blocks.8.attn.proj.bias (changed: True)\n",
      "\tRenaming key: blocks.8.ls1.gamma -> backbone.blocks.8.ls1.gamma (changed: True)\n",
      "\tRenaming key: blocks.8.norm2.weight -> backbone.blocks.8.norm2.weight (changed: True)\n",
      "\tRenaming key: blocks.8.norm2.bias -> backbone.blocks.8.norm2.bias (changed: True)\n",
      "\tRenaming key: blocks.8.mlp.fc1.weight -> backbone.blocks.8.mlp.fc1.weight (changed: True)\n",
      "\tRenaming key: blocks.8.mlp.fc1.bias -> backbone.blocks.8.mlp.fc1.bias (changed: True)\n",
      "\tRenaming key: blocks.8.mlp.fc2.weight -> backbone.blocks.8.mlp.fc2.weight (changed: True)\n",
      "\tRenaming key: blocks.8.mlp.fc2.bias -> backbone.blocks.8.mlp.fc2.bias (changed: True)\n",
      "\tRenaming key: blocks.8.ls2.gamma -> backbone.blocks.8.ls2.gamma (changed: True)\n",
      "\tRenaming key: blocks.9.norm1.weight -> backbone.blocks.9.norm1.weight (changed: True)\n",
      "\tRenaming key: blocks.9.norm1.bias -> backbone.blocks.9.norm1.bias (changed: True)\n",
      "\tRenaming key: blocks.9.attn.qkv.weight -> backbone.blocks.9.attn.qkv.weight (changed: True)\n",
      "\tRenaming key: blocks.9.attn.qkv.bias -> backbone.blocks.9.attn.qkv.bias (changed: True)\n",
      "\tRenaming key: blocks.9.attn.proj.weight -> backbone.blocks.9.attn.proj.weight (changed: True)\n",
      "\tRenaming key: blocks.9.attn.proj.bias -> backbone.blocks.9.attn.proj.bias (changed: True)\n",
      "\tRenaming key: blocks.9.ls1.gamma -> backbone.blocks.9.ls1.gamma (changed: True)\n",
      "\tRenaming key: blocks.9.norm2.weight -> backbone.blocks.9.norm2.weight (changed: True)\n",
      "\tRenaming key: blocks.9.norm2.bias -> backbone.blocks.9.norm2.bias (changed: True)\n",
      "\tRenaming key: blocks.9.mlp.fc1.weight -> backbone.blocks.9.mlp.fc1.weight (changed: True)\n",
      "\tRenaming key: blocks.9.mlp.fc1.bias -> backbone.blocks.9.mlp.fc1.bias (changed: True)\n",
      "\tRenaming key: blocks.9.mlp.fc2.weight -> backbone.blocks.9.mlp.fc2.weight (changed: True)\n",
      "\tRenaming key: blocks.9.mlp.fc2.bias -> backbone.blocks.9.mlp.fc2.bias (changed: True)\n",
      "\tRenaming key: blocks.9.ls2.gamma -> backbone.blocks.9.ls2.gamma (changed: True)\n",
      "\tRenaming key: blocks.10.norm1.weight -> backbone.blocks.10.norm1.weight (changed: True)\n",
      "\tRenaming key: blocks.10.norm1.bias -> backbone.blocks.10.norm1.bias (changed: True)\n",
      "\tRenaming key: blocks.10.attn.qkv.weight -> backbone.blocks.10.attn.qkv.weight (changed: True)\n",
      "\tRenaming key: blocks.10.attn.qkv.bias -> backbone.blocks.10.attn.qkv.bias (changed: True)\n",
      "\tRenaming key: blocks.10.attn.proj.weight -> backbone.blocks.10.attn.proj.weight (changed: True)\n",
      "\tRenaming key: blocks.10.attn.proj.bias -> backbone.blocks.10.attn.proj.bias (changed: True)\n",
      "\tRenaming key: blocks.10.ls1.gamma -> backbone.blocks.10.ls1.gamma (changed: True)\n",
      "\tRenaming key: blocks.10.norm2.weight -> backbone.blocks.10.norm2.weight (changed: True)\n",
      "\tRenaming key: blocks.10.norm2.bias -> backbone.blocks.10.norm2.bias (changed: True)\n",
      "\tRenaming key: blocks.10.mlp.fc1.weight -> backbone.blocks.10.mlp.fc1.weight (changed: True)\n",
      "\tRenaming key: blocks.10.mlp.fc1.bias -> backbone.blocks.10.mlp.fc1.bias (changed: True)\n",
      "\tRenaming key: blocks.10.mlp.fc2.weight -> backbone.blocks.10.mlp.fc2.weight (changed: True)\n",
      "\tRenaming key: blocks.10.mlp.fc2.bias -> backbone.blocks.10.mlp.fc2.bias (changed: True)\n",
      "\tRenaming key: blocks.10.ls2.gamma -> backbone.blocks.10.ls2.gamma (changed: True)\n",
      "\tRenaming key: blocks.11.norm1.weight -> backbone.blocks.11.norm1.weight (changed: True)\n",
      "\tRenaming key: blocks.11.norm1.bias -> backbone.blocks.11.norm1.bias (changed: True)\n",
      "\tRenaming key: blocks.11.attn.qkv.weight -> backbone.blocks.11.attn.qkv.weight (changed: True)\n",
      "\tRenaming key: blocks.11.attn.qkv.bias -> backbone.blocks.11.attn.qkv.bias (changed: True)\n",
      "\tRenaming key: blocks.11.attn.proj.weight -> backbone.blocks.11.attn.proj.weight (changed: True)\n",
      "\tRenaming key: blocks.11.attn.proj.bias -> backbone.blocks.11.attn.proj.bias (changed: True)\n",
      "\tRenaming key: blocks.11.ls1.gamma -> backbone.blocks.11.ls1.gamma (changed: True)\n",
      "\tRenaming key: blocks.11.norm2.weight -> backbone.blocks.11.norm2.weight (changed: True)\n",
      "\tRenaming key: blocks.11.norm2.bias -> backbone.blocks.11.norm2.bias (changed: True)\n",
      "\tRenaming key: blocks.11.mlp.fc1.weight -> backbone.blocks.11.mlp.fc1.weight (changed: True)\n",
      "\tRenaming key: blocks.11.mlp.fc1.bias -> backbone.blocks.11.mlp.fc1.bias (changed: True)\n",
      "\tRenaming key: blocks.11.mlp.fc2.weight -> backbone.blocks.11.mlp.fc2.weight (changed: True)\n",
      "\tRenaming key: blocks.11.mlp.fc2.bias -> backbone.blocks.11.mlp.fc2.bias (changed: True)\n",
      "\tRenaming key: blocks.11.ls2.gamma -> backbone.blocks.11.ls2.gamma (changed: True)\n",
      "\tRenaming key: norm.weight -> backbone.norm.weight (changed: True)\n",
      "\tRenaming key: norm.bias -> backbone.norm.bias (changed: True)\n",
      "Model loaded from /workspaces/HIAAC-KR-Dev-Container/Minerva-Dev/docs/notebooks/examples/seismic/facies_classification/f3/dinov2_vits14_pretrain.pth\n",
      "When loading model, the following keys are missing: ['head.scratch.layer1_rn.weight', 'head.scratch.layer2_rn.weight', 'head.scratch.layer3_rn.weight', 'head.scratch.layer4_rn.weight', 'head.scratch.refinenet1.out_conv.weight', 'head.scratch.refinenet1.out_conv.bias', 'head.scratch.refinenet1.resConfUnit1.conv1.weight', 'head.scratch.refinenet1.resConfUnit1.conv2.weight', 'head.scratch.refinenet1.resConfUnit1.bn1.weight', 'head.scratch.refinenet1.resConfUnit1.bn1.bias', 'head.scratch.refinenet1.resConfUnit1.bn1.running_mean', 'head.scratch.refinenet1.resConfUnit1.bn1.running_var', 'head.scratch.refinenet1.resConfUnit1.bn2.weight', 'head.scratch.refinenet1.resConfUnit1.bn2.bias', 'head.scratch.refinenet1.resConfUnit1.bn2.running_mean', 'head.scratch.refinenet1.resConfUnit1.bn2.running_var', 'head.scratch.refinenet1.resConfUnit2.conv1.weight', 'head.scratch.refinenet1.resConfUnit2.conv2.weight', 'head.scratch.refinenet1.resConfUnit2.bn1.weight', 'head.scratch.refinenet1.resConfUnit2.bn1.bias', 'head.scratch.refinenet1.resConfUnit2.bn1.running_mean', 'head.scratch.refinenet1.resConfUnit2.bn1.running_var', 'head.scratch.refinenet1.resConfUnit2.bn2.weight', 'head.scratch.refinenet1.resConfUnit2.bn2.bias', 'head.scratch.refinenet1.resConfUnit2.bn2.running_mean', 'head.scratch.refinenet1.resConfUnit2.bn2.running_var', 'head.scratch.refinenet2.out_conv.weight', 'head.scratch.refinenet2.out_conv.bias', 'head.scratch.refinenet2.resConfUnit1.conv1.weight', 'head.scratch.refinenet2.resConfUnit1.conv2.weight', 'head.scratch.refinenet2.resConfUnit1.bn1.weight', 'head.scratch.refinenet2.resConfUnit1.bn1.bias', 'head.scratch.refinenet2.resConfUnit1.bn1.running_mean', 'head.scratch.refinenet2.resConfUnit1.bn1.running_var', 'head.scratch.refinenet2.resConfUnit1.bn2.weight', 'head.scratch.refinenet2.resConfUnit1.bn2.bias', 'head.scratch.refinenet2.resConfUnit1.bn2.running_mean', 'head.scratch.refinenet2.resConfUnit1.bn2.running_var', 'head.scratch.refinenet2.resConfUnit2.conv1.weight', 'head.scratch.refinenet2.resConfUnit2.conv2.weight', 'head.scratch.refinenet2.resConfUnit2.bn1.weight', 'head.scratch.refinenet2.resConfUnit2.bn1.bias', 'head.scratch.refinenet2.resConfUnit2.bn1.running_mean', 'head.scratch.refinenet2.resConfUnit2.bn1.running_var', 'head.scratch.refinenet2.resConfUnit2.bn2.weight', 'head.scratch.refinenet2.resConfUnit2.bn2.bias', 'head.scratch.refinenet2.resConfUnit2.bn2.running_mean', 'head.scratch.refinenet2.resConfUnit2.bn2.running_var', 'head.scratch.refinenet3.out_conv.weight', 'head.scratch.refinenet3.out_conv.bias', 'head.scratch.refinenet3.resConfUnit1.conv1.weight', 'head.scratch.refinenet3.resConfUnit1.conv2.weight', 'head.scratch.refinenet3.resConfUnit1.bn1.weight', 'head.scratch.refinenet3.resConfUnit1.bn1.bias', 'head.scratch.refinenet3.resConfUnit1.bn1.running_mean', 'head.scratch.refinenet3.resConfUnit1.bn1.running_var', 'head.scratch.refinenet3.resConfUnit1.bn2.weight', 'head.scratch.refinenet3.resConfUnit1.bn2.bias', 'head.scratch.refinenet3.resConfUnit1.bn2.running_mean', 'head.scratch.refinenet3.resConfUnit1.bn2.running_var', 'head.scratch.refinenet3.resConfUnit2.conv1.weight', 'head.scratch.refinenet3.resConfUnit2.conv2.weight', 'head.scratch.refinenet3.resConfUnit2.bn1.weight', 'head.scratch.refinenet3.resConfUnit2.bn1.bias', 'head.scratch.refinenet3.resConfUnit2.bn1.running_mean', 'head.scratch.refinenet3.resConfUnit2.bn1.running_var', 'head.scratch.refinenet3.resConfUnit2.bn2.weight', 'head.scratch.refinenet3.resConfUnit2.bn2.bias', 'head.scratch.refinenet3.resConfUnit2.bn2.running_mean', 'head.scratch.refinenet3.resConfUnit2.bn2.running_var', 'head.scratch.refinenet4.out_conv.weight', 'head.scratch.refinenet4.out_conv.bias', 'head.scratch.refinenet4.resConfUnit1.conv1.weight', 'head.scratch.refinenet4.resConfUnit1.conv2.weight', 'head.scratch.refinenet4.resConfUnit1.bn1.weight', 'head.scratch.refinenet4.resConfUnit1.bn1.bias', 'head.scratch.refinenet4.resConfUnit1.bn1.running_mean', 'head.scratch.refinenet4.resConfUnit1.bn1.running_var', 'head.scratch.refinenet4.resConfUnit1.bn2.weight', 'head.scratch.refinenet4.resConfUnit1.bn2.bias', 'head.scratch.refinenet4.resConfUnit1.bn2.running_mean', 'head.scratch.refinenet4.resConfUnit1.bn2.running_var', 'head.scratch.refinenet4.resConfUnit2.conv1.weight', 'head.scratch.refinenet4.resConfUnit2.conv2.weight', 'head.scratch.refinenet4.resConfUnit2.bn1.weight', 'head.scratch.refinenet4.resConfUnit2.bn1.bias', 'head.scratch.refinenet4.resConfUnit2.bn1.running_mean', 'head.scratch.refinenet4.resConfUnit2.bn1.running_var', 'head.scratch.refinenet4.resConfUnit2.bn2.weight', 'head.scratch.refinenet4.resConfUnit2.bn2.bias', 'head.scratch.refinenet4.resConfUnit2.bn2.running_mean', 'head.scratch.refinenet4.resConfUnit2.bn2.running_var', 'head.scratch.single_conv.weight', 'head.scratch.single_conv.bias', 'head.scratch.output_conv.0.weight', 'head.scratch.output_conv.0.bias', 'head.scratch.output_conv.2.weight', 'head.scratch.output_conv.2.bias']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DinoV2(\n",
       "  (backbone): DinoVisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x NestedTensorBlock(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): MemEffAttention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "    (head): Identity()\n",
       "  )\n",
       "  (head): DPT(\n",
       "    (scratch): Module(\n",
       "      (layer1_rn): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer2_rn): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer3_rn): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer4_rn): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (refinenet1): FeatureFusionBlock_custom(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit_custom(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): ReLU()\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit_custom(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): ReLU()\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet2): FeatureFusionBlock_custom(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit_custom(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): ReLU()\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit_custom(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): ReLU()\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet3): FeatureFusionBlock_custom(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit_custom(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): ReLU()\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit_custom(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): ReLU()\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet4): FeatureFusionBlock_custom(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit_custom(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): ReLU()\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit_custom(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): ReLU()\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (single_conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (output_conv): Sequential(\n",
       "        (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(32, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_file = \"/workspaces/HIAAC-KR-Dev-Container/Minerva-Dev/docs/notebooks/examples/seismic/facies_classification/f3/dinov2_vits14_pretrain.pth\"\n",
    "\n",
    "from minerva.models.loaders import FromPretrained\n",
    "\n",
    "model = FromPretrained(\n",
    "    model,\n",
    "    ckpt_path=ckpt_file,\n",
    "    strict=False,\n",
    "    keys_to_rename={\"\": \"backbone.\"},\n",
    "    error_on_missing_keys=False\n",
    ")\n",
    "model\n",
    "# model.load_state_dict(ckpt[\"model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log directory set to: /workspaces/HIAAC-KR-Dev-Container/Minerva-Dev/docs/notebooks/examples/seismic/facies_classification/parihaka/logs/dinov2_dpt/seam_ai\n"
     ]
    }
   ],
   "source": [
    "pipeline = get_trainer_pipeline(\n",
    "    model=model,\n",
    "    model_name=model_name,\n",
    "    dataset_name=dataset_name,\n",
    "    log_dir=log_dir,\n",
    "    num_epochs=num_epochs,\n",
    "    accelerator=accelerator,\n",
    "    devices=devices,\n",
    "    is_debug=is_debug,\n",
    "    seed=seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/fabric/loggers/csv_logs.py:268: Experiment logs directory ./logs/dinov2_dpt/seam_ai exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory ./logs/dinov2_dpt/seam_ai/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline info saved at: /workspaces/HIAAC-KR-Dev-Container/Minerva-Dev/docs/notebooks/examples/seismic/facies_classification/parihaka/logs/dinov2_dpt/seam_ai/run_2024-12-10-01-18-44d6cb898cd0a04ae9a46d41172f14dcbf.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name     | Type                  | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | backbone | DinoVisionTransformer | 22.1 M | train\n",
      "1 | head     | DPT                   | 13.6 M | train\n",
      "2 | loss_fn  | CrossEntropyLoss      | 0      | train\n",
      "-----------------------------------------------------------\n",
      "35.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "35.6 M    Total params\n",
      "142.545   Total estimated model params size (MB)\n",
      "301       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 3/3 [00:01<00:00,  3.00it/s, v_num=m_ai]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 3/3 [00:11<00:00,  0.26it/s, v_num=m_ai]\n",
      "Pipeline info saved at: /workspaces/HIAAC-KR-Dev-Container/Minerva-Dev/docs/notebooks/examples/seismic/facies_classification/parihaka/logs/dinov2_dpt/seam_ai/run_2024-12-10-01-18-44d6cb898cd0a04ae9a46d41172f14dcbf.yaml\n"
     ]
    }
   ],
   "source": [
    "pipeline.run(data_module, task=\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at ./logs/dinov2_dpt/seam_ai/checkpoints/last-v2.ckpt\n"
     ]
    }
   ],
   "source": [
    "print(f\"Checkpoint saved at {pipeline.trainer.checkpoint_callback.last_model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
