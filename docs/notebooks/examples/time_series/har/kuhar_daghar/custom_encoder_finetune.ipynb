{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune a Custom Encoder on the DAGHAR Dataset\n",
    "\n",
    "\n",
    "In this notebook, we will fine-tune a custom encoder (previously trained elsewhere) on the DAGHAR KuHAR dataset. The encoder can be **any `torch.nn.Module` that was trained elsewhere** and whose checkpoint is available at a `.ckpt` file (saved using `torch.save(model.state_dict())`). This file may contain only the weights from encoder or may contains the weights of the entire model, including the encoder.\n",
    "\n",
    "To do this, we will first define yor model by:\n",
    "1. Loadig the encoder from the checkpoint, using the `FromPretrained` class. We going to call this encoder as `backbone`.\n",
    "2. Calculating the embedding size of the `backbone`. This is necessary to define the head of the model. This is done by passing a dummy input through the `backbone` and checking the output shape.\n",
    "3. Defining the head of the model. We use a standard MLP classifier as the head, whose input size is the embedding size, followed by a hidden layer of 128 units and an output layer with the number of classes in the dataset, that is, 6.\n",
    "4. Creating a `SimpleSupervisedModel` with the `backbone` and the head. The `SimpleSupervisedModel` is a PyTorch Lightning module that receives the `backbone` and the head as arguments and trains the model end-to-end. This model simply forwards the input through the `backbone`, flattens the output, and passes it through the head to get the logits, whose loss is calculated using the cross-entropy loss.\n",
    "\n",
    "Note that, for sake of reproducibility, you should only change parts related to loading the encoder and defining the head of the model. The rest of the code should remain the same (or at least very similar) to the one provided in this notebook.\n",
    "\n",
    "**Notes:**\n",
    "1. The `backbone` should have a `forward` method that takes a batch of time series as input and returns the output of the encoder (embeddings). Your encoder must accept samples with the shape `(batch_size, channels, steps)`, where `channels` is the number of channels in the time series and `steps` is the number of time steps. For DAGHAR dataset, `channels=6` and `steps=60`. **Thus, your encoder should accept samples with the shape `(batch_size, 6, 60)`.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should install minerva before running this script\n",
    "\n",
    "# !pip install git+https://github.com/discovery-unicamp/Minerva-Dev.git\n",
    "# !pip install kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/_distutils_hack/__init__.py:55: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "from minerva.models.ssl.cpc import CPC\n",
    "from minerva.models.nets.cpc_networks import HARCPCAutoregressive\n",
    "from minerva.data.data_modules.har_rodrigues_24 import HARDataModuleCPC\n",
    "from minerva.pipelines.lightning_pipeline import SimpleLightningPipeline\n",
    "from minerva.models.nets.base import SimpleSupervisedModel\n",
    "\n",
    "from minerva.models.nets.tnc import TSEncoder\n",
    "import torchmetrics\n",
    "\n",
    "from minerva.data.data_modules.har import MultiModalHARSeriesDataModule\n",
    "from minerva.models.loaders import FromPretrained\n",
    "from minerva.models.nets.base import SimpleSupervisedModel\n",
    "from minerva.models.nets.mlp import MLP\n",
    "from minerva.analysis.metrics.balanced_accuracy import BalancedAccuracy\n",
    "from minerva.analysis.model_analysis import TSNEAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution ID: run_20241105-001354\n",
      "Log dir: ./logs/WISDM/run_20241105-001354\n"
     ]
    }
   ],
   "source": [
    "# Name of the experiment\n",
    "execution_id = f'run_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "# Directory to save logs\n",
    "log_dir = f\"./logs/WISDM/{execution_id}\" \n",
    "\n",
    "\n",
    "print(f\"Execution ID: {execution_id}\")\n",
    "print(f\"Log dir: {log_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning a Pre-trained Custom Encoder\n",
    "\n",
    "In this notebook, we will fine-tune a custom encoder on the DAGHAR dataset. The encoder can be any `torch.nn.Module` that was trained elsewhere and whose checkpoint is available at a `.ckpt` file (saved using `torch.save(model.state_dict())`). This file may contain only the weights from encoder or may contains the weights of the entire model, including the encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Defining the Data Module\n",
    "\n",
    "We will use the `MultiModalHARSeriesDataModule` data module to load the DAGHAR dataset for fine-tuning. This data module loads the data in the format required for fine-tuning, which includes sliding window time series data for each sample. Thus, each sample of the dataset will be a 2-element tuple containing the time series (`6x60`, where 6 is the number of features and 60 is the window size) and the corresponding label.\n",
    "\n",
    "The data module requires the following arguments:\n",
    "- `data_path`: Path to the directory containing the dataset (change it to use other dataset from DAGHAR).\n",
    "- `feature_prefix`: The prefix of the columns containing the features. For each prefix, we will create a different channel with all columns that start with the prefix.\n",
    "- `label`: The name of the column containing the labels.\n",
    "- `features_as_channels`: If True, for each prefix, we will create a different channel with all columns that start with the prefix. If False, we will concatenate all columns with the same prefix into a single channel (the sample will be a tensor of 1x360 instead of 6x60).\n",
    "- `cast_to`: The data type to cast the features (float32).\n",
    "- `batch_size`: The batch size for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiModalHARSeriesDataModule(data_path=[PosixPath('/workspaces/container-workspace/standardized_view/WISDM')], batch_size=64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module = MultiModalHARSeriesDataModule(\n",
    "    data_path=\"/workspaces/container-workspace/standardized_view/WISDM\",\n",
    "    feature_prefixes=[\"accel-x\", \"accel-y\", \"accel-z\", \"gyro-x\", \"gyro-y\", \"gyro-z\"],\n",
    "    label=\"standard activity code\",\n",
    "    features_as_channels=True,\n",
    "    cast_to=\"float32\",\n",
    "    batch_size=64,\n",
    "    num_workers=1,\n",
    ")\n",
    "\n",
    "data_module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Defining the Fine-tuning Model\n",
    "\n",
    "We first going to create the `backbone` of our model, that is our encoder. To do this, we:\n",
    "1. Copy-and-paste the code of the encoder in the cell below. Should be a `torch.nn.Module` or equivalent. You should copy all code necessary to define the encoder, including imports, class definition, the `forward` method, and any other methods or classes that are necessary to define the encoder.\n",
    "2. Load the weights of the encoder from the checkpoint file using the `FromPretrained` class. This class receives the path to the checkpoint file and the model class and loads the weights from the checkpoint to the model. The `FromPretrained` class is a simple wrapper around the `torch.load` function that loads the weights from the checkpoint to the model.\n",
    "\n",
    "**NOTE**: \n",
    "1. For sake of tutorial, we will create a dummy encoder and save the random weights to a checkpoint file. You should replace the dummy encoder with your encoder and the checkpoint file with the file containing the weights of your encoder.\n",
    "2. Your encoder **must accept samples with the shape `(batch_size, 6, 60)`**. This is the shape of the samples in the DAGHAR dataset. If your encoder does not accept samples with this shape, you should modify it to accept samples with this shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Definition*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "#from einops import rearrange, reduce, repeat\n",
    "#from einops.layers.torch import Rearrange, Reduce\n",
    "from torchsummary import summary   \n",
    "        \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        self.keys = nn.Linear(emb_size, emb_size)\n",
    "        self.queries = nn.Linear(emb_size, emb_size)\n",
    "        self.values = nn.Linear(emb_size, emb_size)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "\n",
    "    def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
    "        '''\n",
    "        queries = rearrange(self.queries(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        keys = rearrange(self.keys(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        values = rearrange(self.values(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        '''\n",
    "        queries = self.rearrange_tensor(self.queries(x),  num_heads=self.num_heads)\n",
    "        keys = self.rearrange_tensor(self.keys(x), num_heads=self.num_heads)\n",
    "        values = self.rearrange_tensor(self.values(x),  num_heads=self.num_heads)\n",
    "\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)  # batch, num_heads, query_len, key_len\n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "\n",
    "        scaling = self.emb_size ** (1 / 2)\n",
    "        att = F.softmax(energy / scaling, dim=-1)\n",
    "        att = self.att_drop(att)\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "        #print(f'out shape: {out.shape}')\n",
    "        #out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        b, h, n, d = out.shape\n",
    "        out = out.permute(0, 2, 1, 3).reshape(b, n, h * d)\n",
    "        #print(f'out final shape: {out.shape}')\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "    \n",
    "    def rearrange_tensor(self, x: Tensor, num_heads: int) -> Tensor:\n",
    "        \"\"\"\n",
    "        Rearrange tensor from shape (b, n, h * d) to (b, h, n, d).\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor with shape (b, n, h * d).\n",
    "            num_heads (int): Number of heads (h).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Rearranged tensor with shape (b, h, n, d).\n",
    "        \"\"\"\n",
    "        b, n, hd = x.shape\n",
    "        d = hd // num_heads  # Calcula a dimensão `d` dividindo pela quantidade de cabeças\n",
    "\n",
    "        # Redimensiona para (b, n, h, d) e reorganiza para (b, h, n, d)\n",
    "        return x.view(b, n, num_heads, d).permute(0, 2, 1, 3)\n",
    "\n",
    "    \n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size, expansion, drop_p):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )\n",
    "\n",
    "        \n",
    "        \n",
    "class Dis_TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size=100,\n",
    "                 num_heads=5,\n",
    "                 drop_p=0.,\n",
    "                 forward_expansion=4,\n",
    "                 forward_drop_p=0.):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                MultiHeadAttention(emb_size, num_heads, drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                FeedForwardBlock(\n",
    "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )\n",
    "            ))\n",
    "\n",
    "\n",
    "class Dis_TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth=8, **kwargs):\n",
    "        super().__init__(*[Dis_TransformerEncoderBlock(**kwargs) for _ in range(depth)])\n",
    "        \n",
    "\n",
    "class ReduceLayer(nn.Module):\n",
    "    def __init__(self, reduction: str = 'mean'):\n",
    "        super(ReduceLayer, self).__init__()\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        if self.reduction == 'mean':\n",
    "            return x.mean(dim=1)  # Reduz ao longo da dimensão `n`, resultando em (b, e)\n",
    "        elif self.reduction == 'sum':\n",
    "            return x.sum(dim=1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported reduction type: {self.reduction}\")\n",
    "    \n",
    "class PatchEmbedding_Linear(nn.Module):\n",
    "    #what are the proper parameters set here?\n",
    "    def __init__(self, in_channels = 21, patch_size = 16, emb_size = 100, seq_len = 1024):\n",
    "        # self.patch_size = patch_size\n",
    "        super().__init__()\n",
    "        #change the conv2d parameters here\n",
    "        '''\n",
    "        self.projection = nn.Sequential(\n",
    "            Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)',s1 = 1, s2 = patch_size),\n",
    "            nn.Linear(patch_size*in_channels, emb_size))\n",
    "        '''\n",
    "        self.projection = nn.Sequential(\n",
    "            RearrangeLayer(patch_size=patch_size, s1=1),  # Substitui o Rearrange do einops\n",
    "            nn.Linear(patch_size * in_channels, emb_size))\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
    "        self.positions = nn.Parameter(torch.randn((seq_len // patch_size) + 1, emb_size))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = x.unsqueeze(dim=2)\n",
    "        b, _, _, _ = x.shape\n",
    "        x = self.projection(x)\n",
    "        \n",
    "        #cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n",
    "        cls_tokens = self.cls_token.repeat(b, 1, 1) #Personal repeat from pytorch to transfer from einops\n",
    "\n",
    "        #prepend the cls token to the input\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        # position\n",
    "        x += self.positions\n",
    "        return x     \n",
    "    \n",
    "    #For use in change of Rearrange einops layer\n",
    "class RearrangeLayer(nn.Module):\n",
    "    def __init__(self, patch_size: int, s1: int = 1):\n",
    "        super(RearrangeLayer, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.s1 = s1\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        b, c, h_s1, w_s2 = x.shape\n",
    "        h, s1 = h_s1, self.s1\n",
    "        w, s2 = w_s2 // self.patch_size, self.patch_size\n",
    "        \n",
    "        # Rearrange tensor\n",
    "        x = x.view(b, c, h, s1, w, s2)  # shape: (b, c, h, s1, w, s2)\n",
    "        x = x.permute(0, 2, 4, 3, 5, 1)  # shape: (b, h, w, s1, s2, c)\n",
    "        x = x.reshape(b, h * w, s1 * s2 * c)  # shape: (b, h * w, s1 * s2 * c)  \n",
    "\n",
    "        return x\n",
    "        \n",
    "class Encoder(nn.Sequential):\n",
    "    def __init__(self, \n",
    "                 in_channels=3,\n",
    "                 patch_size=15,\n",
    "                 emb_size=50, \n",
    "                 seq_len = 150,\n",
    "                 depth=3,  \n",
    "                 **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbedding_Linear(in_channels, patch_size, emb_size, seq_len),\n",
    "            Dis_TransformerEncoder(depth, emb_size=emb_size, drop_p=0.5, forward_drop_p=0.5, **kwargs),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLACE THIS WITH YOUR ENCODER CHECKPOINT PATH\n",
    "ckpt_path = \"/workspaces/container-workspace/tts-gan/pre-trained-models/daghar_all_dataset/seq_len_60/daghar_all_50000_6axis_60_100/UCI_DAGHAR_Multiclass_50000_D_60_6axis_2024_11_03_22_59_20/Model/checkpoint\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should comment the cell below. It is only use to save the dummy encoder to a checkpoint file. When you replace it with your encoder, you should remove this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.save(DummyEncoder().state_dict(), ckpt_path)\\nprint(f\"Encoder saved at \\'{ckpt_path}\\'\")\\nprint(\"You should remove this torch.save for the final version of your code\")\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COMMENT THIS CELL! It is only here to illustrate a chekpoint. YOU SHOULD USE YOUR OWN ENCODER CHECKPOINT\n",
    "'''\n",
    "torch.save(DummyEncoder().state_dict(), ckpt_path)\n",
    "print(f\"Encoder saved at '{ckpt_path}'\")\n",
    "print(\"You should remove this torch.save for the final version of your code\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiating the Encoder and Loading it from Checkpoint*\n",
    "\n",
    "We will load the encoder from the checkpoint file using the `FromPretrained` class. This class receives the following parameters:\n",
    "- `model`: is a instance of the model class with random weights. We going to replace the weights of this model with the weights from the checkpoint.\n",
    "- `ckpt_path`: is the path to the checkpoint file containing the weights of the encoder.\n",
    "- `filter_keys`: optional parameter that can be used to filter the keys of the checkpoint file. If provided, only the keys that contain the filter keys will be loaded to the model. This is useful when the checkpoint file contains the weights of the entire model and we want to load only the weights of the encoder.\n",
    "- `strict`: if True, the keys of the checkpoint file must match the keys of the model exactly. If False, the keys of the checkpoint file must contain the keys of the model. We usually set this parameter to False to allow loading only the weights of the encoder.\n",
    "- `keys_to_rename`: a dictionary that maps the keys of the checkpoint file to the keys of the model. This is useful when the keys of the checkpoint file do not match the keys of the model. We can rename the keys of the checkpoint file to match the keys of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from /workspaces/container-workspace/tts-gan/pre-trained-models/daghar_all_dataset/seq_len_60/daghar_all_50000_6axis_60_100/UCI_DAGHAR_Multiclass_50000_D_60_6axis_2024_11_03_22_59_20/Model/checkpoint: _IncompatibleKeys(missing_keys=[], unexpected_keys=['2.clshead.1.weight', '2.clshead.1.bias', '2.clshead.2.weight', '2.clshead.2.bias'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/container-workspace/Minerva-Dev/minerva/models/loaders.py:179: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (0): PatchEmbedding_Linear(\n",
       "    (projection): Sequential(\n",
       "      (0): RearrangeLayer()\n",
       "      (1): Linear(in_features=90, out_features=50, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (1): Dis_TransformerEncoder(\n",
       "    (0): Dis_TransformerEncoderBlock(\n",
       "      (0): ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): MultiHeadAttention(\n",
       "            (keys): Linear(in_features=50, out_features=50, bias=True)\n",
       "            (queries): Linear(in_features=50, out_features=50, bias=True)\n",
       "            (values): Linear(in_features=50, out_features=50, bias=True)\n",
       "            (att_drop): Dropout(p=0.5, inplace=False)\n",
       "            (projection): Linear(in_features=50, out_features=50, bias=True)\n",
       "          )\n",
       "          (2): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): FeedForwardBlock(\n",
       "            (0): Linear(in_features=50, out_features=200, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.5, inplace=False)\n",
       "            (3): Linear(in_features=200, out_features=50, bias=True)\n",
       "          )\n",
       "          (2): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Dis_TransformerEncoderBlock(\n",
       "      (0): ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): MultiHeadAttention(\n",
       "            (keys): Linear(in_features=50, out_features=50, bias=True)\n",
       "            (queries): Linear(in_features=50, out_features=50, bias=True)\n",
       "            (values): Linear(in_features=50, out_features=50, bias=True)\n",
       "            (att_drop): Dropout(p=0.5, inplace=False)\n",
       "            (projection): Linear(in_features=50, out_features=50, bias=True)\n",
       "          )\n",
       "          (2): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): FeedForwardBlock(\n",
       "            (0): Linear(in_features=50, out_features=200, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.5, inplace=False)\n",
       "            (3): Linear(in_features=200, out_features=50, bias=True)\n",
       "          )\n",
       "          (2): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Dis_TransformerEncoderBlock(\n",
       "      (0): ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): MultiHeadAttention(\n",
       "            (keys): Linear(in_features=50, out_features=50, bias=True)\n",
       "            (queries): Linear(in_features=50, out_features=50, bias=True)\n",
       "            (values): Linear(in_features=50, out_features=50, bias=True)\n",
       "            (att_drop): Dropout(p=0.5, inplace=False)\n",
       "            (projection): Linear(in_features=50, out_features=50, bias=True)\n",
       "          )\n",
       "          (2): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): FeedForwardBlock(\n",
       "            (0): Linear(in_features=50, out_features=200, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.5, inplace=False)\n",
       "            (3): Linear(in_features=200, out_features=50, bias=True)\n",
       "          )\n",
       "          (2): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiating encoder (random weights)\n",
    "backbone = Encoder(in_channels=6, seq_len=60)\n",
    "\n",
    "# Loading encoder from checkpoint\n",
    "backbone = FromPretrained(\n",
    "    model=backbone,\n",
    "    ckpt_path=ckpt_path,\n",
    "    strict=False,\n",
    "    ckpt_key='dis_state_dict'\n",
    ")\n",
    "backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the head\n",
    "\n",
    "We will define the head of the model. The head is a simple MLP classifier that receives the embeddings from the encoder and outputs the logits. The head consists of a single hidden layer with 128 units and an output layer with the number of classes in the dataset, that is, 6.\n",
    "In order to know the input size of the head, we need to calculate the embedding size of the encoder. This is done by passing a dummy input through the encoder and checking the output shape. Let's pick the first batch of the training data and pass it through the encoder to get the embedding size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O primeiro batch de treino tem shape X=(64, 6, 60) e y=(64,)\n"
     ]
    }
   ],
   "source": [
    "# Pega os dataloader de treino\n",
    "data_module.setup(\"fit\")\n",
    "train_data_loader = data_module.train_dataloader()\n",
    "\n",
    "# Obtem o primeiro batch de treino (64 amostras de 6x60)\n",
    "first_batch = next(iter(train_data_loader))\n",
    "\n",
    "X, y = first_batch\n",
    "print(f\"O primeiro batch de treino tem shape X={tuple(X.shape)} e y={tuple(y.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We made a single forward pass through the encoder with the first batch of the training data and checked the output shape that is `(64, 16, 60)`. The embedding size is `16 x 30 = 480`, that is, the product of the number of channels and the number of steps in the output of the encoder. Thus, the input layer of our MLP head will have 480 units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O embedding tem shape (64, 5, 50)\n",
      "O embedding final, achatado, tem o shape de 250\n"
     ]
    }
   ],
   "source": [
    "embeddings = backbone(X)\n",
    "mlp_input_shape = embeddings.shape[1] * embeddings.shape[2]\n",
    "print(f\"O embedding tem shape {tuple(embeddings.shape)}\")\n",
    "print(f\"O embedding final, achatado, tem o shape de {mlp_input_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the head of the model in the cell below. The head will be a simple MLP classifier with:\n",
    "- A input layer with 480 units (`mlp_input_shape` variable).\n",
    "- A hidden layer with 128 units.\n",
    "- An output layer with 6 units, that is the number of classes in the dataset (`num_classes` variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (0): Linear(in_features=250, out_features=128, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=128, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = 6\n",
    "head = MLP([mlp_input_shape, 128, num_classes])\n",
    "head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the SimpleSupervisedModel\n",
    "\n",
    "Finally, we use the `SimpleSupervisedModel` class to create the supervised model with the encoder as the backbone and the MLP classifier as the head. The `SimpleSupervisedModel` class requires the following arguments:\n",
    "- `backbone`: The backbone model (`FromPretrained` model).\n",
    "- `fc`: The head model (the MLP classifier).\n",
    "- `loss_fn`: The loss function to use for training (`CrossEntropyLoss`).\n",
    "- `flatten`: Whether to flatten the input before passing it through the head. Usually, the input is flattened if the backbone outputs a tensor with more than two dimensions.\n",
    "- `train_metrics`: A dictionary where the keys are the names of the metrics and the values are the functions that calculate the metrics. The metrics function should use `torchmetrics` API to calculate the metrics.\n",
    "- `val_metrics`: A dictionary where the keys are the names of the metrics and the values are the functions that calculate the metrics. The metrics function should use `torchmetrics` API to calculate the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleSupervisedModel(\n",
       "  (backbone): Encoder(\n",
       "    (0): PatchEmbedding_Linear(\n",
       "      (projection): Sequential(\n",
       "        (0): RearrangeLayer()\n",
       "        (1): Linear(in_features=90, out_features=50, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Dis_TransformerEncoder(\n",
       "      (0): Dis_TransformerEncoderBlock(\n",
       "        (0): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): MultiHeadAttention(\n",
       "              (keys): Linear(in_features=50, out_features=50, bias=True)\n",
       "              (queries): Linear(in_features=50, out_features=50, bias=True)\n",
       "              (values): Linear(in_features=50, out_features=50, bias=True)\n",
       "              (att_drop): Dropout(p=0.5, inplace=False)\n",
       "              (projection): Linear(in_features=50, out_features=50, bias=True)\n",
       "            )\n",
       "            (2): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForwardBlock(\n",
       "              (0): Linear(in_features=50, out_features=200, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.5, inplace=False)\n",
       "              (3): Linear(in_features=200, out_features=50, bias=True)\n",
       "            )\n",
       "            (2): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Dis_TransformerEncoderBlock(\n",
       "        (0): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): MultiHeadAttention(\n",
       "              (keys): Linear(in_features=50, out_features=50, bias=True)\n",
       "              (queries): Linear(in_features=50, out_features=50, bias=True)\n",
       "              (values): Linear(in_features=50, out_features=50, bias=True)\n",
       "              (att_drop): Dropout(p=0.5, inplace=False)\n",
       "              (projection): Linear(in_features=50, out_features=50, bias=True)\n",
       "            )\n",
       "            (2): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForwardBlock(\n",
       "              (0): Linear(in_features=50, out_features=200, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.5, inplace=False)\n",
       "              (3): Linear(in_features=200, out_features=50, bias=True)\n",
       "            )\n",
       "            (2): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): Dis_TransformerEncoderBlock(\n",
       "        (0): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): MultiHeadAttention(\n",
       "              (keys): Linear(in_features=50, out_features=50, bias=True)\n",
       "              (queries): Linear(in_features=50, out_features=50, bias=True)\n",
       "              (values): Linear(in_features=50, out_features=50, bias=True)\n",
       "              (att_drop): Dropout(p=0.5, inplace=False)\n",
       "              (projection): Linear(in_features=50, out_features=50, bias=True)\n",
       "            )\n",
       "            (2): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForwardBlock(\n",
       "              (0): Linear(in_features=50, out_features=200, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.5, inplace=False)\n",
       "              (3): Linear(in_features=200, out_features=50, bias=True)\n",
       "            )\n",
       "            (2): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): MLP(\n",
       "    (0): Linear(in_features=250, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=6, bias=True)\n",
       "  )\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SimpleSupervisedModel(\n",
    "    backbone=backbone,\n",
    "    fc=head,\n",
    "    loss_fn=torch.nn.CrossEntropyLoss(),\n",
    "    flatten=True,\n",
    "    train_metrics={\n",
    "        \"acc\": torchmetrics.Accuracy(task=\"multiclass\", num_classes=6),\n",
    "    },\n",
    "    val_metrics={\n",
    "        \"acc\": torchmetrics.Accuracy(task=\"multiclass\", num_classes=6),\n",
    "    },\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Defining the Pytorch Lightning Trainer Configuration\n",
    "\n",
    "We will define the PyTorch Lightning Trainer configuration for fine-tuning the model.\n",
    "\n",
    "The trainer configuration includes the following parameters:\n",
    "- `max_epochs`: The maximum number of epochs to train the model.\n",
    "- `accelerator`: The accelerator to use for training: `cpu`, `gpu`, or `tpu`.\n",
    "- `devices`: The number of accelerators to use for training.\n",
    "- `logger`: The logger to use for logging the training progress.\n",
    "- `limit_*_batches`: The number of batches to use for training and validation. **This is useful for debugging and testing the model.**. You should remove these parameters when training the model for real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lightning.pytorch.trainer.trainer.Trainer at 0x7f71a4fb42b0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath='checkpoints/',\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_last=True\n",
    ")\n",
    "\n",
    "## Logger\n",
    "logger = CSVLogger(save_dir=log_dir, name='tts-finetune', version=execution_id)\n",
    "\n",
    "## Trainer\n",
    "trainer = L.Trainer(\n",
    "    # Maximum number of epochs to train\n",
    "    max_epochs=100,\n",
    "    # Training on GPU\n",
    "    accelerator=\"gpu\",\n",
    "    # We will train using 1 gpu\n",
    "    devices=1,\n",
    "    # Logger for logging\n",
    "    logger=logger,\n",
    "    # List of callbacks\n",
    "    callbacks=[checkpoint_callback],\n",
    "    # Only for testing. Remove for production. We will only train using 1 batch of training and validation\n",
    "    #limit_train_batches=1,\n",
    "    #limit_val_batches=1,\n",
    ")\n",
    "\n",
    "trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Creating the fine-tuning pipeline (and running the training)\n",
    "\n",
    "We will create a `SimpleLightningPipeline` to fine-tune the model. This pipeline receives the following arguments:\n",
    "- `model`: The model to train.\n",
    "- `trainer`: The PyTorch Lightning Trainer to use for training.\n",
    "- `log_dir`: The directory to save the logs, checkpoints, and other artifacts of the training process.\n",
    "- `save_run_status`: If True, save the status of the run to the log directory. This is useful for reprodutibility purposes.\n",
    "- `seed`: The seed to use for random number generators in PyTorch, NumPy, and other libraries.\n",
    "\n",
    "This pipeline is optional, as user can simply use `trainer.fit(model, datamodule)` directly to train the model. However, the pipeline provides a more organized way to train the model and save required information for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log directory set to: /workspaces/container-workspace/Minerva-Dev/docs/notebooks/examples/time_series/har/kuhar_daghar/logs/WISDM/run_20241105-001354\n"
     ]
    }
   ],
   "source": [
    "train_pipeline = SimpleLightningPipeline(\n",
    "    model=model,\n",
    "    trainer=trainer,\n",
    "    log_dir=log_dir,\n",
    "    save_run_status=True,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the pipeline to fine-tune the model on the DAGHAR dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline info saved at: /workspaces/container-workspace/Minerva-Dev/docs/notebooks/examples/time_series/har/kuhar_daghar/logs/WISDM/run_20241105-001354/run_2024-11-05-00-13-55c8e4cc692fa441b3b7e15a188007a8e0.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /workspaces/container-workspace/Minerva-Dev/docs/notebooks/examples/time_series/har/kuhar_daghar/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | backbone | Encoder          | 96.8 K | train\n",
      "1 | fc       | MLP              | 32.9 K | train\n",
      "2 | loss_fn  | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "129 K     Trainable params\n",
      "0         Non-trainable params\n",
      "129 K     Total params\n",
      "0.519     Total estimated model params size (MB)\n",
      "71        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=5` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=5` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 136/136 [00:01<00:00, 77.19it/s, v_num=1354, val_loss=0.381, val_acc=0.850, train_loss=0.169, train_acc=0.928]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 136/136 [00:01<00:00, 73.55it/s, v_num=1354, val_loss=0.381, val_acc=0.850, train_loss=0.169, train_acc=0.928]\n",
      "Pipeline info saved at: /workspaces/container-workspace/Minerva-Dev/docs/notebooks/examples/time_series/har/kuhar_daghar/logs/WISDM/run_20241105-001354/run_2024-11-05-00-13-55c8e4cc692fa441b3b7e15a188007a8e0.yaml\n"
     ]
    }
   ],
   "source": [
    "train_pipeline.run(data_module, task=\"fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Fine-tuned Model\n",
    "\n",
    "After fine-tuning the encoder on the DAGHAR dataset, we will evaluate the model's performance on the test set of the same dataset. We create a simple evaluation pipeline, that will:\n",
    "1. Run forward on test set.\n",
    "2. Calculcate the metrics. This is specified in the `classification_metrics` dictionary, where the keys are the names of the metrics and the values are the functions that calculate the metrics. The metrics function should use `torchmetrics` API to calculate the metrics.\n",
    "3. Perform model analysis, such as plot t-sne embeddings.\n",
    "\n",
    "The test pipeline requires the following arguments:\n",
    "- `model`: The fine-tuned model to evaluate.\n",
    "- `trainer`: The PyTorch Lightning Trainer configuration.\n",
    "- `log_dir`: The directory to save the evaluation logs.\n",
    "- `seed`: The random seed for reproducibility.\n",
    "- `classification_metrics`: A dictionary where the keys are the names of the metrics and the values are the functions that calculate the metrics. The metrics function should use `torchmetrics` API to calculate the metrics.\n",
    "- `model_analysis`: A function that performs model analysis, such as plotting t-sne embeddings.\n",
    "\n",
    "Finally, we run the evaluation pipeline with the test data module to evaluate the model on the test set. We set the `task` parameter of the `run` method to `evaluate` to evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log directory set to: /workspaces/container-workspace/Minerva-Dev/docs/notebooks/examples/time_series/har/kuhar_daghar/logs/WISDM/run_20241105-001354\n"
     ]
    }
   ],
   "source": [
    "test_pipeline = SimpleLightningPipeline(\n",
    "    model=model,\n",
    "    trainer=trainer,\n",
    "    log_dir=log_dir,\n",
    "    save_run_status=True,\n",
    "    seed=42,\n",
    "    classification_metrics={\n",
    "        \"accuracy\": torchmetrics.Accuracy(num_classes=6, task=\"multiclass\"),\n",
    "        \"f1\": torchmetrics.F1Score(num_classes=6, task=\"multiclass\"),\n",
    "        \"precision\": torchmetrics.Precision(num_classes=6, task=\"multiclass\"),\n",
    "        \"recall\": torchmetrics.Recall(num_classes=6, task=\"multiclass\"),\n",
    "        \"balanced_accuracy\": BalancedAccuracy(num_classes=6, task=\"multiclass\"),\n",
    "    },\n",
    "    apply_metrics_per_sample=False,\n",
    "      model_analysis={\n",
    "         \"tsne\": TSNEAnalysis(\n",
    "             height=800,\n",
    "             width=800,\n",
    "             legend_title=\"Activity\",\n",
    "             title=\"t-SNE of CPC Finetuned on KuHar\",\n",
    "             output_filename=\"tsne_cpc_finetuned_kuhar.pdf\",\n",
    "             label_names={\n",
    "                 0: \"sit\",\n",
    "                 1: \"stand\",\n",
    "                 2: \"walk\",\n",
    "                 3: \"stair up\",\n",
    "                 4: \"stair down\",\n",
    "                 5: \"run\",\n",
    "                 6: \"stair up and down\",\n",
    "             },\n",
    "         )\n",
    "     },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline info saved at: /workspaces/container-workspace/Minerva-Dev/docs/notebooks/examples/time_series/har/kuhar_daghar/logs/WISDM/run_20241105-001354/run_2024-11-05-00-17-05c2ecbfb919bd4da5ba3227e13f929560.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /workspaces/container-workspace/Minerva-Dev/docs/notebooks/examples/time_series/har/kuhar_daghar/checkpoints/epoch=46-step=6392.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /workspaces/container-workspace/Minerva-Dev/docs/notebooks/examples/time_series/har/kuhar_daghar/checkpoints/epoch=46-step=6392.ckpt\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=5` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 41/41 [00:00<00:00, 299.08it/s]\n",
      "Running classification metrics...\n",
      "Running model analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/container-workspace/Minerva-Dev/minerva/analysis/metrics/balanced_accuracy.py:58: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "/workspaces/container-workspace/Minerva-Dev/minerva/analysis/model_analysis.py:63: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  X = torch.tensor(X, device=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-SNE plot saved to /workspaces/container-workspace/Minerva-Dev/docs/notebooks/examples/time_series/har/kuhar_daghar/logs/WISDM/run_20241105-001354/tsne_cpc_finetuned_kuhar.pdf\n",
      "{'classification': {'accuracy': [0.8629032373428345], 'f1': [0.8629032373428345], 'precision': [0.8629032373428345], 'recall': [0.8629032373428345], 'balanced_accuracy': [0.8630374670028687]}, 'analysis': {'tsne': '/workspaces/container-workspace/Minerva-Dev/docs/notebooks/examples/time_series/har/kuhar_daghar/logs/WISDM/run_20241105-001354/tsne_cpc_finetuned_kuhar.pdf'}}\n",
      "Metrics saved to /workspaces/container-workspace/Minerva-Dev/docs/notebooks/examples/time_series/har/kuhar_daghar/logs/WISDM/run_20241105-001354/metrics_2024-11-05-00-17-05c2ecbfb919bd4da5ba3227e13f929560.yaml\n",
      "Pipeline info saved at: /workspaces/container-workspace/Minerva-Dev/docs/notebooks/examples/time_series/har/kuhar_daghar/logs/WISDM/run_20241105-001354/run_2024-11-05-00-17-05c2ecbfb919bd4da5ba3227e13f929560.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classification': {'accuracy': [0.8629032373428345],\n",
       "  'f1': [0.8629032373428345],\n",
       "  'precision': [0.8629032373428345],\n",
       "  'recall': [0.8629032373428345],\n",
       "  'balanced_accuracy': [0.8630374670028687]},\n",
       " 'analysis': {'tsne': '/workspaces/container-workspace/Minerva-Dev/docs/notebooks/examples/time_series/har/kuhar_daghar/logs/WISDM/run_20241105-001354/tsne_cpc_finetuned_kuhar.pdf'}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d= test_pipeline.run(\n",
    "    data_module, task=\"evaluate\", ckpt_path=checkpoint_callback.best_model_path\n",
    ")\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UCI {'accuracy': [0.8579710125923157], 'balanced_accuracy': [0.8582032322883606], 'f1': [0.8579710125923157], 'precision': [0.8579710125923157], 'recall': [0.8579710125923157]}\n",
      "UCI {'accuracy': [0.834782600402832], 'balanced_accuracy': [0.8668681979179382], 'f1': [0.834782600402832], 'precision': [0.834782600402832], 'recall': [0.834782600402832]}\n",
      "UCI {'accuracy': [0.8550724387168884], 'balanced_accuracy': [0.8557836413383484], 'f1': [0.8550724387168884], 'precision': [0.8550724387168884], 'recall': [0.8550724387168884]}\n",
      "UCI {'accuracy': [0.8637681007385254], 'balanced_accuracy': [0.8662659525871277], 'f1': [0.8637681007385254], 'precision': [0.8637681007385254], 'recall': [0.8637681007385254]}\n",
      "UCI {'accuracy': [0.8710144758224487], 'balanced_accuracy': [0.8748836517333984], 'f1': [0.8710144758224487], 'precision': [0.8710144758224487], 'recall': [0.8710144758224487]}\n",
      "WISDM {'accuracy': [0.8579109311103821], 'balanced_accuracy': [0.8586177229881287], 'f1': [0.8579109311103821], 'precision': [0.8579109311103821], 'recall': [0.8579109311103821]}\n",
      "WISDM {'accuracy': [0.849078357219696], 'balanced_accuracy': [0.8539758324623108], 'f1': [0.849078357219696], 'precision': [0.849078357219696], 'recall': [0.849078357219696]}\n",
      "WISDM {'accuracy': [0.8629032373428345], 'balanced_accuracy': [0.8630374670028687], 'f1': [0.8629032373428345], 'precision': [0.8629032373428345], 'recall': [0.8629032373428345]}\n",
      "WISDM {'accuracy': [0.8529185652732849], 'balanced_accuracy': [0.8533471822738647], 'f1': [0.8529185652732849], 'precision': [0.8529185652732849], 'recall': [0.8529185652732849]}\n",
      "WISDM {'accuracy': [0.867511510848999], 'balanced_accuracy': [0.8678888082504272], 'f1': [0.867511510848999], 'precision': [0.867511510848999], 'recall': [0.867511510848999]}\n",
      "realworld_waist {'accuracy': [0.6905864477157593], 'balanced_accuracy': [0.7353320717811584], 'f1': [0.6905864477157593], 'precision': [0.6905864477157593], 'recall': [0.6905864477157593]}\n",
      "realworld_waist {'accuracy': [0.7091049551963806], 'balanced_accuracy': [0.7721161246299744], 'f1': [0.7091049551963806], 'precision': [0.7091049551963806], 'recall': [0.7091049551963806]}\n",
      "realworld_waist {'accuracy': [0.6979166865348816], 'balanced_accuracy': [0.7348610758781433], 'f1': [0.6979166865348816], 'precision': [0.6979166865348816], 'recall': [0.6979166865348816]}\n",
      "realworld_waist {'accuracy': [0.6570215821266174], 'balanced_accuracy': [0.7116362452507019], 'f1': [0.6570215821266174], 'precision': [0.6570215821266174], 'recall': [0.6570215821266174]}\n",
      "realworld_waist {'accuracy': [0.7399691343307495], 'balanced_accuracy': [0.7695116996765137], 'f1': [0.7399691343307495], 'precision': [0.7399691343307495], 'recall': [0.7399691343307495]}\n",
      "realworld_thigh {'accuracy': [0.6766735911369324], 'balanced_accuracy': [0.6610706448554993], 'f1': [0.6766735911369324], 'precision': [0.6766735911369324], 'recall': [0.6766735911369324]}\n",
      "realworld_thigh {'accuracy': [0.6535542011260986], 'balanced_accuracy': [0.6534743905067444], 'f1': [0.6535542011260986], 'precision': [0.6535542011260986], 'recall': [0.6535542011260986]}\n",
      "realworld_thigh {'accuracy': [0.6611456274986267], 'balanced_accuracy': [0.65800541639328], 'f1': [0.6611456274986267], 'precision': [0.6611456274986267], 'recall': [0.6611456274986267]}\n",
      "realworld_thigh {'accuracy': [0.6511387228965759], 'balanced_accuracy': [0.666816234588623], 'f1': [0.6511387228965759], 'precision': [0.6511387228965759], 'recall': [0.6511387228965759]}\n",
      "realworld_thigh {'accuracy': [0.669427216053009], 'balanced_accuracy': [0.6623452305793762], 'f1': [0.669427216053009], 'precision': [0.669427216053009], 'recall': [0.669427216053009]}\n",
      "motionsense {'accuracy': [0.8822975754737854], 'balanced_accuracy': [0.8891255259513855], 'f1': [0.8822975754737854], 'precision': [0.8822975754737854], 'recall': [0.8822975754737854]}\n",
      "motionsense {'accuracy': [0.8898305296897888], 'balanced_accuracy': [0.8915204405784607], 'f1': [0.8898305296897888], 'precision': [0.8898305296897888], 'recall': [0.8898305296897888]}\n",
      "motionsense {'accuracy': [0.8832391500473022], 'balanced_accuracy': [0.886753499507904], 'f1': [0.8832391500473022], 'precision': [0.8832391500473022], 'recall': [0.8832391500473022]}\n",
      "motionsense {'accuracy': [0.887005627155304], 'balanced_accuracy': [0.8886788487434387], 'f1': [0.887005627155304], 'precision': [0.887005627155304], 'recall': [0.887005627155304]}\n",
      "motionsense {'accuracy': [0.8832391500473022], 'balanced_accuracy': [0.886864185333252], 'f1': [0.8832391500473022], 'precision': [0.8832391500473022], 'recall': [0.8832391500473022]}\n",
      "kuhar {'accuracy': [0.5416666865348816], 'balanced_accuracy': [0.5459781289100647], 'f1': [0.5416666865348816], 'precision': [0.5416666865348816], 'recall': [0.5416666865348816]}\n",
      "kuhar {'accuracy': [0.5833333134651184], 'balanced_accuracy': [0.59039705991745], 'f1': [0.5833333134651184], 'precision': [0.5833333134651184], 'recall': [0.5833333134651184]}\n",
      "kuhar {'accuracy': [0.5694444179534912], 'balanced_accuracy': [0.5853849053382874], 'f1': [0.5694444179534912], 'precision': [0.5694444179534912], 'recall': [0.5694444179534912]}\n",
      "kuhar {'accuracy': [0.5694444179534912], 'balanced_accuracy': [0.6074718236923218], 'f1': [0.5694444179534912], 'precision': [0.5694444179534912], 'recall': [0.5694444179534912]}\n",
      "kuhar {'accuracy': [0.5902777910232544], 'balanced_accuracy': [0.531099796295166], 'f1': [0.5902777910232544], 'precision': [0.5902777910232544], 'recall': [0.5902777910232544]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_00081 th {\n",
       "  background-color: #f2f2f2;\n",
       "  color: black;\n",
       "  border-color: gray;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_00081 tr:hover {\n",
       "  background-color: #f9f9f9;\n",
       "}\n",
       "#T_00081_row0_col0, #T_00081_row0_col1, #T_00081_row0_col2, #T_00081_row0_col3, #T_00081_row0_col4, #T_00081_row1_col0, #T_00081_row1_col1, #T_00081_row1_col2, #T_00081_row1_col3, #T_00081_row1_col4, #T_00081_row2_col0, #T_00081_row2_col1, #T_00081_row2_col2, #T_00081_row2_col3, #T_00081_row2_col4, #T_00081_row3_col0, #T_00081_row3_col1, #T_00081_row3_col2, #T_00081_row3_col3, #T_00081_row3_col4, #T_00081_row4_col0, #T_00081_row4_col1, #T_00081_row4_col2, #T_00081_row4_col3, #T_00081_row4_col4, #T_00081_row5_col0, #T_00081_row5_col1, #T_00081_row5_col2, #T_00081_row5_col3, #T_00081_row5_col4 {\n",
       "  background-color: white;\n",
       "  color: black;\n",
       "  border-color: lightgray;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_00081\">\n",
       "  <caption>Metricas de Performance por Dataset</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_00081_level0_col0\" class=\"col_heading level0 col0\" >accuracy</th>\n",
       "      <th id=\"T_00081_level0_col1\" class=\"col_heading level0 col1\" >balanced acurracy</th>\n",
       "      <th id=\"T_00081_level0_col2\" class=\"col_heading level0 col2\" >f1</th>\n",
       "      <th id=\"T_00081_level0_col3\" class=\"col_heading level0 col3\" >precision</th>\n",
       "      <th id=\"T_00081_level0_col4\" class=\"col_heading level0 col4\" >recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_00081_level0_row0\" class=\"row_heading level0 row0\" >UCI</th>\n",
       "      <td id=\"T_00081_row0_col0\" class=\"data row0 col0\" >0.8565 ± 0.0122</td>\n",
       "      <td id=\"T_00081_row0_col1\" class=\"data row0 col1\" >0.8644 ± 0.0068</td>\n",
       "      <td id=\"T_00081_row0_col2\" class=\"data row0 col2\" >0.8565 ± 0.0122</td>\n",
       "      <td id=\"T_00081_row0_col3\" class=\"data row0 col3\" >0.8565 ± 0.0122</td>\n",
       "      <td id=\"T_00081_row0_col4\" class=\"data row0 col4\" >0.8565 ± 0.0122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_00081_level0_row1\" class=\"row_heading level0 row1\" >WISDM</th>\n",
       "      <td id=\"T_00081_row1_col0\" class=\"data row1 col0\" >0.8581 ± 0.0066</td>\n",
       "      <td id=\"T_00081_row1_col1\" class=\"data row1 col1\" >0.8594 ± 0.0055</td>\n",
       "      <td id=\"T_00081_row1_col2\" class=\"data row1 col2\" >0.8581 ± 0.0066</td>\n",
       "      <td id=\"T_00081_row1_col3\" class=\"data row1 col3\" >0.8581 ± 0.0066</td>\n",
       "      <td id=\"T_00081_row1_col4\" class=\"data row1 col4\" >0.8581 ± 0.0066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_00081_level0_row2\" class=\"row_heading level0 row2\" >realworld_waist</th>\n",
       "      <td id=\"T_00081_row2_col0\" class=\"data row2 col0\" >0.6989 ± 0.0269</td>\n",
       "      <td id=\"T_00081_row2_col1\" class=\"data row2 col1\" >0.7447 ± 0.0230</td>\n",
       "      <td id=\"T_00081_row2_col2\" class=\"data row2 col2\" >0.6989 ± 0.0269</td>\n",
       "      <td id=\"T_00081_row2_col3\" class=\"data row2 col3\" >0.6989 ± 0.0269</td>\n",
       "      <td id=\"T_00081_row2_col4\" class=\"data row2 col4\" >0.6989 ± 0.0269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_00081_level0_row3\" class=\"row_heading level0 row3\" >realworld_thigh</th>\n",
       "      <td id=\"T_00081_row3_col0\" class=\"data row3 col0\" >0.6624 ± 0.0096</td>\n",
       "      <td id=\"T_00081_row3_col1\" class=\"data row3 col1\" >0.6603 ± 0.0045</td>\n",
       "      <td id=\"T_00081_row3_col2\" class=\"data row3 col2\" >0.6624 ± 0.0096</td>\n",
       "      <td id=\"T_00081_row3_col3\" class=\"data row3 col3\" >0.6624 ± 0.0096</td>\n",
       "      <td id=\"T_00081_row3_col4\" class=\"data row3 col4\" >0.6624 ± 0.0096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_00081_level0_row4\" class=\"row_heading level0 row4\" >motionsense</th>\n",
       "      <td id=\"T_00081_row4_col0\" class=\"data row4 col0\" >0.8851 ± 0.0029</td>\n",
       "      <td id=\"T_00081_row4_col1\" class=\"data row4 col1\" >0.8886 ± 0.0017</td>\n",
       "      <td id=\"T_00081_row4_col2\" class=\"data row4 col2\" >0.8851 ± 0.0029</td>\n",
       "      <td id=\"T_00081_row4_col3\" class=\"data row4 col3\" >0.8851 ± 0.0029</td>\n",
       "      <td id=\"T_00081_row4_col4\" class=\"data row4 col4\" >0.8851 ± 0.0029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_00081_level0_row5\" class=\"row_heading level0 row5\" >kuhar</th>\n",
       "      <td id=\"T_00081_row5_col0\" class=\"data row5 col0\" >0.5708 ± 0.0167</td>\n",
       "      <td id=\"T_00081_row5_col1\" class=\"data row5 col1\" >0.5721 ± 0.0287</td>\n",
       "      <td id=\"T_00081_row5_col2\" class=\"data row5 col2\" >0.5708 ± 0.0167</td>\n",
       "      <td id=\"T_00081_row5_col3\" class=\"data row5 col3\" >0.5708 ± 0.0167</td>\n",
       "      <td id=\"T_00081_row5_col4\" class=\"data row5 col4\" >0.5708 ± 0.0167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f409b3281c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dataframe_image as dfi\n",
    "\n",
    "# Define the root directory\n",
    "root = \"./logs/\"\n",
    "dictionary = {}\n",
    "\n",
    "#Loop over each dataset directory inside the root\n",
    "for dataset in os.listdir(root):\n",
    "    dataset_path = os.path.join(root, dataset)\n",
    "      \n",
    "        # Loop over each run directory within the dataset directory\n",
    "    accuracy = []\n",
    "    balancedaccuracy = []\n",
    "    f1 = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    \n",
    "    for run in os.listdir(dataset_path):\n",
    "        run_path = os.path.join(dataset_path, run)\n",
    "                \n",
    "        # Look for the metrics file with the specified pattern in the run directory\n",
    "        metrics_file = glob.glob(os.path.join(run_path, 'metrics_????-??-??-??-??-*.yaml'))\n",
    "\n",
    "        # Read the YAML file\n",
    "        with open(metrics_file[0], 'r') as file:\n",
    "            data = yaml.safe_load(file)\n",
    "                        \n",
    "        # Extract the 'classification' dictionary if it exists\n",
    "        classification_data = data.get('classification')\n",
    "        print(dataset, classification_data)\n",
    "        accuracy.append(classification_data['accuracy'])\n",
    "        balancedaccuracy.append(classification_data['balanced_accuracy'])\n",
    "        f1.append(classification_data['f1'])\n",
    "        precision.append(classification_data['precision'])\n",
    "        recall.append(classification_data['recall'])\n",
    "\n",
    "    dictionary[dataset] = {'accuracy': f'{np.array(accuracy).mean():.4f} ± {np.array(accuracy).std():.4f}',\n",
    "                           'balanced acurracy':  f'{np.array(balancedaccuracy).mean():.4f} ± {np.array(balancedaccuracy).std():.4f}',\n",
    "                           'f1':  f'{np.array(f1).mean():.4f} ± {np.array(f1).std():.4f}',\n",
    "                           'precision':  f'{np.array(precision).mean():.4f} ± {np.array(precision).std():.4f}',\n",
    "                           'recall':  f'{np.array(recall).mean():.4f} ± {np.array(recall).std():.4f}'}\n",
    "    \n",
    "# Criando o DataFrame\n",
    "df = pd.DataFrame(dictionary).T\n",
    "\n",
    "# Aplicando estilo com fundo branco e bordas sutis\n",
    "styled_df = (\n",
    "    df.style\n",
    "    .set_properties(**{'background-color': 'white', 'color': 'black', 'border-color': 'lightgray'})\n",
    "    .set_table_styles([\n",
    "        {'selector': 'th', 'props': [('background-color', '#f2f2f2'), ('color', 'black'), ('border-color', 'gray'), ('font-weight', 'bold')]},\n",
    "        {'selector': 'tr:hover', 'props': [('background-color', '#f9f9f9')]}\n",
    "    ])\n",
    "    .set_caption(\"Metricas de Performance por Dataset\")  # Adiciona um título ao DataFrame\n",
    ")\n",
    "\n",
    "styled_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxYAAAJOCAYAAAAqFJGJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABv0klEQVR4nO3deXgNZ//H8c9JyCKrNYk0BAmCWIPaGiVqKaULiiK2tjRFLVW1llqq9lKeUktbaqvtQW2pKKF2qsQulNr3PSTz+8PPeXokITEhlvfrus515czcc8/3zJwk53Nm7hmLYRiGAAAAAMAEu/QuAAAAAMCzj2ABAAAAwDSCBQAAAADTCBYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANMIFgAAAABMI1gAeGJ+//139evXT5cuXUrvUgAAQBojWAB4Io4cOaJ69erJzc1NHh4epvubMmWKLBaLYmNjzRf3BISHh8vf3z/d1m+xWNS3b990W3/lypVVuXJlm2mnTp3SO++8o6xZs8pisWjkyJGKioqSxWJRVFRUutR5T3R0tLy8vBQcHKyNGzdq4MCB6tixY7rWBABPO4IFgAe69wHeYrFo7dq1ieYbhiE/Pz9ZLBbVrl07yT5u376thg0bKjw8XJ988kmi+d9++62mTJmS1qW/ELZv36733ntPfn5+cnR0VJYsWRQWFqbJkycrPj4+vct7oE8++UTLli1T9+7d9eOPP6pGjRrpXZLVqFGj9Prrr+vll19WxYoVNWDAADVu3Di9y0pz9363LRaLMmTIoCxZsqhUqVLq0KGDdu/e/cj9Xr9+XX379k33gHjPunXr1LdvX128eDG9SwGeaxnSuwAAzwYnJydNnz5dFStWtJm+evVqHTt2TI6Ojskuu2vXLr377rvq0KFDkvO//fZbZcuWTeHh4Smup2nTpnr33XcfuN7n3cSJE/Xhhx/Ky8tLTZs2VWBgoK5cuaLIyEi1atVKJ06c0Oeff57eZUqSli9fnmjab7/9prp166pLly7Wafnz59eNGzfk4ODwJMtLZOTIkcqcObOcnZ01ZMgQZciQQW5ubula0+NSrVo1NWvWTIZh6NKlS9qxY4emTp2qb7/9Vl999ZU6deqU6j6vX7+uL774QpISHalKD+vWrdMXX3yh8PBweXp6pnc5wHOLYAEgRWrVqqXZs2dr9OjRypDhf386pk+frlKlSuns2bPJLlu8eHEVL148Teq4du2aXFxcZG9vL3t7+zTp81n0xx9/6MMPP1S5cuW0ZMkSmw+9HTt21ObNm/XXX3+lY4W2kgoKp0+fTvQhz87OTk5OTk+oquTlzJnT+nPmzJnTsRJzbt68KQcHB9nZJX+CQv78+fXee+/ZTBs8eLDq1Kmjzp07q2DBgqpVq9bjLhXAc4BToQCkSKNGjXTu3DmtWLHCOi0uLk5z5sxJ9hSRhIQEjRw5UoULF5aTk5O8vLz0wQcf6MKFC9Y2/v7+2rVrl1avXm09JePeN5z3TsNavXq12rVrpxw5cuill16ymXf/GItff/1VoaGhcnNzk7u7u0qXLq3p06db569Zs0b169dXrly55OjoKD8/P33yySe6ceOGTT8nT55UixYt9NJLL8nR0VE+Pj6qW7duisZ0zJ8/X0WKFJGTk5OKFCmiefPmPfL2Sc4XX3whi8WiadOmJflNekhIyAOPAB05ckTt2rVTgQIF5OzsrKxZs6p+/fqJXt/t27f1xRdfKDAwUE5OTsqaNasqVqxo8z5Iybb69xiLe/vOMAyNHTvWut8lJTvGYsOGDapVq5YyZ84sFxcXFS1aVKNGjbLO//PPPxUeHq68efPKyclJ3t7eatmypc6dO5fotR8/flytWrVSzpw55ejoqDx58qht27aKi4uTJJ09e1adO3dWkSJF5OrqKnd3d9WsWVM7duxI1Nfp06fVqlUreXl5ycnJScWKFdPUqVOT3e7/5u/vr9q1a2v58uUqXry4nJycVKhQIc2dOzdR20OHDql+/frKkiWLMmXKpJdfflmLFy+2aXNv282YMUM9e/aUr6+vMmXKpMuXL6eonn/LmjWrZsyYoQwZMmjAgAHW6XFxcerdu7dKlSolDw8Pubi4qFKlSlq1apW1TWxsrLJnzy7pf+/Tf4/xSem+unLlijp27Ch/f385OjoqR44cqlatmrZu3WrTbsOGDapRo4Y8PDyUKVMmhYaGKjo62jq/b9++6tq1qyQpT5481nqelfFZwLOEIxYAUsTf31/lypXTzz//rJo1a0q6+yH+0qVLevfddzV69OhEy3zwwQeaMmWKWrRoofbt2+vw4cMaM2aMtm3bpujoaGXMmFEjR47Uxx9/LFdXV/Xo0UOS5OXlZdNPu3btlD17dvXu3VvXrl1LtsYpU6aoZcuWKly4sLp37y5PT09t27ZNS5cutYaf2bNn6/r162rbtq2yZs2qjRs36ptvvtGxY8c0e/Zsa19vv/22du3apY8//lj+/v46ffq0VqxYoaNHjz5wEPby5cv19ttvq1ChQho0aJDOnTtn/dD9KNsnKdevX1dkZKReeeUV5cqVK9laHmTTpk1at26d3n33Xb300kuKjY3VuHHjVLlyZe3evVuZMmWSdPdD2aBBg9S6dWuVKVNGly9f1ubNm7V161ZVq1btkbbVK6+8oh9//FFNmza1nobzICtWrFDt2rXl4+OjDh06yNvbWzExMVq0aJH19LoVK1bo0KFDatGihby9vbVr1y5999132rVrl/744w9rcPnnn39UpkwZXbx4Ue+//74KFiyo48ePa86cObp+/bocHBx04MABLViwQA0aNJC/v79OnTql8ePHKzQ0VLt377Yezbhx44YqV66sAwcOKCIiQnny5NHs2bMVHh6uixcvJnvq37/t379fDRs21IcffqjmzZtr8uTJql+/vpYuXWrdvqdOnVL58uV1/fp1tW/fXlmzZtXUqVP1xhtvaM6cOXrzzTdt+uzfv78cHBzUpUsX3bp165FPK8uVK5dCQ0O1atUqXb58We7u7rp8+bImTpyoRo0aqU2bNrpy5Yq+//57Va9eXRs3blTx4sWVPXt2jRs3Tm3bttWbb76pt956S5JUtGjRVO2rDz/8UHPmzFFERIQKFSqkc+fOae3atYqJiVHJkiUl3T2drmbNmipVqpT69OkjOzs7TZ48WVWqVNGaNWtUpkwZvfXWW9q3b59+/vlnjRgxQtmyZZMka/gBkIYMAHiAyZMnG5KMTZs2GWPGjDHc3NyM69evG4ZhGPXr1zdeffVVwzAMI3fu3Mbrr79uXW7NmjWGJGPatGk2/S1dujTR9MKFCxuhoaHJrrtixYrGnTt3kpx3+PBhwzAM4+LFi4abm5tRtmxZ48aNGzZtExISrD/fq/3fBg0aZFgsFuPIkSOGYRjGhQsXDEnG119//bDNk0jx4sUNHx8f4+LFi9Zpy5cvNyQZuXPntk5Lzfa5344dOwxJRocOHVJclySjT58+1udJbYf169cbkowffvjBOq1YsWI2+/V+Kd1WoaGhifaxJOOjjz6ymbZq1SpDkrFq1SrDMAzjzp07Rp48eYzcuXMbFy5csGn7sP36888/G5KM33//3TqtWbNmhp2dnbFp06ZE7e/1d/PmTSM+Pt5m3uHDhw1HR0ejX79+1mkjR440JBk//fSTdVpcXJxRrlw5w9XV1bh8+XISW+J/cufObUgyfvnlF+u0S5cuGT4+PkaJEiWs0zp27GhIMtasWWOdduXKFSNPnjyGv7+/tdZ72y5v3rxJbo+kJLUP/q1Dhw6GJGPHjh2GYdzdH7du3bJpc+HCBcPLy8to2bKlddqZM2cSvefuSem+8vDweGBtCQkJRmBgoFG9evVE74U8efIY1apVs077+uuvbf5eAHg8OBUKQIo1aNBAN27c0KJFi3TlyhUtWrQo2dOgZs+eLQ8PD1WrVk1nz561PkqVKiVXV1ebUycepk2bNg8dT7FixQpduXJFn332WaJz9O99AypJzs7O1p+vXbums2fPqnz58jIMQ9u2bbO2cXBwUFRUVIpOS7rnxIkT2r59u5o3b25zSd1q1aqpUKFCNm3NbJ97p7aYGUz87+1w+/ZtnTt3TgEBAfL09LQ51cTT01O7du3S/v37k+3nUbZVSm3btk2HDx9Wx44dE43HSG6/3rx5U2fPntXLL78sSdbXk5CQoPnz56tOnToKCQlJtK57/Tk6OlrHJMTHx+vcuXNydXVVgQIFbLbNkiVL5O3trUaNGlmnZcyYUe3bt9fVq1e1evXqh76+nDlz2hxxcHd3V7NmzbRt2zadPHnSup4yZcrYXDjB1dVV77//vmJjYxNdval58+Y228MMV1dXSXdPS5Ike3t76xGQhIQEnT9/Xnfu3FFISEiiU5SSk5J9Jd19723YsEH//PNPkv1s375d+/fvV+PGjXXu3Dnr79C1a9dUtWpV/f7770pISEj9iwbwyAgWAFIse/bsCgsL0/Tp0zV37lzFx8frnXfeSbLt/v37denSJeXIkUPZs2e3eVy9elWnT59O8Xrz5Mnz0DYHDx6UJBUpUuSB7Y4eParw8HBlyZJFrq6uyp49u0JDQyXJeuM+R0dHffXVV/r111/l5eWlV155RUOGDLF+0EvOkSNHJEmBgYGJ5hUoUMDmuZnt4+7uLul/H/YexY0bN9S7d2/rZWqzZcum7Nmz6+LFizY3MOzXr58uXryo/PnzKzg4WF27dtWff/5pnf+o2yqlUrpfz58/rw4dOsjLy0vOzs7Knj279X1z7/WcOXNGly9ffmhfCQkJGjFihAIDA222zZ9//mmzbY4cOaLAwMBEA6ODgoKs8x8mICDAJiBJdwdTS7KOAThy5Eii98+D1pOS35eUunr1qiTbEDt16lQVLVrUOuYme/bsWrx4cYpvfJmSfSVJQ4YM0V9//SU/Pz+VKVNGffv21aFDh6zz74Xd5s2bJ/odmjhxom7dusXNOIEnjDEWAFKlcePGatOmjU6ePKmaNWsme+nGhIQE5ciRQ9OmTUtyfmrOb06rb1/j4+NVrVo1nT9/Xt26dVPBggXl4uKi48ePKzw83ObbzY4dO6pOnTqaP3++li1bpl69emnQoEH67bffVKJECdO1mNk+AQEBypAhg3bu3PnI6//44481efJkdezYUeXKlZOHh4csFoveffddm+3wyiuv6ODBg1qwYIGWL1+uiRMnasSIERo/frxat24t6fFvq5Ro0KCB1q1bp65du6p48eJydXVVQkKCatSokepvrQcOHKhevXqpZcuW6t+/v7JkySI7Ozt17NjxmfgGPK1+XyTpr7/+kr29vfWD/08//aTw8HDVq1dPXbt2VY4cOWRvb69BgwZZQ+DDpHRfNWjQQJUqVdK8efO0fPlyff311/rqq680d+5c1axZ09r266+/Tvaqc/eOuAB4MggWAFLlzTff1AcffKA//vhDM2fOTLZdvnz5tHLlSlWoUOGhH3Tu/8b2UeTLl0/S3Q9CAQEBSbbZuXOn9u3bp6lTp9oMGP73FY7u77Nz587q3Lmz9u/fr+LFi2vYsGH66aefkmyfO3duSUrytKG9e/cm6jul2+d+mTJlUpUqVfTbb7/p77//lp+fX6qWl6Q5c+aoefPmGjZsmHXazZs3k7yBWJYsWdSiRQu1aNFCV69e1SuvvKK+fftag8W915OabZVS/96vYWFhSba5cOGCIiMj9cUXX6h3797W6ffvh+zZs8vd3f2hl+GdM2eOXn31VX3//fc20y9evGgd+Cvd3d9//vmnEhISbI5a7Nmzxzr/YQ4cOCDDMGx+B/bt2ydJ1oHvuXPnTvT+Se16HsXRo0e1evVqlStXznrEYs6cOcqbN6/mzp1rU3OfPn1slk3udzql++oeHx8ftWvXTu3atdPp06dVsmRJDRgwQDVr1rS+N9zd3ZN9bzysHgBpi1OhAKSKq6urxo0bp759+6pOnTrJtmvQoIHi4+PVv3//RPPu3Llj8wHWxcXF9B1xX3vtNbm5uWnQoEG6efOmzTzDMCTJOk7j3vN7P//7sqXS3asu3d9Hvnz55Obmplu3biVbg4+Pj4oXL66pU6fanIKxYsWKROfBp2b7JKVPnz4yDENNmza1nq7yb1u2bHngZU/t7e1ttoMkffPNN4nu1n3/JUBdXV0VEBBg3Q6Puq1SqmTJksqTJ49GjhyZaJs8aL9Kd29y9292dnaqV6+e/vvf/2rz5s2J1vXv/u7va/bs2Tp+/LjNtFq1aunkyZM2AfvOnTv65ptv5Orqaj3F7kH++ecfm8sRX758WT/88IOKFy8ub29v63o2btyo9evXW9tdu3ZN3333nfz9/RON30kL58+fV6NGjRQfH2+9WpuU9LbesGGDTW2SrFcVu3+fpXRfxcfHJzqNKUeOHMqZM6f1fVWqVCnly5dPQ4cOTfJ34MyZM9afXVxckqwHQNriiAWAVGvevPlD24SGhuqDDz7QoEGDtH37dr322mvKmDGj9u/fr9mzZ2vUqFHW8RmlSpXSuHHj9OWXXyogIEA5cuRQlSpVUlWTu7u7RowYodatW6t06dJq3LixMmfOrB07duj69euaOnWqChYsqHz58qlLly46fvy43N3d9csvvyQadLxv3z5VrVpVDRo0UKFChZQhQwbNmzdPp06d0rvvvvvAOgYNGqTXX39dFStWVMuWLXX+/Hl98803Kly4sM2Hn9Rsn6SUL19eY8eOVbt27VSwYEGbO29HRUVp4cKF+vLLL5Ndvnbt2vrxxx/l4eGhQoUKaf369Vq5cqWyZs1q065QoUKqXLmySpUqpSxZsmjz5s3WS4Ca3VYpYWdnp3HjxqlOnToqXry4WrRoIR8fH+3Zs0e7du3SsmXL5O7ubh3bcfv2bfn6+mr58uU6fPhwov4GDhyo5cuXKzQ0VO+//76CgoJ04sQJzZ49W2vXrpWnp6dq166tfv36qUWLFipfvrx27typadOmKW/evDZ9vf/++/rPf/6j8PBwbdmyRf7+/pozZ46io6M1cuTIFA2uz58/v1q1aqVNmzbJy8tLkyZN0qlTpzR58mRrm88++8x6mef27dsrS5Ysmjp1qg4fPqxffvnlgTe/S4l9+/bpp59+kmEYunz5snbs2KHZs2fr6tWrGj58uGrUqGFtW7t2bc2dO1dvvvmmXn/9dR0+fFjjx49XoUKFbN7fzs7OKlSokGbOnKn8+fMrS5YsKlKkiIoUKZKifXXlyhW99NJLeuedd1SsWDG5urpq5cqV2rRpk/Uom52dnSZOnKiaNWuqcOHCatGihXx9fXX8+HGtWrVK7u7u+u9//yvp7t8YSerRo4feffddZcyYUXXq1LEGDgBpJD0uRQXg2fHvy80+yP2Xm73nu+++M0qVKmU4Ozsbbm5uRnBwsPHpp58a//zzj7XNyZMnjddff91wc3MzJFkvS/qgdd9/udl7Fi5caJQvX95wdnY23N3djTJlyhg///yzdf7u3buNsLAww9XV1ciWLZvRpk0b6+VbJ0+ebBiGYZw9e9b46KOPjIIFCxouLi6Gh4eHUbZsWWPWrFkp2ma//PKLERQUZDg6OhqFChUy5s6dazRv3tzmcrOp2T4PsmXLFqNx48ZGzpw5jYwZMxqZM2c2qlatakydOtXmkqm679KfFy5cMFq0aGFky5bNcHV1NapXr27s2bPHyJ07t9G8eXNruy+//NIoU6aM4enpaTg7OxsFCxY0BgwYYMTFxaVqWz3q5WbvWbt2rVGtWjXDzc3NcHFxMYoWLWp888031vnHjh0z3nzzTcPT09Pw8PAw6tevb/zzzz9JXvL0yJEjRrNmzYzs2bMbkgw/Pz/jo48+sl5G9ebNm0bnzp0NHx8fw9nZ2ahQoYKxfv36JF/DqVOnrNvRwcHBCA4Otr6PHube78yyZcuMokWLGo6OjkbBggWN2bNnJ2p78OBB45133jE8PT0NJycno0yZMsaiRYuS3HZJLZ8cSdaHnZ2d4enpaZQoUcLo0KGDsWvXrkTtExISjIEDBxq5c+c2HB0djRIlShiLFi1K8v29bt06o1SpUoaDg4PNfkjJvrp165bRtWtXo1ixYtZ9XqxYMePbb79NVNO2bduMt956y8iaNavh6Oho5M6d22jQoIERGRlp065///6Gr6+vYWdnx6VngcfEYhj3HY8EAOAFce/Gf++///4TX7e/v7+KFCmiRYsWPfF1A8DjwBgLAMALq06dOqYHmAMA7mKMBQDghbN48WL9888/WrRoUZIDfwEAqUewAAC8cI4dO6ZOnTrJzc1N48aNS+9yAOC5wBgLAAAAAKYxxgIAAACAaQQLAAAAAKYRLAAAAACY9sIN3k5ISNA///wjNzc3WSyW9C4HAAAAeGoZhqErV64oZ86csrN78DGJFy5Y/PPPP/Lz80vvMgAAAIBnxt9//62XXnrpgW1euGDh5uYm6e7GcXd3T+dqAAAAgKfX5cuX5efnZ/0M/SAvXLC4d/qTu7s7wQIAAABIgZQMIWDwNgAAAADTCBYAAAAATCNYAAAAADDthRtjAQAAkNbi4+N1+/bt9C4DSLWMGTPK3t4+TfoiWAAAADwiwzB08uRJXbx4Mb1LAR6Zp6envL29Td/jjWABAADwiO6Fihw5cihTpkzcfBfPFMMwdP36dZ0+fVqS5OPjY6o/ggUAAMAjiI+Pt4aKrFmzpnc5wCNxdnaWJJ0+fVo5cuQwdVoUg7cBAAAewb0xFZkyZUrnSgBz7r2HzY4TIlgAAACYwOlPKTdq1CitX78+vcvAfdLqPUywAAAAwGM3bNgwzZ07VyVLlnzkPiwWi+bPn592RSFNESwAAACQYuHh4bJYLPrwww8Tzfvoo49ksVgUHh5uMz06Olo//vijFixYIEdHR+v0qKgoWSyWFF9V68SJE6pZs6aZ8vEYESwAAACQKn5+fpoxY4Zu3LhhnXbz5k1Nnz5duXLlStS+QoUK2r59uzw9PR9pfXFxcZIkb29vm2CCpwvBAgAAAKlSsmRJ+fn5ae7cudZpc+fOVa5cuVSiRAnrtISEBA0aNEh58uSRs7OzihUrpjlz5kiSYmNj9eqrr0qSMmfObHOko3LlyoqIiFDHjh2VLVs2Va9eXVLiU6GOHTumRo0aKUuWLHJxcVFISIg2bNggSTp48KDq1q0rLy8vubq6qnTp0lq5cqXN6/j2228VGBgoJycneXl56Z133knzbfUi4XKzAAAASLWWLVtq8uTJatKkiSRp0qRJatGihaKioqxtBg0apJ9++knjx49XYGCgfv/9d7333nvKnj27KlasqF9++UVvv/229u7dK3d3d+ulTyVp6tSpatu2raKjo5Nc/9WrVxUaGipfX18tXLhQ3t7e2rp1qxISEqzza9WqpQEDBsjR0VE//PCD6tSpo7179ypXrlzavHmz2rdvrx9//FHly5fX+fPntWbNmse3wV4ABAsAAACk2nvvvafu3bvryJEjku6Oo5gxY4Y1WNy6dUsDBw7UypUrVa5cOUlS3rx5tXbtWv3nP/9RaGiosmTJIknKkSNHotOkAgMDNWTIkGTXP336dJ05c0abNm2y9hMQEGCdX6xYMRUrVsz6vH///po3b54WLlyoiIgIHT16VC4uLqpdu7bc3NyUO3dum6MtSD2CBQAAAFIte/bsev311zVlyhQZhqHXX39d2bJls84/cOCArl+/rmrVqtksFxcXl6IP8KVKlXrg/O3bt6tEiRLWUHG/q1evqm/fvlq8eLFOnDihO3fu6MaNGzp69KgkqVq1asqdO7fy5s2rGjVqqEaNGnrzzTe5L4kJBAsAAAA8kpYtWyoiIkKSNHbsWJt5V69elSQtXrxYvr6+NvNSMgDbxcXlgfP/fdpUUrp06aIVK1Zo6NChCggIkLOzs9555x3rQHA3Nzdt3bpVUVFRWr58uXr37q2+fftq06ZNjzzI/EVHsAAAAMAjqVGjhuLi4mSxWKwDrO8pVKiQHB0ddfToUYWGhia5vIODgyQpPj4+1esuWrSoJk6cqPPnzyd51CI6Olrh4eF68803Jd0NOrGxsTZtMmTIoLCwMIWFhalPnz7y9PTUb7/9prfeeivV9YBgAQAAgEdkb2+vmJgY68//5ubmpi5duuiTTz5RQkKCKlasqEuXLik6Olru7u5q3ry5cufOLYvFokWLFqlWrVpydnaWq6tritbdqFEjDRw4UPXq1dOgQYPk4+Ojbdu2KWfOnCpXrpwCAwM1d+5c1alTRxaLRb169bIO7JakRYsW6dChQ3rllVeUOXNmLVmyRAkJCSpQoEDabaAXDJebBQAAwCNzd3eXu7t7kvP69++vXr16adCgQQoKClKNGjW0ePFi5cmTR5Lk6+urL774Qp999pm8vLysp1WlhIODg5YvX64cOXKoVq1aCg4O1uDBg60BZ/jw4cqcObPKly+vOnXqqHr16jZ3/fb09NTcuXNVpUoVBQUFafz48fr5559VuHBhE1vjxWYxDMNI7yKepMuXL8vDw0OXLl1K9pcAAADgYW7evKnDhw8rT548cnJySu9ygEf2oPdyaj47cyoUAAB47h3tF5xmfeXqvTPN+gKeJ5wKBQAAAMA0jlgAAICnTqmuP6Rpf/Pc0q6vCt9UkCTlcM6h9sHtZZw1ZJfx0b6r9b2QdnXdcPFOs75y+HmmWV94cXDEAgAAAIBpBAsAAAAAphEsAAAAAJhGsAAAAABgGsECAAAAgGkECwAAAACmESwAAAAAmEawAAAAAGAaN8gDAABIY02Gbnyi64tqVeaJri+t3b59WxkzZkzvMmASRywAAABeML9FrVSdt2oosEhuFSyaV03CGyo29rB1/rFjx9SoUSNlyZJFLi4uCgkJ0YYNG6zz//vf/6p06dJycnJStmzZ9Oabb1rnWSwWzZ8/32Z9np6emjJliiQpNjZWFotFM2fOVGhoqJycnDRt2jSdO3dOjRo1kq+vrzJlyqTg4GD9/PPPNv0kJCRoyJAhCggIkKOjo3LlyqUBAwZIkqpUqaKIiAib9mfOnJGDg4MiIyPTYrPhIThi8YQd7Recpv3l6r0zTfsDAADPv+vXr+vDNh+pUFBhXbt2TV8NG6jw99/Tb0vX6MaN6wp7PVS+vr5auHChvL29tXXrViUkJEiSFi9erDfffFM9evTQDz/8oLi4OC1ZsiTVNXz22WcaNmyYSpQoIScnJ928eVOlSpVSt27d5O7ursWLF6tp06bKly+fypS5e0Sme/fumjBhgkaMGKGKFSvqxIkT2rNnjySpdevWioiI0LBhw+To6ChJ+umnn+Tr66sqVaqk0ZbDgxAsAAAAXjC1a71h83zk0DEqVDxAe/fv0abNG3XmzBlt2rRJWbJkkSQFBARY2w4YMEDvvvuuvvjiC+u0YsWKpbqGjh076q233rKZ1qVLF+vPH3/8sZYtW6ZZs2apTJkyunLlikaNGqUxY8aoefPmkqR8+fKpYsWKkqS33npLERERWrBggRo0aCBJmjJlisLDw2WxWFJdH1KPU6EAAABeMIcOH9QHEa1UukJx5SuUSyHl7waD48ePadfunSpRooQ1VNxv+/btqlq1qukaQkJCbJ7Hx8erf//+Cg4OVpYsWeTq6qply5bp6NGjkqSYmBjdunUr2XU7OTmpadOmmjRpkiRp69at+uuvvxQeHm66VqQMRyzwwivV9Yc062ue29dp1lejzO5p1pckDZyddr/uO0t3eXijFIoYVifN+gIApEzTlo30kq+fhn01St5e3kpISFBotfKKu31bTk7OD1zW2fnB8y0WiwzDsJl2+/btRO1cXFxsnn/99dcaNWqURo4cqeDgYLm4uKhjx46Ki4tL0Xqlu6dDFS9eXMeOHdPkyZNVpUoV5c6d+6HLIW0QLFIgbT94pllXkqQK31RIs7744AkAwPPv/IXzOnBwv4YNHqmXy5aXJG3YuN46v1DBwpo+80edP38+yaMWRYsWVWRkpFq0aJFk/9mzZ9eJEyesz/fv36/r168/tK7o6GjVrVtX7733nqS7A7X37dunQoUKSZICAwPl7OysyMhItW7dOsk+goODFRISogkTJmj69OkaM2bMQ9eLtMOpUAAAAC8QTw9PZcmcRT9On6rDsYe0Jvp39e7f0zr/zbpvy9vbW/Xq1VN0dLQOHTqkX375RevX3w0fffr00c8//6w+ffooJiZGO3fu1FdffWVdvkqVKhozZoy2bdumzZs368MPP0zRpWQDAwO1YsUKrVu3TjExMfrggw906tQp63wnJyd169ZNn376qX744QcdPHhQf/zxh77//nubflq3bq3BgwfLMAybq1Xh8SNYAAAAvEDs7Ow0fsz3+nPnDoVWK6/e/T5Xnx79rPMdHBy0fPly5ciRQ7Vq1VJwcLAGDx4se3t7SVLlypU1e/ZsLVy4UMWLF1eVKlW0ceP/7tsxbNgw+fn5qVKlSmrcuLG6dOmiTJkyPbSunj17qmTJkqpevboqV65sDTf/1qtXL3Xu3Fm9e/dWUFCQGjZsqNOnT9u0adSokTJkyKBGjRrJycnJxJZCanEqFAAAQBqb1iVlN6zzvfCYC0lGaKXKWvPbHzbTTh39XzG5c+fWnDlzkl3+rbfeSnRFp3ty5sypZcuW2Uy7ePGi9Wd/f/9EYzAkKUuWLInuf3E/Ozs79ejRQz169Ei2zdmzZ3Xz5k21atXqgX0h7REsAAAA8My7ffu2zp07p549e+rll19WyZIl07ukFw6nQgEAAOCZFx0dLR8fH23atEnjx49P73JeSByxAAAAwDOvcuXKSZ5ihSeHIxYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANMIFgAAAABMI1gAAAAAMI1gAQAA8IJ5s0Ft9ezb3VQfUVFRslgsNnfVflr5+/tr5MiR6V3Gc49gAQAAAMA0bpAHAACQxjL9p36K2l1Io/VlbjA7jXrCkxIXFycHB4dE02/fvq2MGTOmQ0XmccQCAADgBRQff0fde3VVQOFcCiqWT4OHDrC5c/WPP/6okJAQubm5ydvbW40bN9bp06eT7e/cuXNq1KiRfH19lSlTJgUHB+vnn3+2aVO5cmW1b99en376qbJkySJvb2/17dvXps3Fixf1wQcfyMvLS05OTipSpIgWLVpknb927VpVqlRJzs7O8vPzU/v27XXt2jXr/NOnT6tOnTpydnZWnjx5NG3atIdui02bNqlatWrKli2bPDw8FBoaqq1bt6a4rr59+6p48eI27UeOHCl/f3/r8/DwcNWrV08DBgxQzpw5VaBAAcXGxspisWjmzJkKDQ2Vk5OTpk2blqJtmZCQoCFDhiggIECOjo7KlSuXBgwYIEmqUqWKIiIibNqfOXNGDg4OioyMfOj2eFQECwAAgBfQzDkzZG+fQUsXRqp/30EaP+Fb/fTzD9b5t2/fVv/+/bVjxw7Nnz9fsbGxCg8PT7a/mzdvqlSpUlq8eLH++usvvf/++2ratKk2btxo027q1KlycXHRhg0bNGTIEPXr108rVqyQdPfDcs2aNRUdHa2ffvpJu3fv1uDBg2Vvby9JOnjwoGrUqKG3335bf/75p2bOnKm1a9fafIgODw/X33//rVWrVmnOnDn69ttvHxiIJOnKlStq3ry51q5dqz/++EOBgYGqVauWrly5kqK6UioyMlJ79+7VihUrbMLSZ599pg4dOigmJkbVq1dP0bbs3r27Bg8erF69emn37t2aPn26vLy8JEmtW7fW9OnTdevWLWv7n376Sb6+vqpSpUqqak4NToUCAAB4Afn6+Kp/n4GyWCwKyBeomD279d3EcWrauLkkqWXLlta2efPm1ejRo1W6dGldvXpVrq6uifvz9VWXLl2szz/++GMtW7ZMs2bNUpkyZazTixYtqj59+kiSAgMDNWbMGEVGRqpatWpauXKlNm7cqJiYGOXPn9+67nsGDRqkJk2aqGPHjtblR48erdDQUI0bN05Hjx7Vr7/+qo0bN6p06dKSpO+//15BQUEP3Bb3f9j+7rvv5OnpqdWrV6t27doPrSulXFxcNHHiROspULGxsZKkjh076q233rJp+6BteeXKFY0aNUpjxoxR8+Z391e+fPlUsWJFSdJbb72liIgILViwQA0aNJAkTZkyReHh4bJYLKmuO6U4YgEAAPACKlkyxOZDZkipMjoUe1Dx8fGSpC1btqhOnTrKlSuX3NzcFBoaKkk6evRokv3Fx8erf//+Cg4OVpYsWeTq6qply5Ylal+0aFGb5z4+PtYjCtu3b9dLL71k/fB+vx07dmjKlClydXW1PqpXr66EhAQdPnxYMTExypAhg0qVKmVdpmDBgvL09Hzgtjh16pTatGmjwMBAeXh4yN3dXVevXrXW/rC6Uio4ODjJcRUhISE2zx+2LWNiYnTr1i1VrVo1yfU4OTmpadOmmjRpkiRp69at+uuvvx54xCktcMQCAAAANq5du6bq1aurevXqmjZtmrJnz66jR4+qevXqiouLS3KZr7/+WqNGjdLIkSMVHBwsFxcXdezYMVH7+wcmWywWJSQkSJKcnZ0fWNfVq1f1wQcfqH379onm5cqVS/v27UvNy7Rq3ry5zp07p1GjRil37txydHRUuXLlrLU/rC47Ozub8SnS3VPJ7ufi4pLk8vdPf9i2fFg90t3ToYoXL65jx45p8uTJqlKlinLnzv3Q5cwgWAAAALyAtm3bYvN8y9ZNyuufT/b29tqzZ4/OnTunwYMHy8/PT5K0efPmB/YXHR2tunXr6r333pN0d1zCvn37VKhQoRTXVLRoUR07dkz79u1L8uhAyZIltXv3bgUEBCS5fMGCBXXnzh1t2bLFeirU3r17H3qvjejoaH377beqVauWJOnvv//W2bNnU1xX9uzZdfLkSRmGYT0KtH379pS85GTredC2DAwMlLOzsyIjI9W6desk+wgODlZISIgmTJig6dOna8yYMY9cT0pxKhQAAMAL6Ng/x9S7Xw8dOLhfcxfM0fdTJqhNyw8k3f3238HBQd98840OHTqkhQsXqn///g/sLzAwUCtWrNC6desUExOjDz74QKdOnUpVTaGhoXrllVf09ttva8WKFTp8+LB+/fVXLV26VJLUrVs3rVu3ThEREdq+fbv279+vBQsWWAdvFyhQQDVq1NAHH3ygDRs2aMuWLWrduvVDv+EPDAzUjz/+qJiYGG3YsEFNmjSxWeZhdVWuXFlnzpzRkCFDdPDgQY0dO1a//vprql77/fU8aFs6OTmpW7du+vTTT/XDDz/o4MGD+uOPP/T999/b9NO6dWsNHjxYhmHozTfffOR6UopgAQAA8AJq8HZD3bx5QzXeqKruPbuqTcsP1LRJuKS738BPmTJFs2fPVqFChTR48GANHTr0gf317NlTJUuWVPXq1VW5cmV5e3urXr16qa7rl19+UenSpdWoUSMVKlRIn376qXXcR9GiRbV69Wrt27dPlSpVUokSJdS7d2/lzJnTuvzkyZOVM2dOhYaG6q233tL777+vHDlyPHCd33//vS5cuKCSJUuqadOmat++faJlHlRXUFCQvv32W40dO1bFihXTxo0bbQZfp1ZKtmWvXr3UuXNn9e7dW0FBQWrYsGGiq181atRIGTJkUKNGjeTk5PTI9aSUxbj/hLDn3OXLl+Xh4aFLly7J3d09RcuU6vrDwxul0Dy3r9OsL0lqlDllryElBs5OuzPjdpZ+9F+m+0UMq5NmfSXlad2/ablvpRd3/wJ4NqXl32bp8fx9zuGcQ+2D28vL10t2GR/tu1rftLpDnqQbLt5p1lcOP8806wvpJzY2Vvny5dOmTZtUsmTJZNvdvHlThw8fVp48eRIFkNR8dmaMBQAAAPAcuX37ts6dO6eePXvq5ZdffmCoSEvpfirU2LFj5e/vLycnJ5UtWzbRTVTuN3LkSBUoUMB6t8VPPvlEN2/efELVAgAAAE+36Oho+fj4aNOmTRo/fvwTW2+6HrGYOXOmOnXqpPHjx6ts2bIaOXKkqlevrr179yZ5Ltz06dP12WefadKkSSpfvrz27dtnvdHH8OHD0+EVAAAAAE+XypUrJ7r87ZOQrkcshg8frjZt2qhFixYqVKiQxo8fr0yZMllv5nG/devWqUKFCmrcuLH8/f312muvqVGjRg89ygEAAADg8Uq3YBEXF6ctW7YoLCzsf8XY2SksLEzr169Pcpny5ctry5Yt1iBx6NAhLVmyxHrNYQAAAADpI91OhTp79qzi4+Pl5eVlM93Ly0t79uxJcpnGjRvr7NmzqlixogzD0J07d/Thhx/q888/T3Y9t27d0q1bt6zPL1++nDYvAAAAAIBVug/eTo2oqCgNHDhQ3377rbZu3aq5c+dq8eLFD7xhy6BBg+Th4WF93Lt7JAAAAIC0k25HLLJlyyZ7e/tEd2Q8deqUvL2Tvg5zr1691LRpU+uty4ODg3Xt2jW9//776tGjh+zsEuek7t27q1OnTtbnly9fJlwAAAAAaSzdjlg4ODioVKlSioyMtE5LSEhQZGSkypUrl+Qy169fTxQe7O3tJSnZke+Ojo5yd3e3eQAAAABIW+l6KlSnTp00YcIETZ06VTExMWrbtq2uXbumFi1aSJKaNWum7t27W9vXqVNH48aN04wZM3T48GGtWLFCvXr1Up06dawBAwAAAA9mGIY6d+uoAsF55JUrs/7atTO9S8JzIF3vY9GwYUOdOXNGvXv31smTJ1W8eHEtXbrUOqD76NGjNkcoevbsKYvFop49e+r48ePKnj276tSpowEDBqTXSwAAAEik1axWT3R9S6t9n6r2v0Wt1Mw50zVv5n+VO5e/Dh4+oPdavKs/d+7QqdMnNW/ePNWrV+/xFIvnVroGC0mKiIhQREREkvOioqJsnmfIkEF9+vRRnz59nkBlAAAAz6fYI4fllcNLpUPKSpJ27vpThQsVUeOG76nF+03TuTo8q9I9WAAAAODJad+pnWbO+VmS5JUrs/xe8tPmdX+q6qvV0rkyPOsIFgAAAC+QL/sOkn/uPPpx+hQt++9vsmOcKtIIwQIAAOAF4u7uIRcXV9nb2ytHDq+HLwCk0DN1gzwAAAAATyeCBQAAAADTCBYAAAAATGOMBQAAwAvu2rWrOhx72Pr88OHD2r59u7JkyaJcuXKlY2V4lhAsAAAAXnDb/9yutxrWsT7v1KmTJKl58+aaMmVKOlWFZw3BAgAAII193yBld8L2vfCYC0nGB63b6oPWba3PK5SrqFNH/1dMDj/PdKgKzzrGWAAAAAAwjWABAAAAwDSCBQAAAADTCBYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANMIFgAAAABMI1gAAAAAMI1gAQAAgMcuKipKFotFFy9eTNO2eHpkSO8CAAAAnjen6n+QsnZptL6S3/0njXp6fMqXL68TJ07Iw8MjTdvi6cERCwAAADxQXFyc6T4cHBzk7e0ti8WSpm3x9CBYAAAAvGDebFBb3Xt1VfdeXRVQOJeCiuXT4KEDZBiGJMnf31/9+/dXs2bN5O7urvfff1+StHbtWlWqVEnOzs7y8/NT+/btde3aNWu/t27dUrdu3eTn5ydHR0cFBATo+++/l5T49KYjR46oTp06ypw5s1xcXFS4cGEtWbIkybaS9Msvv6hw4cJydHSUv7+/hg0bZvOa/P39NXDgQLVs2VJubm7KlSuXvvvuu8e1CZEEggUAAMALaOacGbK3z6ClCyPVv+8gjZ/wrX76+Qfr/KFDh6pYsWLatm2bevXqpYMHD6pGjRp6++239eeff2rmzJlau3atIiIirMs0a9ZMP//8s0aPHq2YmBj95z//kaura5Lr/+ijj3Tr1i39/vvv2rlzp7766qtk227ZskUNGjTQu+++q507d6pv377q1auXpkyZYtNu2LBhCgkJ0bZt29SuXTu1bdtWe/fuNb+xkCKMsQAAAHgB+fr4qn+fgbJYLArIF6iYPbv13cRxatq4uSSpSpUq6ty5s7V969at1aRJE3Xs2FGSFBgYqNGjRys0NFTjxo3T0aNHNWvWLK1YsUJhYWGSpLx58ya7/qNHj+rtt99WcHDwQ9sOHz5cVatWVa9evSRJ+fPn1+7du/X1118rPDzc2q5WrVpq166dJKlbt24aMWKEVq1apQIFCqR+AyHVCBYAkEJH+wWnWV+5eu9Ms74A4FGULBliM4YhpFQZjZ8wVvHx8Xefh4TYtN+xY4f+/PNPTZs2zTrNMAwlJCTo8OHD2rlzp+zt7RUaGpqi9bdv315t27bV8uXLFRYWprfffltFixZNsm1MTIzq1q1rM61ChQoaOXKk4uPjZW9vL0k2y1ssFnl7e+v06dMpqgfmcSoUAAAAEnFxcbF5fvXqVX3wwQfavn279bFjxw7t379f+fLlk7Ozc6r6b926tQ4dOqSmTZtq586dCgkJ0TfffGOq5owZM9o8t1gsSkhIMNUnUo5gAQAA8ALatm2LzfMtWzcpr38+67f/9ytZsqR2796tgICARA8HBwcFBwcrISFBq1evTnENfn5++vDDDzV37lx17txZEyZMSLJdUFCQoqOjbaZFR0crf/78ydaLJ49gAQAA8AI69s8x9e7XQwcO7tfcBXP0/ZQJatMy+ftvdOvWTevWrVNERIS2b9+u/fv3a8GCBdbB2/7+/mrevLlatmyp+fPn6/Dhw4qKitKsWbOS7K9jx45atmyZDh8+rK1bt2rVqlUKCgpKsm3nzp0VGRmp/v37a9++fZo6darGjBmjLl26mN8QSDOMsQAAAHgBNXi7oW7evKEab1SVvZ292rT8QE2bhCfbvmjRolq9erV69OihSpUqyTAM5cuXTw0bNrS2GTdunD7//HO1a9dO586dU65cufT5558n2V98fLw++ugjHTt2TO7u7qpRo4ZGjBiRZNuSJUtq1qxZ6t27t/r37y8fHx/169fPZuA20h/BAgAAII15zU7ZnbB9LzzmQh4gQ4aM+rLvIA0ZODzRvNjY2CSXKV26tJYvX55sn05OTho+fLiGD0/cZ+XKla33yZD0wPEU97eVpLfffltvv/12ssskVfP27duTbY+0x6lQAAAAAEwjWAAAAAAwjVOhAAAAXjDzZi1K7xLwHOKIBQAAAADTCBYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANMIFgAAAABMI1gAAADgsevbt6+KFy9ufR4eHq569eqlWz1Ie9zHAgAAII2tHLL/ia6veZvAJ7o+ICkcsQAAAHjBxcXFpXcJeA4QLAAAAF4wbzaore69uqpn3+4KKpZPDZu+rZi9u9Wo2TvKU/AleXl5qWnTpjp79qx1mYSEBA0ZMkQBAQFydHRUrly5NGDAAOv8bt26KX/+/MqUKZPy5s2rXr166fbt2+nx8pBOCBYAAAAvoJlzZsghY0b9d+5S9fysj95+t66KFC6q5Yt+09KlS3Xq1Ck1aNDA2r579+4aPHiwevXqpd27d2v69Ony8vKyzndzc9OUKVO0e/dujRo1ShMmTNCIESPS46UhnTDGAgAA4AWUN09e9e7RT5I0fPRQBRcuqh7dekuScvh5atKkSfLz89O+ffvk4+OjUaNGacyYMWrevLkkKV++fKpYsaK1v549e1p/9vf3V5cuXTRjxgx9+umnT/BVIT0RLAAAAF5ARYsUt/68e/dfil6/RnkKviRJslj+1+7gwYO6ePGibt26papVqybb38yZMzV69GgdPHhQV69e1Z07d+Tu7v64ysdTiGABAADwAsqUKZP152vXr+q1sBrq1b2vJCmrz/8CgY+Pjw4dOvTAvtavX68mTZroiy++UPXq1eXh4aEZM2Zo2LBhj6V2PJ0IFgAAAC+44CLFtPjX/8rvpVzKkCGDcvh52swPDAyUs7OzIiMj1bp160TLr1u3Trlz51aPHj2s044cOfK4y8ZThmAB4LlWqusPadbXPLc060oVvqmQZn0NnJ12f8p3lu6SZn1JUsSwOmnaH4DHo2Wz1vrp5x/0YURrfdS2vQLicuvAgQOaMWOGJk6cKCcnJ3Xr1k2ffvqpHBwcVKFCBZ05c0a7du1Sq1atFBgYqKNHj2rGjBkqXbq0Fi9erHnz5qX3y8ITxlWhAAAAXnDe3j5aNHep4hPi1bDJWwoODlbHjh3l6ekpO7u7Hxd79eqlzp07q3fv3goKClLDhg11+vRpSdIbb7yhTz75RBERESpevLjWrVunXr16pedLQjrgiAUAAEAaC/s0ZXfC9r3wmAtJxrxZixJNy5snnyZ/96MkJToVSpLs7OzUo0cPm9Od/m3IkCEaMmSIzbSOHTtaf+7bt6/69u1rfT5lypRU142nG0csAAAAAJhGsAAAAABgGsECAAAAgGkECwAAAACmESwAAAAAmEawAAAAeASGYciQkd5lAKYZRtq8jwkWAAAAj+DK7Su6k3BH8XHx6V0KYMr169clSRkzZjTVD/exAAAAeAQ3429q/Yn1ejXDq8qszLJ3sE91H3EJaVfP7TtxadbXzZs306wvPL0Mw9D169d1+vRpeXp6yt4+9e/hfyNYAAAAPKIVx1dIksrdKacMdhlkkSVVy9+6nna13Ha8kWZ9Xb6ZKc36wtPP09NT3t7epvshWAAAADwiQ4aWH1+u1SdWy93BXRZL6oLFJ0vT7qPYvuCWadbXe91eTbO+8HTLmDGj6SMV9xAsAAAATLqVcEtnbp5J9XLG6bT7KHbz8p0068vJySnN+sKLg8HbAAAAAEwjWAAAAAAwjWABAAAAwDSCBQAAAADTCBYAAAAATCNYAAAAADCNy80CAJ5Zpbr+kGZ9bfm6WZr1VeGbCmnW18DZafevemfpLmnWV8SwOmnWF4DnA8ECAABJR/sFp11nmd3Tri8AeEZwKhQAAAAA0wgWAAAAAEwjWAAAAAAwjWABAAAAwDSCBQAAAADTCBYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANMIFgAAAABMI1gAAAAAMI1gAQAAAMA0ggUAAAAA0wgWAAAAAEwjWAAAAAAwjWABAAAAwDSCBQAAAADTCBYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANMIFgAAAABMI1gAAAAAMI1gAQAAAMA0ggUAAAAA0wgWAAAAAEwjWAAAAAAwLd2DxdixY+Xv7y8nJyeVLVtWGzdufGD7ixcv6qOPPpKPj48cHR2VP39+LVmy5AlVCwAAACApGdJz5TNnzlSnTp00fvx4lS1bViNHjlT16tW1d+9e5ciRI1H7uLg4VatWTTly5NCcOXPk6+urI0eOyNPT88kXDwAAAMAqXYPF8OHD1aZNG7Vo0UKSNH78eC1evFiTJk3SZ599lqj9pEmTdP78ea1bt04ZM2aUJPn7+z/JkgEAAAAkId1OhYqLi9OWLVsUFhb2v2Ls7BQWFqb169cnuczChQtVrlw5ffTRR/Ly8lKRIkU0cOBAxcfHP6myAQAAACQh3Y5YnD17VvHx8fLy8rKZ7uXlpT179iS5zKFDh/Tbb7+pSZMmWrJkiQ4cOKB27drp9u3b6tOnT5LL3Lp1S7du3bI+v3z5ctq9CAAAAACSnoLB26mRkJCgHDly6LvvvlOpUqXUsGFD9ejRQ+PHj092mUGDBsnDw8P68PPze4IVAwAAAC+GdAsW2bJlk729vU6dOmUz/dSpU/L29k5yGR8fH+XPn1/29vbWaUFBQTp58qTi4uKSXKZ79+66dOmS9fH333+n3YsAAAAAICkdg4WDg4NKlSqlyMhI67SEhARFRkaqXLlySS5ToUIFHThwQAkJCdZp+/btk4+PjxwcHJJcxtHRUe7u7jYPAAAAAGkrXU+F6tSpkyZMmKCpU6cqJiZGbdu21bVr16xXiWrWrJm6d+9ubd+2bVudP39eHTp00L59+7R48WINHDhQH330UXq9BAAAAABK58vNNmzYUGfOnFHv3r118uRJFS9eXEuXLrUO6D569Kjs7P6Xffz8/LRs2TJ98sknKlq0qHx9fdWhQwd169YtvV4CAAAAAKVzsJCkiIgIRUREJDkvKioq0bRy5crpjz/+eMxVAQAAAEiNZ+qqUAAAAACeTgQLAAAAAKYRLAAAAACYRrAAAAAAYBrBAgAAAIBpBAsAAAAAphEsAAAAAJhGsAAAAABgGsECAAAAgGnpfudtAAAAvFhKdf0hzfqa5/Z1mvUlSY0yu6dZXwNnp91H7Z2lu6RZXxHD6qRZX//GEQsAAAAAphEsAAAAAJhGsAAAAABgGsECAAAAgGkECwAAAACmESwAAAAAmEawAAAAAGAawQIAAACAaQQLAAAAAKYRLAAAAACYRrAAAAAAYBrBAgAAAIBpBAsAAAAAphEsAAAAAJhGsAAAAABgGsECAAAAgGkECwAAAACmESwAAAAAmEawAAAAAGAawQIAAACAaQQLAAAAAKYRLAAAAACYRrAAAAAAYBrBAgAAAIBpBAsAAAAAphEsAAAAAJhGsAAAAABgGsECAAAAgGkECwAAAACmESwAAAAAmEawAAAAAGAawQIAAACAaQQLAAAAAKYRLAAAAACYRrAAAAAAYBrBAgAAAIBpBAsAAAAAphEsAAAAAJhGsAAAAABgWqqChWEYOnr0qG7evPm46gEAAADwDEp1sAgICNDff//9uOoBAAAA8AxKVbCws7NTYGCgzp0797jqAQAAAPAMSvUYi8GDB6tr167666+/Hkc9AAAAAJ5BGVK7QLNmzXT9+nUVK1ZMDg4OcnZ2tpl//vz5NCsOAAAAwLMh1cFi5MiRj6EMAAAAAM+yVAeL5s2bP446AAAAADzDUh0sJCk+Pl7z589XTEyMJKlw4cJ64403ZG9vn6bFAQAAAHg2pDpYHDhwQLVq1dLx48dVoEABSdKgQYPk5+enxYsXK1++fGleJAAAAICnW6qvCtW+fXvly5dPf//9t7Zu3aqtW7fq6NGjypMnj9q3b/84agQAAADwlEv1EYvVq1frjz/+UJYsWazTsmbNqsGDB6tChQppWhwAAACAZ0Oqj1g4OjrqypUriaZfvXpVDg4OaVIUAAAAgGdLqoNF7dq19f7772vDhg0yDEOGYeiPP/7Qhx9+qDfeeONx1AgAAADgKZfqYDF69Gjly5dP5cqVk5OTk5ycnFShQgUFBARo1KhRj6NGAAAAAE+5VI+x8PT01IIFC7R//37t2bNHkhQUFKSAgIA0Lw4AAADAs+GR7mMhSYGBgQoMDEzLWgAAAAA8o1IULDp16pTiDocPH/7IxQAAAAB4NqUoWGzbti1FnVksFlPFAAAAAHg2pShYrFq16nHXAQAAAOAZluqrQgEAAADA/R5p8PbmzZs1a9YsHT16VHFxcTbz5s6dmyaFAQAAAHh2pPqIxYwZM1S+fHnFxMRo3rx5un37tnbt2qXffvtNHh4ej6NGAAAAAE+5VAeLgQMHasSIEfrvf/8rBwcHjRo1Snv27FGDBg2UK1eux1EjAAAAgKdcqoPFwYMH9frrr0uSHBwcdO3aNVksFn3yySf67rvv0rxAAAAAAE+/VAeLzJkz68qVK5IkX19f/fXXX5Kkixcv6vr162lbHQAAAIBnQoqDxb0A8corr2jFihWSpPr166tDhw5q06aNGjVqpKpVqz6eKgEAAAA81VJ8VaiiRYuqdOnSqlevnurXry9J6tGjhzJmzKh169bp7bffVs+ePR9boQAAAACeXikOFqtXr9bkyZM1aNAgDRgwQG+//bZat26tzz777HHWBwAAAOAZkOJToSpVqqRJkybpxIkT+uabbxQbG6vQ0FDlz59fX331lU6ePPk46wQAAADwFEv14G0XFxe1aNFCq1ev1r59+1S/fn2NHTtWuXLl0htvvPE4agQAAADwlEt1sPi3gIAAff755+rZs6fc3Ny0ePHitKoLAAAAwDMkxWMs7vf7779r0qRJ+uWXX2RnZ6cGDRqoVatWaVkbAAAAgGdEqoLFP//8oylTpmjKlCk6cOCAypcvr9GjR6tBgwZycXF5XDUCAAAAeMqlOFjUrFlTK1euVLZs2dSsWTO1bNlSBQoUeJy1AQAAAHhGpDhYZMyYUXPmzFHt2rVlb2//OGsCAAAA8IxJcbBYuHDh46wDAAAAwDPM1FWhAAAAAEAiWAAAAABIAwQLAAAAAKYRLAAAAACYRrAAAAAAYBrBAgAAAIBpBAsAAAAAphEsAAAAAJhGsAAAAABgGsECAAAAgGkECwAAAACmESwAAAAAmPZUBIuxY8fK399fTk5OKlu2rDZu3Jii5WbMmCGLxaJ69eo93gIBAAAAPFC6B4uZM2eqU6dO6tOnj7Zu3apixYqpevXqOn369AOXi42NVZcuXVSpUqUnVCkAAACA5KR7sBg+fLjatGmjFi1aqFChQho/frwyZcqkSZMmJbtMfHy8mjRpoi+++EJ58+Z9gtUCAAAASEq6Bou4uDht2bJFYWFh1ml2dnYKCwvT+vXrk12uX79+ypEjh1q1avUkygQAAADwEBnSc+Vnz55VfHy8vLy8bKZ7eXlpz549SS6zdu1aff/999q+fXuK1nHr1i3dunXL+vzy5cuPXC8AAACApKX7qVCpceXKFTVt2lQTJkxQtmzZUrTMoEGD5OHhYX34+fk95ioBAACAF0+6HrHIli2b7O3tderUKZvpp06dkre3d6L2Bw8eVGxsrOrUqWOdlpCQIEnKkCGD9u7dq3z58tks0717d3Xq1Mn6/PLly4QLAAAAII2la7BwcHBQqVKlFBkZab1kbEJCgiIjIxUREZGofcGCBbVz506baT179tSVK1c0atSoJAODo6OjHB0dH0v9AAAAAO5K12AhSZ06dVLz5s0VEhKiMmXKaOTIkbp27ZpatGghSWrWrJl8fX01aNAgOTk5qUiRIjbLe3p6SlKi6QAAAACenHQPFg0bNtSZM2fUu3dvnTx5UsWLF9fSpUutA7qPHj0qO7tnaigIAAAA8MJJ92AhSREREUme+iRJUVFRD1x2ypQpaV8QAAAAgFThUAAAAAAA0wgWAAAAAEwjWAAAAAAwjWABAAAAwDSCBQAAAADTCBYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANMIFgAAAABMI1gAAAAAMI1gAQAAAMA0ggUAAAAA0wgWAAAAAEwjWAAAAAAwjWABAAAAwDSCBQAAAADTCBYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANMIFgAAAABMI1gAAAAAMI1gAQAAAMA0ggUAAAAA0wgWAAAAAEwjWAAAAAAwjWABAAAAwDSCBQAAAADTCBYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANMIFgAAAABMI1gAAAAAMI1gAQAAAMA0ggUAAAAA0wgWAAAAAEwjWAAAAAAwjWABAAAAwDSCBQAAAADTCBYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANMIFgAAAABMI1gAAAAAMI1gAQAAAMA0ggUAAAAA0wgWAAAAAEwjWAAAAAAwjWABAAAAwDSCBQAAAADTCBYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANMIFgAAAABMI1gAAAAAMI1gAQAAAMA0ggUAAAAA0wgWAAAAAEwjWAAAAAAwjWABAAAAwDSCBQAAAADTCBYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANMIFgAAAABMI1gAAAAAMI1gAQAAAMA0ggUAAAAA0wgWAAAAAEwjWAAAAAAwjWABAAAAwDSCBQAAAADTCBYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANMIFgAAAABMI1gAAAAAMI1gAQAAAMA0ggUAAAAA0wgWAAAAAEwjWAAAAAAwjWABAAAAwDSCBQAAAADTCBYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANMIFgAAAABMI1gAAAAAMI1gAQAAAMA0ggUAAAAA056KYDF27Fj5+/vLyclJZcuW1caNG5NtO2HCBFWqVEmZM2dW5syZFRYW9sD2AAAAAB6/dA8WM2fOVKdOndSnTx9t3bpVxYoVU/Xq1XX69Okk20dFRalRo0ZatWqV1q9fLz8/P7322ms6fvz4E64cAAAAwD3pHiyGDx+uNm3aqEWLFipUqJDGjx+vTJkyadKkSUm2nzZtmtq1a6fixYurYMGCmjhxohISEhQZGfmEKwcAAABwT7oGi7i4OG3ZskVhYWHWaXZ2dgoLC9P69etT1Mf169d1+/ZtZcmS5XGVCQAAAOAhMqTnys+ePav4+Hh5eXnZTPfy8tKePXtS1Ee3bt2UM2dOm3Dyb7du3dKtW7eszy9fvvzoBQMAAABIUrqfCmXG4MGDNWPGDM2bN09OTk5Jthk0aJA8PDysDz8/vydcJQAAAPD8S9dgkS1bNtnb2+vUqVM200+dOiVvb+8HLjt06FANHjxYy5cvV9GiRZNt1717d126dMn6+Pvvv9OkdgAAAAD/k67BwsHBQaVKlbIZeH1vIHa5cuWSXW7IkCHq37+/li5dqpCQkAeuw9HRUe7u7jYPAAAAAGkrXcdYSFKnTp3UvHlzhYSEqEyZMho5cqSuXbumFi1aSJKaNWsmX19fDRo0SJL01VdfqXfv3po+fbr8/f118uRJSZKrq6tcXV3T7XUAAAAAL7J0DxYNGzbUmTNn1Lt3b508eVLFixfX0qVLrQO6jx49Kju7/x1YGTdunOLi4vTOO+/Y9NOnTx/17dv3SZYOAAAA4P+le7CQpIiICEVERCQ5LyoqyuZ5bGzs4y8IAAAAQKo801eFAgAAAPB0IFgAAAAAMI1gAQAAAMA0ggUAAAAA0wgWAAAAAEwjWAAAAAAwjWABAAAAwDSCBQAAAADTCBYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANMIFgAAAABMI1gAAAAAMI1gAQAAAMA0ggUAAAAA0wgWAAAAAEwjWAAAAAAwjWABAAAAwDSCBQAAAADTCBYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANMIFgAAAABMI1gAAAAAMI1gAQAAAMA0ggUAAAAA0wgWAAAAAEwjWAAAAAAwjWABAAAAwDSCBQAAAADTCBYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANMIFgAAAABMI1gAAAAAMI1gAQAAAMA0ggUAAAAA0wgWAAAAAEwjWAAAAAAwjWABAAAAwDSCBQAAAADTCBYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANMIFgAAAABMI1gAAAAAMI1gAQAAAMA0ggUAAAAA0wgWAAAAAEwjWAAAAAAwjWABAAAAwDSCBQAAAADTCBYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANMIFgAAAABMI1gAAAAAMI1gAQAAAMA0ggUAAAAA0wgWAAAAAEwjWAAAAAAwjWABAAAAwDSCBQAAAADTCBYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANMIFgAAAABMI1gAAAAAMI1gAQAAAMA0ggUAAAAA0wgWAAAAAEwjWAAAAAAwjWABAAAAwDSCBQAAAADTCBYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANMIFgAAAABMI1gAAAAAMI1gAQAAAMA0ggUAAAAA0wgWAAAAAEwjWAAAAAAwjWABAAAAwDSCBQAAAADTCBYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANMIFgAAAABMeyqCxdixY+Xv7y8nJyeVLVtWGzdufGD72bNnq2DBgnJyclJwcLCWLFnyhCoFAAAAkJR0DxYzZ85Up06d1KdPH23dulXFihVT9erVdfr06STbr1u3To0aNVKrVq20bds21atXT/Xq1dNff/31hCsHAAAAcE+6B4vhw4erTZs2atGihQoVKqTx48crU6ZMmjRpUpLtR40apRo1aqhr164KCgpS//79VbJkSY0ZM+YJVw4AAADgnnQNFnFxcdqyZYvCwsKs0+zs7BQWFqb169cnucz69ett2ktS9erVk20PAAAA4PHLkJ4rP3v2rOLj4+Xl5WUz3cvLS3v27ElymZMnTybZ/uTJk0m2v3Xrlm7dumV9funSJUnS5cuXU1xn/K0bKW77MFcyxqdZX5J058adNOvrWtp1pRu3rqdZX6nZV4/iad2/ablvJfZvWnha9+/Tum8l9m9aeFr377O0byX2b2o9S/uXz1apl5r9e6+tYRgPb2yko+PHjxuSjHXr1tlM79q1q1GmTJkkl8mYMaMxffp0m2ljx441cuTIkWT7Pn36GJJ48ODBgwcPHjx48ODxiI+///77oZ/t0/WIRbZs2WRvb69Tp07ZTD916pS8vb2TXMbb2ztV7bt3765OnTpZnyckJOj8+fPKmjWrLBaLyVfw9Lt8+bL8/Pz0999/y93dPb3LQRpj/z7f2L/PN/bv84t9+3x70favYRi6cuWKcubM+dC26RosHBwcVKpUKUVGRqpevXqS7n7wj4yMVERERJLLlCtXTpGRkerYsaN12ooVK1SuXLkk2zs6OsrR0dFmmqenZ1qU/0xxd3d/Id78Lyr27/ON/ft8Y/8+v9i3z7cXaf96eHikqF26BgtJ6tSpk5o3b66QkBCVKVNGI0eO1LVr19SiRQtJUrNmzeTr66tBgwZJkjp06KDQ0FANGzZMr7/+umbMmKHNmzfru+++S8+XAQAAALzQ0j1YNGzYUGfOnFHv3r118uRJFS9eXEuXLrUO0D569Kjs7P538ary5ctr+vTp6tmzpz7//HMFBgZq/vz5KlKkSHq9BAAAAOCFl+7BQpIiIiKSPfUpKioq0bT69eurfv36j7mq54Ojo6P69OmT6HQwPB/Yv8839u/zjf37/GLfPt/Yv8mzGEZKrh0FAAAAAMlL9ztvAwAAAHj2ESwAAAAAmEawAIAHiI2NlcVi0fbt25/oesPDw62X4U5O5cqVbS69/ST5+/tr5MiR6bLutPQ8798pU6Y89PLqKanjfs/Lvn+QqKgoWSwWXbx4Mb1LQRoz83vF++LhCBbPqOR+Me7/R3L58mX16NFDBQsWlJOTk7y9vRUWFqa5c+dab82enh9OXgTjx4+Xm5ub7ty5Y5129epVZcyYUZUrV7Zpe++P1sGDBxP9896xY4feeOMN5ciRQ05OTvL391fDhg11+vRpSf/7gHTv4ebmpsKFC+ujjz7S/v37bdYzZcoUWSwWBQUFJap39uzZslgs8vf3T7NtgOfPpk2b9P7776eo7YvwQTS9Peo2HjVqlKZMmZLm9TxLkvofWL58eZ04cSLF1+4HcBfB4jl28eJFlS9fXj/88IO6d++urVu36vfff1fDhg316aef6tKlS+ld4gvh1Vdf1dWrV7V582brtDVr1sjb21sbNmzQzZs3rdNXrVqlXLlyKV++fDZ9nDlzRlWrVlWWLFm0bNkyxcTEaPLkycqZM6euXbtm03blypU6ceKEduzYoYEDByomJkbFihVTZGSkTTsXFxedPn1a69evt5n+/fffK1euXGn18tNVXFxcepeQavHx8UpISEjvMh4qe/bsypQpU7rWwP41z8PD44W8aezDODg4yNvbWxaLJb1LwQvgWfxblhyCxXPs888/V2xsrDZs2KDmzZurUKFCyp8/v9q0aaPt27fL1dU1vUt8IRQoUEA+Pj42l06OiopS3bp1lSdPHv3xxx8201999dVEfURHR+vSpUuaOHGiSpQooTx58ujVV1/ViBEjlCdPHpu2WbNmlbe3t/Lmzau6detq5cqVKlu2rFq1aqX4+HhruwwZMqhx48aaNGmSddqxY8cUFRWlxo0bp+EWeHIqV66siIgIdezYUdmyZVP16tX1119/qWbNmnJ1dZWXl5eaNm2qs2fPWpdZunSpKlasKE9PT2XNmlW1a9fWwYMHk11HSEiIhg4dan1er149ZcyYUVevXpV0dxtaLBYdOHBAknThwgU1a9ZMmTNnVqZMmVSzZk2bI0j3jjIuXLhQhQoVkqOjo44ePZpovdeuXVOzZs3k6uoqHx8fDRs2LMXbZcyYMTb3+pk/f74sFovGjx9vnRYWFqaePXtKkg4ePKi6devKy8tLrq6uKl26tFauXGnT57+/ITcMQ3379lWuXLnk6OionDlzqn379pLu7pMjR47ok08+sR5Ne1Ts3+S3y4O28bJlyxQUFCRXV1fVqFFDJ06csM67/1SoK1euqEmTJnJxcZGPj49GjBiR5Df6169fV8uWLeXm5qZcuXI9sZvUVq5cWR9//LE6duyozJkzy8vLSxMmTLDeWNfNzU0BAQH69ddfrcusXr1aZcqUkaOjo3x8fPTZZ59ZjyCHh4dr9erVGjVqlHXbxcbGJnnKyy+//KLChQvL0dFR/v7+ifaRv7+/Bg4cmOx2iYuLU0REhHx8fOTk5KTcuXNbb/4r3f0ysHXr1sqePbvc3d1VpUoV7dixwzq/b9++Kl68uH788Uf5+/vLw8ND7777rq5cuWJtM2fOHAUHB8vZ2VlZs2ZVWFiYzZdPEydOVFBQkJycnFSwYEF9++235nfKM27x4sXy8PDQgAEDEp0WefHiRVkslkS3PtiyZYtCQkKUKVMmlS9fXnv37rXOS+nfz/79+6tZs2Zyd3dP8dHfZwHB4jmVkJCgGTNmqEmTJsqZM2ei+a6ursqQ4am4jckL4dVXX9WqVausz1etWqXKlSsrNDTUOv3GjRvasGFDksHC29tbd+7c0bx585TaK0Tb2dmpQ4cOOnLkiLZs2WIzr2XLlpo1a5auX78u6e6HoBo1alhvUPksmjp1qhwcHBQdHa3BgwerSpUqKlGihDZv3qylS5fq1KlTatCggbX9tWvX1KlTJ23evFmRkZGys7PTm2++mey3yqGhodZ/MoZhaM2aNfL09NTatWsl3f0Q4+vrq4CAAEl3P7hs3rxZCxcu1Pr162UYhmrVqqXbt29b+7x+/bq++uorTZw4Ubt27VKOHDkSrbdr165avXq1FixYoOXLlysqKkpbt25N0TYJDQ3V7t27debMGWuN2bJls76O27dva/369dZT865evapatWopMjJS27ZtU40aNVSnTp0kPxBLdz9wjRgxQv/5z3+0f/9+zZ8/X8HBwZKkuXPn6qWXXlK/fv104sQJmw+1j4L9m9iDtvH169c1dOhQ/fjjj/r999919OhRdenSJdm+OnXqpOjoaC1cuFArVqzQmjVrkqxj2LBhCgkJ0bZt29SuXTu1bdvW5sPV4zR16lRly5ZNGzdu1Mcff6y2bduqfv36Kl++vLZu3arXXntNTZs21fXr13X8+HHVqlVLpUuX1o4dOzRu3Dh9//33+vLLLyXdPRWsXLlyatOmjXXb+fn5JVrnli1b1KBBA7377rvauXOn+vbtq169eiU6jexB22X06NFauHChZs2apb1792ratGk2p5zWr19fp0+f1q+//qotW7aoZMmSqlq1qs6fP29tc/DgQc2fP1+LFi3SokWLtHr1ag0ePFiSdOLECTVq1EgtW7ZUTEyMoqKi9NZbb1n/Z0ybNk29e/fWgAEDFBMTo4EDB6pXr16aOnVqWu6eZ8r06dPVqFEjTZs2TU2aNEnxcj169NCwYcO0efNmZciQQS1btrTOS+nfz6FDh6pYsWLatm2bevXqlWavKd0ZeCaFhoYaHTp0SDR98uTJhoeHh3Hq1ClDkjF8+PBH7gtpZ8KECYaLi4tx+/Zt4/Lly0aGDBmM06dPG9OnTzdeeeUVwzAMIzIy0pBkHDlyxDAMw8idO7cxYsQIax+ff/65kSFDBiNLlixGjRo1jCFDhhgnT560zj98+LAhydi2bVui9cfExBiSjJkzZxqG8b/3iWEYRvHixY2pU6caCQkJRr58+YwFCxYYI0aMMHLnzv1YtsXjFBoaapQoUcL6vH///sZrr71m0+bvv/82JBl79+5Nso8zZ84YkoydO3cahpF4uy5cuNDw8PAw7ty5Y2zfvt3w9vY2OnToYHTr1s0wDMNo3bq10bhxY8MwDGPfvn2GJCM6Otra/9mzZw1nZ2dj1qxZhmHc3ReSjO3bt9vU0bx5c6Nu3bqGYRjGlStXDAcHB+syhmEY586dM5ydnVP0u5uQkGBkzZrVmD17tmEYd/f5oEGDDG9vb8MwDGPt2rVGxowZjWvXriXbR+HChY1vvvnG+vzf789hw4YZ+fPnN+Li4pJc9v738qNi/yYvqW18b90HDhywThs7dqzh5eWVZB2XL182MmbMaH2fGIZhXLx40ciUKZNNHblz5zbee+896/OEhAQjR44cxrhx41JUqxmhoaFGxYoVrc/v3LljuLi4GE2bNrVOO3HihCHJWL9+vfH5558bBQoUMBISEqzzx44da7i6uhrx8fHWPu/fzqtWrTIkGRcuXDAMwzAaN25sVKtWzaZN165djUKFClmfP2y7fPzxx0aVKlVsarlnzZo1hru7u3Hz5k2b6fny5TP+85//GIZhGH369DEyZcpkXL582aaGsmXLGoZhGFu2bDEkGbGxsUluu3z58hnTp0+3mda/f3+jXLlySbZ/Xt3b32PGjDE8PDyMqKgowzCS/h964cIFQ5KxatUqwzD+975YuXKltc3ixYsNScaNGzeSXWdSfz/r1auXti/sKcERi+eUwX0PnyqVK1fWtWvXtGnTJq1Zs0b58+dX9uzZFRoaah1nERUVpbx58yY7vmHAgAE6efKkxo8fr8KFC2v8+PEqWLCgdu7c+dD133s/JHUaSsuWLTV58mStXr1a165dU61atcy92HRWqlQp6887duzQqlWr5Orqan0ULFhQkqynw+zfv1+NGjVS3rx55e7ubv0GMblv5ytVqqQrV65o27ZtWr16tUJDQ1W5cmXrt9yrV6+2fvMfExOjDBkyqGzZstbls2bNqgIFCigmJsY6zcHBQUWLFk32NR08eFBxcXE2/WTJkkUFChRI0TaxWCx65ZVXFBUVpYsXL2r37t1q166dbt26pT179mj16tUqXbq0dczE1atX1aVLFwUFBcnT01Ourq6KiYlJdpvUr19fN27cUN68edWmTRvNmzfP5mIFaYn9mzqZMmWyGbPl4+NjveDD/Q4dOqTbt2+rTJky1mkeHh5J1vHv12OxWOTt7Z1sv2nt3+u2t7dX1qxZrUfIJFmPuJ4+fVoxMTEqV66czd++ChUq6OrVqzp27FiK1xkTE6MKFSrYTKtQoYL2799vc4rpg7ZLeHi4tm/frgIFCqh9+/Zavny5te2OHTt09epVZc2a1eb9fPjwYZtT9/z9/eXm5mZ9/u/9WaxYMVWtWlXBwcGqX7++JkyYoAsXLki6e+Tu4MGDatWqlU3/X3755QNPDXxezZkzR5988olWrFih0NDQVC//7/3s4+MjSdb9kNK/nyEhISZewdOLc2GeUe7u7kkOvr548aI8PDyUPXt2eXp6as+ePelQHe4XEBCgl156SatWrdKFCxesf8hy5swpPz8/rVu3TqtWrVKVKlUe2E/WrFlVv3591a9fXwMHDlSJEiU0dOjQhx7Kvvch5/7xGJLUpEkTffrpp+rbt6+aNm36zJ8i5+LiYv356tWrqlOnjr766qtE7e79M6hTp45y586tCRMmKGfOnEpISFCRIkWSHUzn6empYsWKKSoqSuvXr1e1atX0yiuvqGHDhtq3b5/279+f6n9Uzs7Oj32QaOXKlfXdd99pzZo1KlGihNzd3a1h494H6Hu6dOmiFStWaOjQoQoICJCzs7PeeeedZLeJn5+f9u7dq5UrV2rFihVq166dvv76a61evVoZM2ZM09fB/k2d+7e/xWJJky+ekur3SQ1KT2rd/552b1unxyD5B22XkiVL6vDhw/r111+1cuVKNWjQQGFhYZozZ46uXr2aaCzePf8eXP+g/u3t7bVixQqtW7dOy5cv1zfffKMePXpow4YN1i8NJkyYYBNg7y33oilRooS2bt2qSZMmKSQkRBaLRXZ2d79r//fvx79Pafy3B73fUvr3899/y54nHLF4RhUoUCDJ8163bt2q/Pnzy87OTu+++66mTZumf/75J1G7q1evPrZvFJG0V199VVFRUYqKirK5zOwrr7yiX3/9VRs3bkxyfEVyHBwclC9fvkRXhbpfQkKCRo8erTx58qhEiRKJ5mfJkkVvvPGGVq9ebXOe6POgZMmS2rVrl/z9/RUQEGDzcHFx0blz57R371717NlTVatWVVBQkPUbvge5Nzbm999/V+XKlZUlSxYFBQVpwIAB8vHxUf78+SVJQUFBunPnjjZs2GBd9t46CxUqlOLXkS9fPmXMmNGmnwsXLmjfvn0p7uPeOIvZs2db33+VK1fWypUrFR0dbfOejI6OVnh4uN58800FBwfL29tbsbGxD+zf2dlZderU0ejRo60fyu8dTXNwcLD5VjetsH//Jy22cd68eZUxY0Zt2rTJOu3SpUupquNpExQUZB37ck90dLTc3Nz00ksvSUrZtgsKClJ0dLTNtOjoaOXPnz9VH8zd3d3VsGFDTZgwQTNnztQvv/yi8+fPq2TJkjp58qQyZMiQ6L2cLVu2FPdvsVhUoUIFffHFF9q2bZscHBw0b948eXl5KWfOnDp06FCi/pP6wul5ly9fPq1atUoLFizQxx9/LOnule4k2YxRepT72zzK38/nCcHiGdW2bVvt27dP7du3159//qm9e/dq+PDh+vnnn9W5c2dJd0+d8fPzU9myZfXDDz9o9+7d2r9/vyZNmqQSJUpYr3KCJ+PVV1/V2rVrtX37dptvPENDQ/Wf//xHcXFxyQaLRYsW6b333tOiRYu0b98+7d27V0OHDtWSJUtUt25dm7bnzp3TyZMndejQIS1cuFBhYWHauHGjvv/++2T/AU6ZMkVnz561nkbyvPjoo490/vx5NWrUSJs2bdLBgwe1bNkytWjRQvHx8cqcObOyZs2q7777TgcOHNBvv/2mTp06PbTfypUra9myZcqQIYN1m1WuXFnTpk2z2beBgYGqW7eu2rRpo7Vr12rHjh1677335Ovrm2i/PYirq6tatWqlrl276rffftNff/2l8PBw6zdsKVG0aFFlzpxZ06dPtwkW8+fP161bt2xO8wgMDNTcuXO1fft27dixQ40bN37gt79TpkzR999/r7/++kuHDh3STz/9JGdnZ+XOnVvS3dM3fv/9dx0/ftzmik1msX//Jy22sZubm5o3b66uXbtq1apV2rVrl1q1aiU7O7tn9rKr7dq1099//62PP/5Ye/bs0YIFC9SnTx916tTJun39/f21YcMGxcbG6uzZs0m+1zt37qzIyEj1799f+/bt09SpUzVmzJgHDoS/373/0Xv27NG+ffs0e/ZseXt7y9PTU2FhYSpXrpzq1aun5cuXKzY2VuvWrVOPHj1sLlX+IBs2bNDAgQO1efNmHT16VHPnztWZM2es9yv64osvNGjQII0ePVr79u3Tzp07NXnyZA0fPjzFr+F5kj9/fq1atUq//PKLOnbsKGdnZ7388ssaPHiwYmJitHr1auuV8lIjtX8/nzcEi2dU3rx59fvvv2vPnj0KCwtT2bJlNWvWLM2ePVs1atSQdPeb6D/++EPvvfeevvzyS5UoUUKVKlXSzz//rK+//pob/zxhr776qm7cuKGAgACbqy6FhobqypUr1svSJqVQoULKlCmTOnfurOLFi+vll1/WrFmzNHHiRDVt2tSmbVhYmHx8fBQcHKzPPvtMQUFB+vPPPx94NOTepQmfNzlz5lR0dLTi4+P12muvKTg4WB07dpSnp6fs7OxkZ2enGTNmaMuWLSpSpIg++eQTff311w/tt1KlSkpISLD5kFm5cmXFx8cnuunh5MmTVapUKdWuXVvlypWTYRhasmRJqk8R+vrrr1WpUiXVqVNHYWFhqlixos14g4exWCyqVKmSLBaLKlasKOlu2HB3d1dISIjNYfnhw4crc+bMKl++vOrUqaPq1aurZMmSyfbt6empCRMmqEKFCipatKhWrlyp//73v9b3VL9+/RQbG6t8+fJZvxVMC+zf/0mrbTx8+HCVK1dOtWvXVlhYmCpUqGC9POmzyNfXV0uWLNHGjRtVrFgxffjhh2rVqpXNB8YuXbrI3t5ehQoVUvbs2ZMcf1OyZEnNmjVLM2bMUJEiRdS7d2/169dP4eHhKa7Fzc1NQ4YMUUhIiEqXLq3Y2FgtWbLEGtyWLFmiV155RS1atFD+/Pn17rvv6siRIym+Sp+7u7t+//131apVS/nz51fPnj01bNgw1axZU5LUunVrTZw4UZMnT1ZwcLBCQ0M1ZcqUF/KIxT0FChTQb7/9Zv1SdtKkSbpz545KlSqljh07Wq8elhqp/fv5vLEYjPIFAABJuHbtmnx9fTVs2DC1atUqvcsB8JR7tkdpAgCANLNt2zbt2bNHZcqU0aVLl9SvXz9JStWpXQBeXAQLAHjGrVmzxnq6Q1IYT/Vse9L7d+jQodq7d68cHBxUqlQprVmzJlUDiAG8uDgVCgCecTdu3NDx48eTnX/vLtF4NrF/ATwrCBYAAAAATOOqUAAAAABMI1gAAAAAMI1gAQAAAMA0ggUAAAAA0wgWAAAAAEwjWAAAHio8PFwWi0UWi0UZM2aUl5eXqlWrpkmTJikhISHF/UyZMkWenp6Pr9BkhIeHq169ek98vQDwIiFYAABSpEaNGjpx4oRiY2P166+/6tVXX1WHDh1Uu3Zt3blzJ73LAwCkM4IFACBFHB0d5e3tLV9fX5UsWVKff/65FixYoF9//VVTpkyRJA0fPlzBwcFycXGRn5+f2rVrZ70zdFRUlFq0aKFLly5Zj3707dtXkvTjjz8qJCREbm5u8vb2VuPGjXX69Gnrui9cuKAmTZooe/bscnZ2VmBgoCZPnmyd//fff6tBgwby9PRUlixZVLduXcXGxkqS+vbtq6lTp2rBggXW9UZFRT2JTQYALxSCBQDgkVWpUkXFihXT3LlzJUl2dnYaPXq0du3apalTp+q3337Tp59+KkkqX768Ro4cKXd3d504cUInTpxQly5dJEm3b99W//79tWPHDs2fP1+xsbEKDw+3rqdXr17avXu3fv31V8XExGjcuHHKli2bddnq1avLzc1Na9asUXR0tFxdXVWjRg3FxcWpS5cuatCggfWIy4kTJ1S+fPknu6EA4AWQIb0LAAA82woWLKg///xTktSxY0frdH9/f3355Zf68MMP9e2338rBwUEeHh6yWCzy9va26aNly5bWn/PmzavRo0erdOnSunr1qlxdXXX06FGVKFFCISEh1r7vmTlzphISEjRx4kRZLBZJ0uTJk+Xp6amoqCi99tprcnZ21q1btxKtFwCQdjhiAQAwxTAM6wf6lStXqmrVqvL19ZWbm5uaNm2qc+fO6fr16w/sY8uWLapTp45y5colNzc3hYaGSpKOHj0qSWrbtq1mzJih4sWL69NPP9W6deusy+7YsUMHDhyQm5ubXF1d5erqqixZsujmzZs6ePDgY3rVAID7ESwAAKbExMQoT548io2NVe3atVW0aFH98ssv2rJli8aOHStJiouLS3b5a9euqXr16nJ3d9e0adO0adMmzZs3z2a5mjVr6siRI/rkk0/0zz//qGrVqtbTqK5evapSpUpp+/btNo99+/apcePGj/nVAwDu4VQoAMAj++2337Rz50598skn2rJlixISEjRs2DDZ2d393mrWrFk27R0cHBQfH28zbc+ePTp37pwGDx4sPz8/SdLmzZsTrSt79uxq3ry5mjdvrkqVKqlr164aOnSoSpYsqZkzZypHjhxyd3dPss6k1gsASFscsQAApMitW7d08uRJHT9+XFu3btXAgQNVt25d1a5dW82aNVNAQIBu376tb775RocOHdKPP/6o8ePH2/Th7++vq1evKjIyUmfPntX169eVK1cuOTg4WJdbuHCh+vfvb7Nc7969tWDBAh04cEC7du3SokWLFBQUJElq0qSJsmXLprp162rNmjU6fPiwoqKi1L59ex07dsy63j///FN79+7V2bNndfv27Sez0QDgBUKwAACkyNKlS+Xj4yN/f3/VqFFDq1at0ujRo7VgwQLZ29urWLFiGj58uL766isVKVJE06ZN06BBg2z6KF++vD788EM1bNhQ2bNn15AhQ5Q9e3ZNmTJFs2fPVqFChTR48GANHTrUZjkHBwd1795dRYsW1SuvvCJ7e3vNmDFDkpQpUyb9/vvvypUrl9566y0FBQWpVatWunnzpvUIRps2bVSgQAGFhIQoe/bsio6OfjIbDQBeIBbDMIz0LgIAAADAs40jFgAAAABMI1gAAAAAMI1gAQAAAMA0ggUAAAAA0wgWAAAAAEwjWAAAAAAwjWABAAAAwDSCBQAAAADTCBYAAAAATCNYAAAAADCNYAEAAADANIIFAAAAANP+D3Q2e5PI2J+6AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convertendo as métricas para float para visualização\n",
    "df_numeric = df.replace(r' ± .*', '', regex=True).astype(float)\n",
    "\n",
    "# Criando um gráfico de barras\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(data=df_numeric.reset_index().melt(id_vars='index'), \n",
    "            x='index', y='value', hue='variable')\n",
    "plt.title('Métricas de Classificação por Dataset')\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('Valor')\n",
    "plt.legend(title='Métricas')\n",
    "#plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
