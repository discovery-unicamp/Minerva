

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../../../">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>minerva.models.nets.image.vit &mdash; minerva  documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../../../../_static/graphviz.css?v=4ae1632d" />

  
      <script src="../../../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../../../_static/documentation_options.js?v=5929fcd5"></script>
      <script src="../../../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../index.html" class="icon icon-home">
            minerva
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../getting_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../design.html">Minerva Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tutorials.html">Examples and Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../contributing.html">Contributing to Minerva</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../api.html">Programming Reference</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../index.html">minerva</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">minerva.models.nets.image.vit</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for minerva.models.nets.image.vit</h1><div class="highlight"><pre>
<span></span><span class="c1"># Standard library imports</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">collections</span><span class="w"> </span><span class="kn">import</span> <span class="n">OrderedDict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">functools</span><span class="w"> </span><span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="c1"># Third-party imports</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">lightning</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">L</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">timm.models.vision_transformer</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">timm.models.vision_transformer</span><span class="w"> </span><span class="kn">import</span> <span class="n">Block</span><span class="p">,</span> <span class="n">PatchEmbed</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.models.vision_transformer</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">Conv2dNormActivation</span><span class="p">,</span>
    <span class="n">ConvStemConfig</span><span class="p">,</span>
    <span class="n">EncoderBlock</span><span class="p">,</span>
    <span class="n">_log_api_usage_once</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">minerva.models.nets.base</span><span class="w"> </span><span class="kn">import</span> <span class="n">SimpleSupervisedModel</span>

<span class="c1"># Local imports</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">minerva.utils.position_embedding</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_2d_sincos_pos_embed</span>


<div class="viewcode-block" id="_Encoder">
<a class="viewcode-back" href="../../../../../autoapi/minerva/models/nets/image/vit/index.html#minerva.models.nets.image.vit._Encoder">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">_Encoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Transformer Model Encoder for sequence to sequence translation.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">seq_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_layers</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">mlp_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">attention_dropout</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">aux_output</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">aux_output_layers</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">norm_layer</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">),</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Note that batch_size is on the first dim because</span>
        <span class="c1"># we have batch_first=True in nn.MultiAttention() by default</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">aux_output</span> <span class="o">=</span> <span class="n">aux_output</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">aux_output_layers</span> <span class="o">=</span> <span class="n">aux_output_layers</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">pos_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">std</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
        <span class="p">)</span>  <span class="c1"># from BERT</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        <span class="n">layers</span><span class="p">:</span> <span class="n">OrderedDict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">):</span>
            <span class="n">layers</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;encoder_layer_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">EncoderBlock</span><span class="p">(</span>
                <span class="n">num_heads</span><span class="p">,</span>
                <span class="n">hidden_dim</span><span class="p">,</span>
                <span class="n">mlp_dim</span><span class="p">,</span>
                <span class="n">dropout</span><span class="p">,</span>
                <span class="n">attention_dropout</span><span class="p">,</span>
                <span class="n">norm_layer</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ln</span> <span class="o">=</span> <span class="n">norm_layer</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">)</span>

<div class="viewcode-block" id="_Encoder.forward">
<a class="viewcode-back" href="../../../../../autoapi/minerva/models/nets/image/vit/index.html#minerva.models.nets.image.vit._Encoder.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_assert</span><span class="p">(</span>
            <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;Expected (batch_size, seq_length, hidden_dim) got </span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_embedding</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">aux_output</span><span class="p">:</span>
            <span class="n">aux_outputs</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">):</span>
                <span class="nb">input</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">aux_output_layers</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
                    <span class="n">aux_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ln</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="nb">input</span><span class="p">)))</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">ln</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="nb">input</span><span class="p">)),</span> <span class="n">aux_outputs</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">ln</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="nb">input</span><span class="p">)))</span></div>
</div>



<div class="viewcode-block" id="_VisionTransformerBackbone">
<a class="viewcode-back" href="../../../../../autoapi/minerva/models/nets/image/vit/index.html#minerva.models.nets.image.vit._VisionTransformerBackbone">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">_VisionTransformerBackbone</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Vision Transformer as per https://arxiv.org/abs/2010.11929.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">image_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]],</span>
        <span class="n">patch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_layers</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">mlp_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">original_resolution</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">attention_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">num_classes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
        <span class="n">aux_output</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">aux_output_layers</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">norm_layer</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">),</span>
        <span class="n">conv_stem_configs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">ConvStemConfig</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes a Vision Transformer (ViT) model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        image_size : int or Tuple[int, int]</span>
<span class="sd">            The size of the input image. If an int is provided, it is assumed</span>
<span class="sd">            to be a square image. If a tuple of ints is provided, it represents</span>
<span class="sd">            the height and width of the image.</span>
<span class="sd">        patch_size : int</span>
<span class="sd">            The size of each patch in the image.</span>
<span class="sd">        num_layers : int</span>
<span class="sd">            The number of transformer layers in the model.</span>
<span class="sd">        num_heads : int</span>
<span class="sd">            The number of attention heads in the transformer layers.</span>
<span class="sd">        hidden_dim : int</span>
<span class="sd">            The dimensionality of the hidden layers in the transformer.</span>
<span class="sd">        mlp_dim : int</span>
<span class="sd">            The dimensionality of the feed-forward MLP layers in the transformer</span>
<span class="sd">        original_resolution : Tuple[int, int], optional</span>
<span class="sd">            The original resolution of the input image in the pre-training</span>
<span class="sd">            weights. When None, positional embeddings will not be interpolated.</span>
<span class="sd">            Defaults to None.</span>
<span class="sd">        dropout : float, optional</span>
<span class="sd">            The dropout rate to apply. Defaults to 0.0.</span>
<span class="sd">        attention_dropout : float, optional</span>
<span class="sd">            The dropout rate to apply to the attention weights. Defaults to 0.0</span>
<span class="sd">        num_classes : int, optional</span>
<span class="sd">            The number of output classes. Defaults to 1000.</span>
<span class="sd">        norm_layer : Callable[..., torch.nn.Module], optional</span>
<span class="sd">            The normalization layer to use. Defaults to nn.LayerNorm with</span>
<span class="sd">            epsilon=1e-6.</span>
<span class="sd">        conv_stem_configs : List[ConvStemConfig], optional</span>
<span class="sd">            The configuration for the convolutional stem layers.</span>
<span class="sd">            If provided, the input image will be processed by these</span>
<span class="sd">            convolutional layers before being passed to the transformer.</span>
<span class="sd">            Defaults to None.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">_log_api_usage_once</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">aux_output</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">aux_output_layers</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span>
                <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_layers</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">aux_output_layers</span>
            <span class="p">),</span> <span class="s2">&quot;Invalid layer index in aux_output_layers&quot;</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">image_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">_assert</span><span class="p">(</span>
                <span class="n">image_size</span> <span class="o">%</span> <span class="n">patch_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span>
                <span class="s2">&quot;Input shape indivisible by patch size!&quot;</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">image_size</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">_assert</span><span class="p">(</span>
                <span class="n">image_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">%</span> <span class="n">patch_size</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">image_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="n">patch_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span>
                <span class="s2">&quot;Input shape indivisible by patch size!&quot;</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">image_size</span> <span class="o">=</span> <span class="n">image_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">patch_size</span> <span class="o">=</span> <span class="n">patch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_dim</span> <span class="o">=</span> <span class="n">mlp_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_dropout</span> <span class="o">=</span> <span class="n">attention_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">=</span> <span class="n">num_classes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm_layer</span> <span class="o">=</span> <span class="n">norm_layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">aux_output</span> <span class="o">=</span> <span class="n">aux_output</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">aux_output_layers</span> <span class="o">=</span> <span class="n">aux_output_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">original_resolution</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">original_resolution</span> <span class="k">if</span> <span class="n">original_resolution</span> <span class="k">else</span> <span class="n">image_size</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">conv_stem_configs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># As per https://arxiv.org/abs/2106.14881</span>
            <span class="n">seq_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
            <span class="n">prev_channels</span> <span class="o">=</span> <span class="mi">3</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">conv_stem_layer_config</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">conv_stem_configs</span><span class="p">):</span>
                <span class="n">seq_proj</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;conv_bn_relu_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                    <span class="n">Conv2dNormActivation</span><span class="p">(</span>
                        <span class="n">in_channels</span><span class="o">=</span><span class="n">prev_channels</span><span class="p">,</span>
                        <span class="n">out_channels</span><span class="o">=</span><span class="n">conv_stem_layer_config</span><span class="o">.</span><span class="n">out_channels</span><span class="p">,</span>
                        <span class="n">kernel_size</span><span class="o">=</span><span class="n">conv_stem_layer_config</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span>
                        <span class="n">stride</span><span class="o">=</span><span class="n">conv_stem_layer_config</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                        <span class="n">norm_layer</span><span class="o">=</span><span class="n">conv_stem_layer_config</span><span class="o">.</span><span class="n">norm_layer</span><span class="p">,</span>
                        <span class="n">activation_layer</span><span class="o">=</span><span class="n">conv_stem_layer_config</span><span class="o">.</span><span class="n">activation_layer</span><span class="p">,</span>
                    <span class="p">),</span>
                <span class="p">)</span>
                <span class="n">prev_channels</span> <span class="o">=</span> <span class="n">conv_stem_layer_config</span><span class="o">.</span><span class="n">out_channels</span>
            <span class="n">seq_proj</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span>
                <span class="s2">&quot;conv_last&quot;</span><span class="p">,</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
                    <span class="n">in_channels</span><span class="o">=</span><span class="n">prev_channels</span><span class="p">,</span>
                    <span class="n">out_channels</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">,</span>
                    <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">conv_proj</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">seq_proj</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">conv_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
                <span class="n">in_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                <span class="n">out_channels</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">,</span>
                <span class="n">kernel_size</span><span class="o">=</span><span class="n">patch_size</span><span class="p">,</span>
                <span class="n">stride</span><span class="o">=</span><span class="n">patch_size</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">image_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">seq_length</span> <span class="o">=</span> <span class="p">(</span><span class="n">image_size</span> <span class="o">//</span> <span class="n">patch_size</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">image_size</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="n">seq_length</span> <span class="o">=</span> <span class="p">(</span><span class="n">image_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="n">patch_size</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">image_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="n">patch_size</span><span class="p">)</span>

        <span class="c1"># Add a class token</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">class_token</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">))</span>
        <span class="n">seq_length</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">_Encoder</span><span class="p">(</span>
            <span class="n">seq_length</span><span class="o">=</span><span class="n">seq_length</span><span class="p">,</span>
            <span class="n">num_layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
            <span class="n">hidden_dim</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">,</span>
            <span class="n">mlp_dim</span><span class="o">=</span><span class="n">mlp_dim</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
            <span class="n">attention_dropout</span><span class="o">=</span><span class="n">attention_dropout</span><span class="p">,</span>
            <span class="n">norm_layer</span><span class="o">=</span><span class="n">norm_layer</span><span class="p">,</span>
            <span class="n">aux_output</span><span class="o">=</span><span class="n">aux_output</span><span class="p">,</span>
            <span class="n">aux_output_layers</span><span class="o">=</span><span class="n">aux_output_layers</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span> <span class="o">=</span> <span class="n">seq_length</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv_proj</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">):</span>
            <span class="c1"># Init the patchify stem</span>
            <span class="n">fan_in</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">conv_proj</span><span class="o">.</span><span class="n">in_channels</span>
                <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_proj</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_proj</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">trunc_normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv_proj</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">fan_in</span><span class="p">))</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_proj</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv_proj</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_proj</span><span class="o">.</span><span class="n">conv_last</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">conv_proj</span><span class="o">.</span><span class="n">conv_last</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span>
        <span class="p">):</span>
            <span class="c1"># Init the last 1x1 conv of the conv stem</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">conv_proj</span><span class="o">.</span><span class="n">conv_last</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
                <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                <span class="n">std</span><span class="o">=</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.0</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_proj</span><span class="o">.</span><span class="n">conv_last</span><span class="o">.</span><span class="n">out_channels</span><span class="p">),</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_proj</span><span class="o">.</span><span class="n">conv_last</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv_proj</span><span class="o">.</span><span class="n">conv_last</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>

<div class="viewcode-block" id="_VisionTransformerBackbone._process_input">
<a class="viewcode-back" href="../../../../../autoapi/minerva/models/nets/image/vit/index.html#minerva.models.nets.image.vit._VisionTransformerBackbone._process_input">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">_process_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Process the input tensor and return the reshaped tensor and dimensions.</span>

<span class="sd">        Args:</span>
<span class="sd">            x (torch.Tensor): The input tensor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple[torch.Tensor, int, int]: The reshaped tensor, number of rows,</span>
<span class="sd">            and number of columns.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_size</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">image_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">_assert</span><span class="p">(</span>
                <span class="n">h</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">image_size</span><span class="p">,</span>
                <span class="sa">f</span><span class="s2">&quot;Wrong image height! Expected </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">image_size</span><span class="si">}</span><span class="s2"> but got </span><span class="si">{</span><span class="n">h</span><span class="si">}</span><span class="s2">!&quot;</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">_assert</span><span class="p">(</span>
                <span class="n">w</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">image_size</span><span class="p">,</span>
                <span class="sa">f</span><span class="s2">&quot;Wrong image width! Expected </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">image_size</span><span class="si">}</span><span class="s2"> but got </span><span class="si">{</span><span class="n">w</span><span class="si">}</span><span class="s2">!&quot;</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">image_size</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">_assert</span><span class="p">(</span>
                <span class="n">h</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">image_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="sa">f</span><span class="s2">&quot;Wrong image height! Expected </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">image_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> but got </span><span class="si">{</span><span class="n">h</span><span class="si">}</span><span class="s2">!&quot;</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">_assert</span><span class="p">(</span>
                <span class="n">w</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">image_size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                <span class="sa">f</span><span class="s2">&quot;Wrong image width! Expected </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">image_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> but got </span><span class="si">{</span><span class="n">w</span><span class="si">}</span><span class="s2">!&quot;</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid image size type!&quot;</span><span class="p">)</span>

        <span class="n">n_h</span> <span class="o">=</span> <span class="n">h</span> <span class="o">//</span> <span class="n">p</span>
        <span class="n">n_w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">//</span> <span class="n">p</span>

        <span class="c1"># (n, c, h, w) -&gt; (n, hidden_dim, n_h, n_w)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># (n, hidden_dim, n_h, n_w) -&gt; (n, hidden_dim, (n_h * n_w))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">n_h</span> <span class="o">*</span> <span class="n">n_w</span><span class="p">)</span>

        <span class="c1"># (n, hidden_dim, (n_h * n_w)) -&gt; (n, (n_h * n_w), hidden_dim)</span>
        <span class="c1"># The self attention layer expects inputs in the format (N, S, E)</span>
        <span class="c1"># where S is the source sequence length, N is the batch size, E is the</span>
        <span class="c1"># embedding dimension</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_h</span><span class="p">,</span> <span class="n">n_w</span></div>


<div class="viewcode-block" id="_VisionTransformerBackbone.interpolate_pos_embeddings">
<a class="viewcode-back" href="../../../../../autoapi/minerva/models/nets/image/vit/index.html#minerva.models.nets.image.vit._VisionTransformerBackbone.interpolate_pos_embeddings">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">interpolate_pos_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pretrained_pos_embed</span><span class="p">,</span> <span class="n">new_img_size</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Interpolate encoder&#39;s positional embeddings to fit a new input size.</span>

<span class="sd">        Args:</span>
<span class="sd">            pretrained_pos_embed (torch.Tensor): Pretrained positional embeddings.</span>
<span class="sd">            new_img_size (Tuple[int, int]): New height and width of the input image.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">new_img_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_size</span><span class="p">,</span>
            <span class="n">new_img_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_size</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">new_grid_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>

        <span class="c1"># Reshape pretrained positional embeddings to match the original grid size</span>

        <span class="n">original_resolution</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">original_resolution</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">original_resolution</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">)</span>
            <span class="k">else</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">original_resolution</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">original_resolution</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="n">pos_embed_reshaped</span> <span class="o">=</span> <span class="n">pretrained_pos_embed</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="mi">1</span><span class="p">,</span>
            <span class="n">original_resolution</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_size</span><span class="p">,</span>
            <span class="n">original_resolution</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_size</span><span class="p">,</span>
            <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Interpolate positional embeddings to the new grid size</span>
        <span class="n">pos_embed_interpolated</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span>
                <span class="n">pos_embed_reshaped</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span>
                    <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span>
                <span class="p">),</span>  <span class="c1"># (1, C, H, W) for interpolation</span>
                <span class="n">size</span><span class="o">=</span><span class="n">new_grid_size</span><span class="p">,</span>
                <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;bilinear&quot;</span><span class="p">,</span>
                <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">pos_embed_reshaped</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="p">)</span>

        <span class="c1"># Concatenate the CLS token and the interpolated positional embeddings</span>
        <span class="n">cls_token</span> <span class="o">=</span> <span class="n">pretrained_pos_embed</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">pos_embed_interpolated</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">cls_token</span><span class="p">,</span> <span class="n">pos_embed_interpolated</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">pos_embed_interpolated</span>

        <span class="k">return</span> <span class="n">pos_embed_interpolated</span></div>


<div class="viewcode-block" id="_VisionTransformerBackbone.load_backbone">
<a class="viewcode-back" href="../../../../../autoapi/minerva/models/nets/image/vit/index.html#minerva.models.nets.image.vit._VisionTransformerBackbone.load_backbone">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_backbone</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">freeze</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Loads pretrained weights and handles positional embedding resizing</span>
<span class="sd">        if necessary.&quot;&quot;&quot;</span>
        <span class="c1"># Load the pretrained state dict</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>

        <span class="c1"># Expected shape for positional embeddings based on current model image size</span>

        <span class="n">image_size</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">image_size</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">image_size</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">)</span>
            <span class="k">else</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">image_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">image_size</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="n">expected_pos_embed_shape</span> <span class="o">=</span> <span class="p">(</span>
            <span class="mi">1</span><span class="p">,</span>
            <span class="p">(</span><span class="n">image_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_size</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">image_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_size</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Check if positional embeddings need interpolation</span>
        <span class="k">if</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;encoder.pos_embedding&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">expected_pos_embed_shape</span><span class="p">:</span>
            <span class="c1"># Extract the positional embeddings from the state dict</span>
            <span class="n">pretrained_pos_embed</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;encoder.pos_embedding&quot;</span><span class="p">]</span>

            <span class="c1"># Interpolate to match the current image size</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Interpolating positional embeddings to match the new image size.&quot;</span><span class="p">)</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="n">pos_embed_interpolated</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">interpolate_pos_embeddings</span><span class="p">(</span>
                    <span class="n">pretrained_pos_embed</span><span class="p">,</span> <span class="p">(</span><span class="n">image_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">image_size</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                <span class="p">)</span>
            <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;encoder.pos_embedding&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pos_embed_interpolated</span>

        <span class="c1"># Load the (potentially modified) state dict into the encoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Optionally freeze parameters</span>
        <span class="k">if</span> <span class="n">freeze</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
                <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span></div>


<div class="viewcode-block" id="_VisionTransformerBackbone.forward">
<a class="viewcode-back" href="../../../../../autoapi/minerva/models/nets/image/vit/index.html#minerva.models.nets.image.vit._VisionTransformerBackbone.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Forward pass of the Vision Transformer Backbone.</span>

<span class="sd">        Args:</span>
<span class="sd">            x (torch.Tensor): The input tensor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            torch.Tensor: The output tensor.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Reshape and permute the input tensor</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">n_h</span><span class="p">,</span> <span class="n">n_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_process_input</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">n</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Expand the class token to the full batch</span>
        <span class="n">batch_class_token</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">class_token</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">batch_class_token</span><span class="p">,</span> <span class="n">x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">aux_output</span><span class="p">:</span>
            <span class="n">x</span><span class="p">,</span> <span class="n">aux_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span>
            <span class="n">B</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">n_h</span><span class="p">,</span> <span class="n">n_w</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">aux_output</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">aux_outputs</span><span class="p">):</span>
                <span class="n">aux_outputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">aux_output</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span>
                <span class="n">B</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">aux_outputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
                <span class="n">aux_outputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">aux_outputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                    <span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">n_h</span><span class="p">,</span> <span class="n">n_w</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
                    <span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
                    <span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">aux_outputs</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Classifier &quot;token&quot; as used by standard language architectures</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span>

        <span class="n">B</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">n_h</span><span class="p">,</span> <span class="n">n_w</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">x</span></div>


<div class="viewcode-block" id="_VisionTransformerBackbone.load_weights">
<a class="viewcode-back" href="../../../../../autoapi/minerva/models/nets/image/vit/index.html#minerva.models.nets.image.vit._VisionTransformerBackbone.load_weights">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">freeze</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>

        <span class="n">state_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">weights_path</span><span class="p">)</span>

        <span class="c1"># Get expected positional embedding shape based on current image size</span>

        <span class="n">image_size</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">image_size</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">image_size</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">)</span>
            <span class="k">else</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">image_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">image_size</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="n">expected_pos_embed_shape</span> <span class="o">=</span> <span class="p">(</span>
            <span class="mi">1</span><span class="p">,</span>
            <span class="p">(</span><span class="n">image_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_size</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">image_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_size</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Check if positional embeddings need interpolation</span>
        <span class="k">if</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;encoder.pos_embedding&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">expected_pos_embed_shape</span><span class="p">:</span>
            <span class="c1"># Extract the positional embeddings from the state dict</span>
            <span class="n">pretrained_pos_embed</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;encoder.pos_embedding&quot;</span><span class="p">]</span>

            <span class="c1"># Interpolate to match the current image size</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Interpolating positional embeddings to match the new image size.&quot;</span><span class="p">)</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="n">pos_embed_interpolated</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">interpolate_pos_embeddings</span><span class="p">(</span>
                    <span class="n">pretrained_pos_embed</span><span class="p">,</span> <span class="p">(</span><span class="n">image_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">image_size</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                <span class="p">)</span>
            <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;encoder.pos_embedding&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pos_embed_interpolated</span>

        <span class="c1"># Load the (potentially modified) state dict</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Optionally freeze parameters</span>
        <span class="k">if</span> <span class="n">freeze</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
                <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span></div>
</div>



<span class="c1">###################################</span>

<span class="c1">############### SFM ###############</span>

<span class="c1">###################################</span>


<div class="viewcode-block" id="MaskedAutoencoderViT">
<a class="viewcode-back" href="../../../../../autoapi/minerva/models/nets/image/vit/index.html#minerva.models.nets.image.vit.MaskedAutoencoderViT">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">MaskedAutoencoderViT</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Masked Autoencoder with VisionTransformer backbone.</span>

<span class="sd">    Args:</span>
<span class="sd">        img_size (int): Size of input image.</span>
<span class="sd">        patch_size (int): Size of image patch.</span>
<span class="sd">        in_chans (int): Number of input channels.</span>
<span class="sd">        embed_dim (int): Dimension of token embeddings.</span>
<span class="sd">        depth (int): Number of transformer blocks.</span>
<span class="sd">        num_heads (int): Number of attention heads.</span>
<span class="sd">        decoder_embed_dim (int): Dimension of decoder embeddings.</span>
<span class="sd">        decoder_depth (int): Number of decoder transformer blocks.</span>
<span class="sd">        decoder_num_heads (int): Number of decoder attention heads.</span>
<span class="sd">        mlp_ratio (float): Ratio of MLP hidden layer size to embedding size.</span>
<span class="sd">        norm_layer (torch.nn.LayerNorm): Normalization layer.</span>
<span class="sd">        norm_pix_loss (bool): Whether to normalize pixel loss.</span>

<span class="sd">    References:</span>
<span class="sd">        - timm: https://github.com/rwightman/pytorch-image-models/tree/master/timm</span>
<span class="sd">        - DeiT: https://github.com/facebookresearch/deit</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">img_size</span><span class="o">=</span><span class="mi">224</span><span class="p">,</span>
        <span class="n">patch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
        <span class="n">in_chans</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">embed_dim</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
        <span class="n">depth</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
        <span class="n">decoder_embed_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
        <span class="n">decoder_depth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
        <span class="n">decoder_num_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
        <span class="n">mlp_ratio</span><span class="o">=</span><span class="mf">4.0</span><span class="p">,</span>
        <span class="n">norm_layer</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">,</span>
        <span class="n">norm_pix_loss</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># MAE encoder specifics</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">patch_embed</span> <span class="o">=</span> <span class="n">PatchEmbed</span><span class="p">(</span><span class="n">img_size</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">,</span> <span class="n">in_chans</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">num_patches</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_embed</span><span class="o">.</span><span class="n">num_patches</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">cls_token</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pos_embed</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_patches</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>  <span class="c1"># fixed sin-cos embedding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_chans</span> <span class="o">=</span> <span class="n">in_chans</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">Block</span><span class="p">(</span>
                    <span class="n">embed_dim</span><span class="p">,</span>
                    <span class="n">num_heads</span><span class="p">,</span>
                    <span class="n">mlp_ratio</span><span class="p">,</span>
                    <span class="n">qkv_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">norm_layer</span><span class="o">=</span><span class="n">norm_layer</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">depth</span><span class="p">)</span>
            <span class="p">]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">norm_layer</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>

        <span class="c1"># MAE decoder specifics</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder_embed</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">decoder_embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mask_token</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">decoder_embed_dim</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">decoder_pos_embed</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_patches</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">decoder_embed_dim</span><span class="p">),</span>
            <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>  <span class="c1"># fixed sin-cos embedding</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">decoder_blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">Block</span><span class="p">(</span>
                    <span class="n">decoder_embed_dim</span><span class="p">,</span>
                    <span class="n">decoder_num_heads</span><span class="p">,</span>
                    <span class="n">mlp_ratio</span><span class="p">,</span>
                    <span class="n">qkv_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">norm_layer</span><span class="o">=</span><span class="n">norm_layer</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">decoder_depth</span><span class="p">)</span>
            <span class="p">]</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">decoder_norm</span> <span class="o">=</span> <span class="n">norm_layer</span><span class="p">(</span><span class="n">decoder_embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder_pred</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
            <span class="n">decoder_embed_dim</span><span class="p">,</span> <span class="n">patch_size</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">in_chans</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>  <span class="c1"># decoder to patch</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">norm_pix_loss</span> <span class="o">=</span> <span class="n">norm_pix_loss</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">initialize_weights</span><span class="p">()</span>

<div class="viewcode-block" id="MaskedAutoencoderViT.initialize_weights">
<a class="viewcode-back" href="../../../../../autoapi/minerva/models/nets/image/vit/index.html#minerva.models.nets.image.vit.MaskedAutoencoderViT.initialize_weights">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Initialization</span>
        <span class="n">pos_embed</span> <span class="o">=</span> <span class="n">get_2d_sincos_pos_embed</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pos_embed</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
            <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">patch_embed</span><span class="o">.</span><span class="n">num_patches</span><span class="o">**</span><span class="mf">0.5</span><span class="p">),</span>
            <span class="n">cls_token</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pos_embed</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">pos_embed</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

        <span class="n">decoder_pos_embed</span> <span class="o">=</span> <span class="n">get_2d_sincos_pos_embed</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">decoder_pos_embed</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
            <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">patch_embed</span><span class="o">.</span><span class="n">num_patches</span><span class="o">**</span><span class="mf">0.5</span><span class="p">),</span>
            <span class="n">cls_token</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder_pos_embed</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">decoder_pos_embed</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_embed</span><span class="o">.</span><span class="n">proj</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">view</span><span class="p">([</span><span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">]))</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_token</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mask_token</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_init_weights</span><span class="p">)</span></div>


<div class="viewcode-block" id="MaskedAutoencoderViT._init_weights">
<a class="viewcode-back" href="../../../../../autoapi/minerva/models/nets/image/vit/index.html#minerva.models.nets.image.vit.MaskedAutoencoderViT._init_weights">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">_init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">m</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">)</span> <span class="ow">and</span> <span class="n">m</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">):</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span></div>


<div class="viewcode-block" id="MaskedAutoencoderViT.patchify">
<a class="viewcode-back" href="../../../../../autoapi/minerva/models/nets/image/vit/index.html#minerva.models.nets.image.vit.MaskedAutoencoderViT.patchify">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">patchify</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">imgs</span><span class="p">):</span>  <span class="c1"># input: (32, 1, 224, 224)</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Extract patches from input images.</span>

<span class="sd">        Args:</span>
<span class="sd">            imgs (torch.Tensor): Input images of shape (N, C, H, W).</span>

<span class="sd">        Returns:</span>
<span class="sd">            torch.Tensor: Patches of shape (N, num_patches, patch_size^2 * in_chans).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_embed</span><span class="o">.</span><span class="n">patch_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">imgs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="n">imgs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="ow">and</span> <span class="n">imgs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">%</span> <span class="n">p</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="p">)</span>  <span class="c1"># only square images are supported, and the size must be divisible by the patch size</span>

        <span class="n">h</span> <span class="o">=</span> <span class="n">w</span> <span class="o">=</span> <span class="n">imgs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">//</span> <span class="n">p</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">imgs</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="p">(</span><span class="n">imgs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_chans</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
        <span class="p">)</span>  <span class="c1"># Transform images into (32, 1, 14, 16, 14, 16)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;nchpwq-&gt;nhwpqc&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>  <span class="c1"># reshape into (32, 14, 14, 16, 16, 1)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="p">(</span><span class="n">imgs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">h</span> <span class="o">*</span> <span class="n">w</span><span class="p">,</span> <span class="n">p</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_chans</span><span class="p">)</span>
        <span class="p">)</span>  <span class="c1"># Transform into (32, 196, 256)</span>
        <span class="k">return</span> <span class="n">x</span></div>


<div class="viewcode-block" id="MaskedAutoencoderViT.unpatchify">
<a class="viewcode-back" href="../../../../../autoapi/minerva/models/nets/image/vit/index.html#minerva.models.nets.image.vit.MaskedAutoencoderViT.unpatchify">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">unpatchify</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reconstruct images from patches.</span>

<span class="sd">        Args:</span>
<span class="sd">            x (torch.Tensor): Patches of shape (N, L, patch_size^2 * in_chans).</span>

<span class="sd">        Returns:</span>
<span class="sd">            torch.Tensor: Reconstructed images of shape (N, C, H, W).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_embed</span><span class="o">.</span><span class="n">patch_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">w</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">h</span> <span class="o">*</span> <span class="n">w</span> <span class="o">==</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;nhwpqc-&gt;nchpwq&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="n">imgs</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">3</span><span class="p">,</span> <span class="n">h</span> <span class="o">*</span> <span class="n">p</span><span class="p">,</span> <span class="n">h</span> <span class="o">*</span> <span class="n">p</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">imgs</span></div>


<div class="viewcode-block" id="MaskedAutoencoderViT.random_masking">
<a class="viewcode-back" href="../../../../../autoapi/minerva/models/nets/image/vit/index.html#minerva.models.nets.image.vit.MaskedAutoencoderViT.random_masking">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">random_masking</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask_ratio</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Perform per-sample random masking by per-sample shuffling.</span>

<span class="sd">        Args:</span>
<span class="sd">            x (torch.Tensor): Input tensor of shape (N, L, D).</span>
<span class="sd">            mask_ratio (float): Ratio of values to mask.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Masked input,</span>
<span class="sd">            binary mask, shuffled indices.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">N</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">len_keep</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">L</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">mask_ratio</span><span class="p">))</span>

        <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="n">ids_shuffle</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">noise</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">ids_restore</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">ids_shuffle</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">ids_keep</span> <span class="o">=</span> <span class="n">ids_shuffle</span><span class="p">[:,</span> <span class="p">:</span><span class="n">len_keep</span><span class="p">]</span>
        <span class="n">x_masked</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">ids_keep</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">D</span><span class="p">))</span>

        <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">mask</span><span class="p">[:,</span> <span class="p">:</span><span class="n">len_keep</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">ids_restore</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x_masked</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">ids_restore</span></div>


<div class="viewcode-block" id="MaskedAutoencoderViT.forward_encoder">
<a class="viewcode-back" href="../../../../../autoapi/minerva/models/nets/image/vit/index.html#minerva.models.nets.image.vit.MaskedAutoencoderViT.forward_encoder">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward_encoder</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask_ratio</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward pass through the encoder.</span>

<span class="sd">        Args:</span>
<span class="sd">            x (torch.Tensor): Input tensor of shape (N, C, H, W).</span>
<span class="sd">            mask_ratio (float): Ratio of values to mask.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Encoded</span>
<span class="sd">            representation, binary mask, shuffled indices.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_embed</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_embed</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:,</span> <span class="p">:]</span>

        <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">ids_restore</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_masking</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask_ratio</span><span class="p">)</span>

        <span class="n">cls_token</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_token</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_embed</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">cls_tokens</span> <span class="o">=</span> <span class="n">cls_token</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">cls_tokens</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">blk</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">blk</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">ids_restore</span></div>


<div class="viewcode-block" id="MaskedAutoencoderViT.forward_decoder">
<a class="viewcode-back" href="../../../../../autoapi/minerva/models/nets/image/vit/index.html#minerva.models.nets.image.vit.MaskedAutoencoderViT.forward_decoder">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward_decoder</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">ids_restore</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward pass through the decoder.</span>

<span class="sd">        Args:</span>
<span class="sd">            x (torch.Tensor): Input tensor of shape (N, L, D).</span>
<span class="sd">            ids_restore (torch.Tensor): Indices to restore the original order</span>
<span class="sd">            of patches.</span>

<span class="sd">        Returns:</span>
<span class="sd">            torch.Tensor: Decoded output tensor of shape (N, L, patch_size^2 * in_chans).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder_embed</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">mask_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask_token</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span>
            <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ids_restore</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span>
        <span class="p">)</span>
        <span class="n">x_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:,</span> <span class="p">:],</span> <span class="n">mask_tokens</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span>
            <span class="n">x_</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">ids_restore</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
        <span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">x_</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder_pos_embed</span>

        <span class="k">for</span> <span class="n">blk</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder_blocks</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">blk</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder_pred</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:,</span> <span class="p">:]</span>

        <span class="k">return</span> <span class="n">x</span></div>


<div class="viewcode-block" id="MaskedAutoencoderViT.forward_loss">
<a class="viewcode-back" href="../../../../../autoapi/minerva/models/nets/image/vit/index.html#minerva.models.nets.image.vit.MaskedAutoencoderViT.forward_loss">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">imgs</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculate the loss.</span>

<span class="sd">        Args:</span>
<span class="sd">            imgs (torch.Tensor): Input images of shape (N, C, H, W).</span>
<span class="sd">            pred (torch.Tensor): Predicted output of shape (N, L, patch_size^2 * in_chans).</span>
<span class="sd">            mask (torch.Tensor): Binary mask of shape (N, L).</span>

<span class="sd">        Returns:</span>
<span class="sd">            torch.Tensor: Computed loss value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">target</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">patchify</span><span class="p">(</span><span class="n">imgs</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_pix_loss</span><span class="p">:</span>
            <span class="n">mean</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">var</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">target</span> <span class="o">=</span> <span class="p">(</span><span class="n">target</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">var</span> <span class="o">+</span> <span class="mf">1.0e-6</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">pred</span> <span class="o">-</span> <span class="n">target</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">loss</span> <span class="o">*</span> <span class="n">mask</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">loss</span></div>


<div class="viewcode-block" id="MaskedAutoencoderViT.forward">
<a class="viewcode-back" href="../../../../../autoapi/minerva/models/nets/image/vit/index.html#minerva.models.nets.image.vit.MaskedAutoencoderViT.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">imgs</span><span class="p">,</span> <span class="n">mask_ratio</span><span class="o">=</span><span class="mf">0.75</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward pass.</span>

<span class="sd">        Args:</span>
<span class="sd">            imgs (torch.Tensor): Input images of shape (N, C, H, W).</span>
<span class="sd">            mask_ratio (float): Ratio of values to mask.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Loss value,</span>
<span class="sd">            predicted output, binary mask.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">latent</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">ids_restore</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_encoder</span><span class="p">(</span><span class="n">imgs</span><span class="p">,</span> <span class="n">mask_ratio</span><span class="p">)</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_decoder</span><span class="p">(</span><span class="n">latent</span><span class="p">,</span> <span class="n">ids_restore</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_loss</span><span class="p">(</span><span class="n">imgs</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">mask</span></div>


<div class="viewcode-block" id="MaskedAutoencoderViT.training_step">
<a class="viewcode-back" href="../../../../../autoapi/minerva/models/nets/image/vit/index.html#minerva.models.nets.image.vit.MaskedAutoencoderViT.training_step">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Training step.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch (Tuple[torch.Tensor]): Input batch of images and corresponding labels.</span>
<span class="sd">            batch_idx (int): Index of the current batch.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Dict[str, torch.Tensor]: Dictionary containing the loss value for</span>
<span class="sd">            the current step.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">imgs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="n">loss</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">imgs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;train_loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;loss&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="p">}</span></div>


<div class="viewcode-block" id="MaskedAutoencoderViT.validation_step">
<a class="viewcode-back" href="../../../../../autoapi/minerva/models/nets/image/vit/index.html#minerva.models.nets.image.vit.MaskedAutoencoderViT.validation_step">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Validation step.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch (Tuple[torch.Tensor]): Input batch of images and</span>
<span class="sd">            corresponding labels.</span>
<span class="sd">            batch_idx (int): Index of the current batch.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Dict[str, torch.Tensor]: Dictionary containing the loss value for</span>
<span class="sd">            the current step.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">imgs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="n">loss</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">imgs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;val_loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;val_loss&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="p">}</span></div>


<div class="viewcode-block" id="MaskedAutoencoderViT.configure_optimizers">
<a class="viewcode-back" href="../../../../../autoapi/minerva/models/nets/image/vit/index.html#minerva.models.nets.image.vit.MaskedAutoencoderViT.configure_optimizers">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Configure optimizer.</span>

<span class="sd">        Returns:</span>
<span class="sd">            torch.optim.Optimizer: Optimizer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">optimizer</span></div>
</div>



<span class="c1"># Define model architectures</span>

<span class="c1"># mae_vit_small_patch16_dec512d8b</span>
<span class="c1"># decoder: 512 dim, 8 blocks, depth: 6</span>
<span class="n">mae_vit_small_patch16</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
    <span class="n">MaskedAutoencoderViT</span><span class="p">,</span>
    <span class="n">patch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>
    <span class="n">depth</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
    <span class="n">num_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
    <span class="n">decoder_embed_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
    <span class="n">decoder_depth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">decoder_num_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">mlp_ratio</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">norm_layer</span><span class="o">=</span><span class="n">partial</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># mae_vit_base_patch16_dec512d8b</span>
<span class="c1"># decoder: 512 dim, 8 blocks,</span>
<span class="n">mae_vit_base_patch16</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
    <span class="n">MaskedAutoencoderViT</span><span class="p">,</span>
    <span class="n">patch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>
    <span class="n">depth</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
    <span class="n">num_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
    <span class="n">decoder_embed_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
    <span class="n">decoder_depth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">decoder_num_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">mlp_ratio</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">norm_layer</span><span class="o">=</span><span class="n">partial</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># mae_vit_large_patch16_dec512d8b</span>
<span class="c1"># decoder: 512 dim, 8 blocks</span>
<span class="n">mae_vit_large_patch16</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
    <span class="n">MaskedAutoencoderViT</span><span class="p">,</span>
    <span class="n">patch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
    <span class="n">depth</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span>
    <span class="n">num_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">decoder_embed_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
    <span class="n">decoder_depth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">decoder_num_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">mlp_ratio</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">norm_layer</span><span class="o">=</span><span class="n">partial</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># mae_vit_huge_patch14_dec512d8b</span>
<span class="c1"># decoder: 512 dim, 8 blocks</span>
<span class="n">mae_vit_huge_patch14</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
    <span class="n">MaskedAutoencoderViT</span><span class="p">,</span>
    <span class="n">patch_size</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="o">=</span><span class="mi">1280</span><span class="p">,</span>
    <span class="n">depth</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">num_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">decoder_embed_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
    <span class="n">decoder_depth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">decoder_num_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">mlp_ratio</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">norm_layer</span><span class="o">=</span><span class="n">partial</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># mae_vit_large_patch16_dec256d4b</span>
<span class="c1"># decoder: 256 dim, 8 blocks</span>
<span class="n">mae_vit_large_patch16D4d256</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
    <span class="n">MaskedAutoencoderViT</span><span class="p">,</span>
    <span class="n">patch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
    <span class="n">depth</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span>
    <span class="n">num_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">decoder_embed_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
    <span class="n">decoder_depth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">decoder_num_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">mlp_ratio</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">norm_layer</span><span class="o">=</span><span class="n">partial</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">),</span>
<span class="p">)</span>


<span class="c1"># mae_vit_base_patch16_dec256d4b</span>
<span class="n">mae_vit_base_patch16D4d256</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
    <span class="n">MaskedAutoencoderViT</span><span class="p">,</span>
    <span class="n">patch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>
    <span class="n">depth</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
    <span class="n">num_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
    <span class="n">decoder_embed_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
    <span class="n">decoder_depth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">decoder_num_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">mlp_ratio</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">norm_layer</span><span class="o">=</span><span class="n">partial</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">),</span>
<span class="p">)</span>


<span class="c1">################################################################################</span>
<span class="c1"># SFM DOWNSTREAM TASKS</span>
<span class="c1">################################################################################</span>


<div class="viewcode-block" id="VisionTransformer">
<a class="viewcode-back" href="../../../../../autoapi/minerva/models/nets/image/vit/index.html#minerva.models.nets.image.vit.VisionTransformer">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">VisionTransformer</span><span class="p">(</span>
    <span class="n">timm</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">vision_transformer</span><span class="o">.</span><span class="n">VisionTransformer</span><span class="p">,</span> <span class="n">L</span><span class="o">.</span><span class="n">LightningModule</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Vision Transformer with support for global average pooling&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">global_pool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">VisionTransformer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">global_pool</span> <span class="o">=</span> <span class="n">global_pool</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">VIT_MLAHead</span><span class="p">(</span>
            <span class="n">mla_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">segmentation_head</span> <span class="o">=</span> <span class="n">SegmentationHead</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_pool</span><span class="p">:</span>
            <span class="n">norm_layer</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;norm_layer&quot;</span><span class="p">]</span>
            <span class="n">embed_dim</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;embed_dim&quot;</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fc_norm</span> <span class="o">=</span> <span class="n">norm_layer</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span>  <span class="c1"># remove the original norm</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

<div class="viewcode-block" id="VisionTransformer.forward_features">
<a class="viewcode-back" href="../../../../../autoapi/minerva/models/nets/image/vit/index.html#minerva.models.nets.image.vit.VisionTransformer.forward_features">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_embed</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">_H</span><span class="p">,</span> <span class="n">_W</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">H</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_embed</span><span class="o">.</span><span class="n">patch_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="n">W</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_embed</span><span class="o">.</span><span class="n">patch_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="p">)</span>
        <span class="n">cls_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_token</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span>
            <span class="n">B</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span>
        <span class="p">)</span>  <span class="c1"># stole cls_tokens impl from Phil Wang, thanks</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">cls_tokens</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_embed</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_drop</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">featureskip</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">featureskipnum</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">for</span> <span class="n">blk</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">blk</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">featureskipnum</span> <span class="o">%</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">)</span> <span class="o">//</span> <span class="mi">4</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">featureskip</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:,</span> <span class="p">:])</span>
                <span class="c1"># print(featureskipnum)</span>
            <span class="n">featureskipnum</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span>
            <span class="n">featureskip</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="n">featureskip</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">featureskip</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
            <span class="n">featureskip</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span>
            <span class="n">h</span><span class="o">=</span><span class="n">_H</span><span class="p">,</span>
            <span class="n">w</span><span class="o">=</span><span class="n">_W</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span></div>


<div class="viewcode-block" id="VisionTransformer.forward">
<a class="viewcode-back" href="../../../../../autoapi/minerva/models/nets/image/vit/index.html#minerva.models.nets.image.vit.VisionTransformer.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_features</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span></div>
</div>



<div class="viewcode-block" id="Conv2dReLU">
<a class="viewcode-back" href="../../../../../autoapi/minerva/models/nets/image/vit/index.html#minerva.models.nets.image.vit.Conv2dReLU">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">Conv2dReLU</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">use_batchnorm</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="ow">not</span> <span class="p">(</span><span class="n">use_batchnorm</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">bn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">)</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">Conv2dReLU</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">conv</span><span class="p">,</span> <span class="n">bn</span><span class="p">,</span> <span class="n">relu</span><span class="p">)</span></div>



<div class="viewcode-block" id="DecoderBlock">
<a class="viewcode-back" href="../../../../../autoapi/minerva/models/nets/image/vit/index.html#minerva.models.nets.image.vit.DecoderBlock">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">DecoderBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">,</span>
        <span class="n">skip_channels</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">use_batchnorm</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">Conv2dReLU</span><span class="p">(</span>
            <span class="n">in_channels</span> <span class="o">+</span> <span class="n">skip_channels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">use_batchnorm</span><span class="o">=</span><span class="n">use_batchnorm</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">Conv2dReLU</span><span class="p">(</span>
            <span class="n">out_channels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">use_batchnorm</span><span class="o">=</span><span class="n">use_batchnorm</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">up</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">UpsamplingBilinear2d</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<div class="viewcode-block" id="DecoderBlock.forward">
<a class="viewcode-back" href="../../../../../autoapi/minerva/models/nets/image/vit/index.html#minerva.models.nets.image.vit.DecoderBlock.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">skip</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">skip</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">skip</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">up</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span></div>
</div>



<div class="viewcode-block" id="SegmentationHead">
<a class="viewcode-back" href="../../../../../autoapi/minerva/models/nets/image/vit/index.html#minerva.models.nets.image.vit.SegmentationHead">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">SegmentationHead</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">):</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">upsampling</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">conv2d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">kernel_size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">upsampling</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">UpsamplingBilinear2d</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="n">upsampling</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">upsampling</span> <span class="o">&gt;</span> <span class="mi">1</span>
            <span class="k">else</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">conv2d</span><span class="p">,</span> <span class="n">upsampling</span><span class="p">)</span></div>



<div class="viewcode-block" id="DecoderCup">
<a class="viewcode-back" href="../../../../../autoapi/minerva/models/nets/image/vit/index.html#minerva.models.nets.image.vit.DecoderCup">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">DecoderCup</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># self.config = config</span>
        <span class="n">head_channels</span> <span class="o">=</span> <span class="mi">512</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_more</span> <span class="o">=</span> <span class="n">Conv2dReLU</span><span class="p">(</span>
            <span class="mi">1024</span><span class="p">,</span>
            <span class="n">head_channels</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">use_batchnorm</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">decoder_channels</span> <span class="o">=</span> <span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>

        <span class="n">in_channels</span> <span class="o">=</span> <span class="p">[</span><span class="n">head_channels</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">decoder_channels</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">out_channels</span> <span class="o">=</span> <span class="n">decoder_channels</span>

        <span class="c1"># if self.config.n_skip != 0:</span>
        <span class="c1">#     skip_channels = self.config.skip_channels</span>
        <span class="c1">#     for i in range(4-self.config.n_skip):  # re-select the skip channels according to n_skip</span>
        <span class="c1">#         skip_channels[3-i]=0</span>
        <span class="c1"># else:</span>
        <span class="c1">#     skip_channels=[0,0,0,0]</span>
        <span class="n">skip_channels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_feature1</span> <span class="o">=</span> <span class="n">Conv2dReLU</span><span class="p">(</span>
            <span class="mi">1024</span><span class="p">,</span> <span class="n">skip_channels</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">use_batchnorm</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_feature2</span> <span class="o">=</span> <span class="n">Conv2dReLU</span><span class="p">(</span>
            <span class="mi">1024</span><span class="p">,</span> <span class="n">skip_channels</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">use_batchnorm</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">up2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">UpsamplingBilinear2d</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_feature3</span> <span class="o">=</span> <span class="n">Conv2dReLU</span><span class="p">(</span>
            <span class="mi">1024</span><span class="p">,</span> <span class="n">skip_channels</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">use_batchnorm</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">up3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">UpsamplingBilinear2d</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_feature4</span> <span class="o">=</span> <span class="n">Conv2dReLU</span><span class="p">(</span>
            <span class="mi">1024</span><span class="p">,</span> <span class="n">skip_channels</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">use_batchnorm</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">up4</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">UpsamplingBilinear2d</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>

        <span class="c1"># skip_channels=[128,64,32,8]</span>
        <span class="n">blocks</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">DecoderBlock</span><span class="p">(</span><span class="n">in_ch</span><span class="p">,</span> <span class="n">out_ch</span><span class="p">,</span> <span class="n">sk_ch</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">in_ch</span><span class="p">,</span> <span class="n">out_ch</span><span class="p">,</span> <span class="n">sk_ch</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">skip_channels</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span><span class="n">blocks</span><span class="p">)</span>

<div class="viewcode-block" id="DecoderCup.TransShape">
<a class="viewcode-back" href="../../../../../autoapi/minerva/models/nets/image/vit/index.html#minerva.models.nets.image.vit.DecoderCup.TransShape">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">TransShape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">head_channels</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">up</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">n_patch</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="p">)</span>  <span class="c1"># reshape from (B, n_patch, hidden) to (B, h, w, hidden)</span>

        <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_patch</span><span class="p">)),</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_patch</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">up</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_feature1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">up</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_feature2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">up2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">up</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_feature3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">up3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">up</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_feature4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">up4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span></div>


<div class="viewcode-block" id="DecoderCup.forward">
<a class="viewcode-back" href="../../../../../autoapi/minerva/models/nets/image/vit/index.html#minerva.models.nets.image.vit.DecoderCup.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">features</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">n_patch</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">hidden_states</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="p">)</span>  <span class="c1"># reshape from (B, n_patch, hidden) to (B, h, w, hidden)</span>
        <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_patch</span><span class="p">)),</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_patch</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_more</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">skip_channels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">decoder_block</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">features</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">skip</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">TransShape</span><span class="p">(</span>
                    <span class="n">features</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">head_channels</span><span class="o">=</span><span class="n">skip_channels</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">up</span><span class="o">=</span><span class="n">i</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">skip</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">decoder_block</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">skip</span><span class="o">=</span><span class="n">skip</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span></div>
</div>



<div class="viewcode-block" id="MLAHead">
<a class="viewcode-back" href="../../../../../autoapi/minerva/models/nets/image/vit/index.html#minerva.models.nets.image.vit.MLAHead">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">MLAHead</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mla_channels</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">mlahead_channels</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">norm_cfg</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MLAHead</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">mla_channels</span><span class="p">,</span> <span class="n">mlahead_channels</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">mlahead_channels</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">mlahead_channels</span><span class="p">,</span> <span class="n">mlahead_channels</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">mlahead_channels</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">mla_channels</span><span class="p">,</span> <span class="n">mlahead_channels</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">mlahead_channels</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">mlahead_channels</span><span class="p">,</span> <span class="n">mlahead_channels</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">mlahead_channels</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head4</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">mla_channels</span><span class="p">,</span> <span class="n">mlahead_channels</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">mlahead_channels</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">mlahead_channels</span><span class="p">,</span> <span class="n">mlahead_channels</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">mlahead_channels</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head5</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">mla_channels</span><span class="p">,</span> <span class="n">mlahead_channels</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">mlahead_channels</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">mlahead_channels</span><span class="p">,</span> <span class="n">mlahead_channels</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">mlahead_channels</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="p">)</span>

<div class="viewcode-block" id="MLAHead.forward">
<a class="viewcode-back" href="../../../../../autoapi/minerva/models/nets/image/vit/index.html#minerva.models.nets.image.vit.MLAHead.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mla_p2</span><span class="p">,</span> <span class="n">mla_p3</span><span class="p">,</span> <span class="n">mla_p4</span><span class="p">,</span> <span class="n">mla_p5</span><span class="p">):</span>
        <span class="n">head2</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">head2</span><span class="p">(</span><span class="n">mla_p2</span><span class="p">),</span>
            <span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">mla_p2</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">mla_p2</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span>
            <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;bilinear&quot;</span><span class="p">,</span>
            <span class="n">align_corners</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">head3</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">head3</span><span class="p">(</span><span class="n">mla_p3</span><span class="p">),</span>
            <span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">mla_p3</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">mla_p3</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span>
            <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;bilinear&quot;</span><span class="p">,</span>
            <span class="n">align_corners</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">head4</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">head4</span><span class="p">(</span><span class="n">mla_p4</span><span class="p">),</span>
            <span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">mla_p4</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">mla_p4</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span>
            <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;bilinear&quot;</span><span class="p">,</span>
            <span class="n">align_corners</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">head5</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">head5</span><span class="p">(</span><span class="n">mla_p5</span><span class="p">),</span>
            <span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">mla_p5</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">mla_p5</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span>
            <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;bilinear&quot;</span><span class="p">,</span>
            <span class="n">align_corners</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">head2</span><span class="p">,</span> <span class="n">head3</span><span class="p">,</span> <span class="n">head4</span><span class="p">,</span> <span class="n">head5</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></div>
</div>



<div class="viewcode-block" id="VIT_MLAHead">
<a class="viewcode-back" href="../../../../../autoapi/minerva/models/nets/image/vit/index.html#minerva.models.nets.image.vit.VIT_MLAHead">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">VIT_MLAHead</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Vision Transformer with support for patch or hybrid CNN input stage&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">img_size</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>
        <span class="n">mla_channels</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
        <span class="n">mlahead_channels</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
        <span class="n">num_classes</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
        <span class="n">norm_layer</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">,</span>
        <span class="n">norm_cfg</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">VIT_MLAHead</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">img_size</span> <span class="o">=</span> <span class="n">img_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm_cfg</span> <span class="o">=</span> <span class="n">norm_cfg</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mla_channels</span> <span class="o">=</span> <span class="n">mla_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">BatchNorm</span> <span class="o">=</span> <span class="n">norm_layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlahead_channels</span> <span class="o">=</span> <span class="n">mlahead_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">=</span> <span class="n">num_classes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlahead</span> <span class="o">=</span> <span class="n">MLAHead</span><span class="p">(</span>
            <span class="n">mla_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">mla_channels</span><span class="p">,</span>
            <span class="n">mlahead_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">mlahead_channels</span><span class="p">,</span>
            <span class="n">norm_cfg</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">norm_cfg</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cls</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlahead_channels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<div class="viewcode-block" id="VIT_MLAHead.forward">
<a class="viewcode-back" href="../../../../../autoapi/minerva/models/nets/image/vit/index.html#minerva.models.nets.image.vit.VIT_MLAHead.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">x3</span><span class="p">,</span> <span class="n">x4</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">w</span><span class="o">=</span><span class="mi">14</span><span class="p">):</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">n_patch</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="n">x1</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">h</span> <span class="o">==</span> <span class="n">w</span><span class="p">:</span>
            <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_patch</span><span class="p">)),</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_patch</span><span class="p">))</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">x1</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">x1</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">x2</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">x2</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
        <span class="n">x3</span> <span class="o">=</span> <span class="n">x3</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">x3</span> <span class="o">=</span> <span class="n">x3</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
        <span class="n">x4</span> <span class="o">=</span> <span class="n">x4</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">x4</span> <span class="o">=</span> <span class="n">x4</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlahead</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">x3</span><span class="p">,</span> <span class="n">x4</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">h</span> <span class="o">*</span> <span class="mi">16</span><span class="p">,</span> <span class="n">w</span> <span class="o">*</span> <span class="mi">16</span><span class="p">),</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;bilinear&quot;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span></div>
</div>



<div class="viewcode-block" id="vit_base_patch16_downstream_regression">
<a class="viewcode-back" href="../../../../../autoapi/minerva/models/nets/image/vit/index.html#minerva.models.nets.image.vit.vit_base_patch16_downstream_regression">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">vit_base_patch16_downstream_regression</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">VisionTransformer</span><span class="p">(</span>
        <span class="n">patch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
        <span class="n">embed_dim</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>
        <span class="n">depth</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
        <span class="n">mlp_ratio</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">qkv_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">norm_layer</span><span class="o">=</span><span class="n">partial</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">),</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span></div>



<div class="viewcode-block" id="vit_large_patch16_downstream_regression">
<a class="viewcode-back" href="../../../../../autoapi/minerva/models/nets/image/vit/index.html#minerva.models.nets.image.vit.vit_large_patch16_downstream_regression">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">vit_large_patch16_downstream_regression</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">VisionTransformer</span><span class="p">(</span>
        <span class="n">patch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
        <span class="n">embed_dim</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
        <span class="n">depth</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
        <span class="n">mlp_ratio</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">qkv_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">norm_layer</span><span class="o">=</span><span class="n">partial</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">),</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span></div>



<div class="viewcode-block" id="vit_huge_patch14_downstream_regression">
<a class="viewcode-back" href="../../../../../autoapi/minerva/models/nets/image/vit/index.html#minerva.models.nets.image.vit.vit_huge_patch14_downstream_regression">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">vit_huge_patch14_downstream_regression</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">VisionTransformer</span><span class="p">(</span>
        <span class="n">patch_size</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span>
        <span class="n">embed_dim</span><span class="o">=</span><span class="mi">1280</span><span class="p">,</span>
        <span class="n">depth</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
        <span class="n">mlp_ratio</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">qkv_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">norm_layer</span><span class="o">=</span><span class="n">partial</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">),</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span></div>



<div class="viewcode-block" id="interpolate_pos_embed">
<a class="viewcode-back" href="../../../../../autoapi/minerva/models/nets/image/vit/index.html#minerva.models.nets.image.vit.interpolate_pos_embed">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">interpolate_pos_embed</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">checkpoint_model</span><span class="p">,</span> <span class="n">newsize1</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">newsize2</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="s2">&quot;pos_embed&quot;</span> <span class="ow">in</span> <span class="n">checkpoint_model</span><span class="p">:</span>
        <span class="n">pos_embed_checkpoint</span> <span class="o">=</span> <span class="n">checkpoint_model</span><span class="p">[</span><span class="s2">&quot;pos_embed&quot;</span><span class="p">]</span>
        <span class="n">embedding_size</span> <span class="o">=</span> <span class="n">pos_embed_checkpoint</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">num_patches</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">patch_embed</span><span class="o">.</span><span class="n">num_patches</span>
        <span class="n">num_extra_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">pos_embed</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">num_patches</span>
        <span class="c1"># height (== width) for the checkpoint position embedding</span>
        <span class="n">orig_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="n">pos_embed_checkpoint</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">num_extra_tokens</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>
        <span class="c1"># height (== width) for the new position embedding</span>
        <span class="n">new_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">num_patches</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="c1"># class_token and dist_token are kept unchanged</span>
        <span class="k">if</span> <span class="n">orig_size</span> <span class="o">!=</span> <span class="n">new_size</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">newsize1</span> <span class="o">==</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">newsize1</span><span class="p">,</span> <span class="n">newsize2</span> <span class="o">=</span> <span class="n">new_size</span><span class="p">,</span> <span class="n">new_size</span>
            <span class="nb">print</span><span class="p">(</span>
                <span class="s2">&quot;Position interpolate from </span><span class="si">%d</span><span class="s2">x</span><span class="si">%d</span><span class="s2"> to </span><span class="si">%d</span><span class="s2">x</span><span class="si">%d</span><span class="s2">&quot;</span>
                <span class="o">%</span> <span class="p">(</span><span class="n">orig_size</span><span class="p">,</span> <span class="n">orig_size</span><span class="p">,</span> <span class="n">newsize1</span><span class="p">,</span> <span class="n">newsize2</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">extra_tokens</span> <span class="o">=</span> <span class="n">pos_embed_checkpoint</span><span class="p">[:,</span> <span class="p">:</span><span class="n">num_extra_tokens</span><span class="p">]</span>
            <span class="c1"># only the position tokens are interpolated</span>
            <span class="n">pos_tokens</span> <span class="o">=</span> <span class="n">pos_embed_checkpoint</span><span class="p">[:,</span> <span class="n">num_extra_tokens</span><span class="p">:]</span>
            <span class="n">pos_tokens</span> <span class="o">=</span> <span class="n">pos_tokens</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">orig_size</span><span class="p">,</span> <span class="n">orig_size</span><span class="p">,</span> <span class="n">embedding_size</span>
            <span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">pos_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span>
                <span class="n">pos_tokens</span><span class="p">,</span>
                <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">newsize1</span><span class="p">,</span> <span class="n">newsize2</span><span class="p">),</span>
                <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;bicubic&quot;</span><span class="p">,</span>
                <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">pos_tokens</span> <span class="o">=</span> <span class="n">pos_tokens</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">new_pos_embed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">extra_tokens</span><span class="p">,</span> <span class="n">pos_tokens</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">checkpoint_model</span><span class="p">[</span><span class="s2">&quot;pos_embed&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_pos_embed</span></div>



<div class="viewcode-block" id="SFM_BasePatch16_Downstream">
<a class="viewcode-back" href="../../../../../autoapi/minerva/models/nets/image/vit/index.html#minerva.models.nets.image.vit.SFM_BasePatch16_Downstream">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">SFM_BasePatch16_Downstream</span><span class="p">(</span><span class="n">SimpleSupervisedModel</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">img_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
        <span class="n">num_classes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
        <span class="n">in_chans</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">loss_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-3</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create a SFM model with a ViT base backbone. The ViT-Base-16 backbone</span>
<span class="sd">        has the following configuration:</span>
<span class="sd">        - Patch size: 16</span>
<span class="sd">        - Embedding dimension: 768</span>
<span class="sd">        - Depth: 12</span>
<span class="sd">        - Number of heads: 12</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        img_size : Union[int, Tuple[int, ...]]</span>
<span class="sd">            Size of the input image. Note that, to use default pre-trained SFM</span>
<span class="sd">            model, the size should be (512, 512).</span>
<span class="sd">        num_classes : int</span>
<span class="sd">            Number of classes for segmentation head. Default is 6.</span>
<span class="sd">        in_chans : int</span>
<span class="sd">            Number of input channels. Default is 1.</span>
<span class="sd">        loss_fn : Optional[torch.nn.Module], optional</span>
<span class="sd">            Loss function, by default None</span>
<span class="sd">        learning_rate : float, optional</span>
<span class="sd">            Learning rate value, by default 1e-3</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">backbone</span><span class="o">=</span><span class="n">vit_base_patch16_downstream_regression</span><span class="p">(</span>
                <span class="n">img_size</span><span class="o">=</span><span class="n">img_size</span><span class="p">,</span>
                <span class="n">num_classes</span><span class="o">=</span><span class="n">num_classes</span><span class="p">,</span>
                <span class="n">in_chans</span><span class="o">=</span><span class="n">in_chans</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="n">fc</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">(),</span>
            <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span> <span class="ow">or</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(),</span>
            <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
            <span class="n">flatten</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

<div class="viewcode-block" id="SFM_BasePatch16_Downstream._single_step">
<a class="viewcode-back" href="../../../../../autoapi/minerva/models/nets/image/vit/index.html#minerva.models.nets.image.vit.SFM_BasePatch16_Downstream._single_step">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">_single_step</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">step_name</span><span class="p">:</span> <span class="nb">str</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
        <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_single_step</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">step_name</span><span class="p">)</span></div>


<div class="viewcode-block" id="SFM_BasePatch16_Downstream.predict_step">
<a class="viewcode-back" href="../../../../../autoapi/minerva/models/nets/image/vit/index.html#minerva.models.nets.image.vit.SFM_BasePatch16_Downstream.predict_step">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">predict_step</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits</span></div>
</div>

</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Unicamp.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>